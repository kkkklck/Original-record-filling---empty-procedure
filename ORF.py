
# === åŸå§‹è®°å½•è‡ªåŠ¨å¡«å†™ç¨‹åº ===


from pathlib import Path
import re, copy, math, warnings, sys, os, unicodedata, ctypes
from collections import defaultdict
from typing import Union
from docx import Document
from docx.shared import RGBColor, Pt
from openpyxl.styles import Font, Alignment


warnings.filterwarnings("ignore", category=SyntaxWarning)

TITLE = "åŸå§‹è®°å½•è‡ªåŠ¨å¡«å†™ç¨‹åº"
VERSION = "v 1.0.1"


# ===== é»˜è®¤è·¯å¾„ =====
SCRIPT_DIR = Path(__file__).resolve().parent
WORD_SRC_DEFAULT = Path(r"D:\eg\eg.docx")
XLSX_WITH_SUPPORT_DEFAULT = SCRIPT_DIR / "åŸå§‹è®°å½•excelæ¨¡æ¿.xlsx"
DEFAULT_FONT_PT = 9

# æ¯é¡µ 5 ç»„ã€æ¯ç»„ 5 è¡Œã€æ¯è¡Œ 8 è¯»æ•°+å¹³å‡å€¼
PER_LINE_PER_BLOCK = 5
BLOCKS_PER_SHEET = 5
MU_DIGITS_THRESHOLD = 4  # éœ€æ±‚ï¼šå››ä½æ•°â†’Î¼

# æœ¬æ¬¡è¿è¡Œåªæç¤ºä¸€æ¬¡
_hint_shown = False

# æ‰“å°é¡ºåºï¼šå¯è‡ªè¡Œè°ƒæ•´ä½ç½®
CATEGORY_ORDER = ["é’¢æŸ±", "é’¢æ¢", "æ”¯æ’‘", "ç½‘æ¶", "å…¶ä»–"]

# æ”¯æ’‘/ç½‘æ¶ åˆ†æ¡¶ç­–ç•¥ï¼š"number"=æŒ‰ç¼–å·ï¼Œ"floor"=æŒ‰æ¥¼å±‚ï¼›ä»…æœ¬æ¬¡è¿è¡Œç”Ÿæ•ˆ
support_bucket_strategy = None
net_bucket_strategy = None

# è½»é‡è¯†åˆ«ç¼“å­˜ï¼šé¿å…é‡å¤è¯»å– Word
_PROBE_CACHE = {
    "src": None,
    "grouped": None,
    "all_rows": None,
    "categories": None,
}

# â€”â€” ä¸¥é˜²è·¨ç±»/è·¨ Î¼ å†™ä¸²ï¼ˆå¼€å…³ï¼‰â€”â€”
STRICT_CROSS_CAT_GUARD = True

def _sheet_cat_from_title(title: str) -> str | None:
    """æ ¹æ® sheet åæ¨æ–­ç±»åˆ«ï¼šå»æ‰ï¼ˆnï¼‰å’Œ Î¼ åæ¯”å¯¹å‰ç¼€ã€‚"""
    base = re.sub(r"ï¼ˆ\d+ï¼‰$", "", (title or "").strip())
    base = base.replace(" Î¼", "Î¼")  # å®¹é”™ï¼šæœ‰äººæ‰‹æŠ–åŠ ç©ºæ ¼
    base_wo_mu = base.replace("Î¼", "")
    for c in CATEGORY_ORDER:
        if base_wo_mu.startswith(c):
            return c
    return None

def _is_mu_title(title: str) -> bool:
    return "Î¼" in (title or "")

def _filter_pages_for_cat(pages: list[str], cat: str) -> list[str]:
    """åªä¿ç•™å±äº cat çš„é¡µåï¼ˆå†ä¿é™©ï¼‰ã€‚"""
    return [p for p in pages if _sheet_cat_from_title(p) == cat]


# === é€šç”¨è¾“å…¥å°è£… ===
def enable_ansi():
    if os.name != "nt":
        return True
    k32 = ctypes.windll.kernel32
    h = k32.GetStdHandle(-11)  # STD_OUTPUT_HANDLE
    mode = ctypes.c_uint32()
    if not k32.GetConsoleMode(h, ctypes.byref(mode)):
        return False
    return bool(k32.SetConsoleMode(h, mode.value | 0x0004))  # ENABLE_VIRTUAL_TERMINAL_PROCESSING


enable_ansi()







class BackStep(Exception):
    """ç”¨æˆ·è¾“å…¥ q è¯·æ±‚è¿”å›ä¸Šä¸€æ­¥ã€‚"""
    pass


class AbortToPath(Exception):
    """ç”¨æˆ·ä¸»åŠ¨ä¸­æ–­å½“å‰æ¨¡å¼å¹¶è¿”å›è·¯å¾„è¾“å…¥ã€‚"""
    pass


def ask(prompt: str, allow_empty: bool = True, lower: bool = False) -> str:
    """ç»Ÿä¸€çš„æ§åˆ¶å°è¾“å…¥å‡½æ•°ã€‚

    å‚æ•°:
        prompt: æç¤ºå­—ç¬¦ä¸²ã€‚
        allow_empty: æ˜¯å¦å…è®¸ç©ºè¾“å…¥ï¼›False æ—¶ä¼šé‡å¤è¯¢é—®ã€‚
        lower: è¿”å›å€¼æ˜¯å¦å°å†™åŒ–ã€‚

    è¿”å›:
        ç”¨æˆ·è¾“å…¥çš„å­—ç¬¦ä¸²ï¼ˆå¯å°å†™åŒ–ï¼‰ã€‚

    ç‰¹æ®Š:
        è¾“å…¥ ``q`` å°†è§¦å‘ :class:`BackStep` å¼‚å¸¸ã€‚
        ä»…è¯†åˆ«å°å†™ ``q``ï¼Œå¤§å†™ ``Q`` åœ¨æ­¤é˜¶æ®µè§†ä¸ºæ™®é€šå­—ç¬¦ã€‚
    """
    while True:
        raw = input(f"{prompt}\nâ†’ ").strip()
        if raw == "q":
            raise BackStep()
        if not allow_empty and raw == "":
            continue
        return raw.lower() if lower else raw


def show_help_browser():
    """å¸®åŠ©æµè§ˆå™¨åŒ…è£…ã€‚"""
    tutorial_browser()



def ask_path() -> str | None:
    """é¡¶å±‚è·¯å¾„è¾“å…¥ã€‚

    è¿”å› ``None`` è¡¨ç¤ºç”¨æˆ·æŸ¥çœ‹å¸®åŠ©åç»§ç»­ï¼›
    è¿”å› ``"__QUIT__"`` è¡¨ç¤ºç”¨æˆ·è¯·æ±‚é€€å‡ºç¨‹åºï¼›
    å…¶ä»–è¿”å›å€¼ä¸ºç”¨æˆ·è¾“å…¥çš„è·¯å¾„å­—ç¬¦ä¸²ã€‚
    """
    raw = input("ğŸ“‚ è¯·è¾“å…¥ Word æºè·¯å¾„ï¼ˆegï¼šD:\ç¤ºä¾‹.docxï¼‰\nâ†’ ").strip()
    if raw == "help":
        show_help_browser()
        return None
    if raw == "Q":
        return "__QUIT__"
    return raw


def is_valid_path(p: str) -> bool:
    """ç®€å•æ ¡éªŒè·¯å¾„æ˜¯å¦å­˜åœ¨ã€‚"""
    path_obj = Path(p.strip('"'))
    return path_obj.exists() and path_obj.is_file()


# ---- æ–‡ä»¶å ç”¨å‹å¥½æç¤ºå°è£… ----
class FileInUse(Exception):
    pass


def _is_in_use_error(e: Exception) -> bool:
    # Windows å¸¸è§ï¼šWinError 32ï¼ˆå…±äº«å†²çªï¼‰ï¼Œæˆ– PermissionError 13
    msg = str(e).lower()
    code32 = getattr(e, "winerror", None) == 32
    perm13 = isinstance(e, PermissionError)
    hit_msg = ("being used by another process" in msg or
               "used by another process" in msg or
               "permission denied" in msg)
    return bool(code32 or perm13 or hit_msg)


def load_workbook_safe(path, **kw):
    from openpyxl import load_workbook
    try:
        return load_workbook(path, **kw)
    except Exception as e:
        if _is_in_use_error(e):
            raise FileInUse(f"Excel æ¨¡æ¿/æ–‡ä»¶è¢«å ç”¨ï¼š{path}") from e
        raise


def save_workbook_safe(wb, path):
    try:
        wb.save(path)
    except Exception as e:
        if _is_in_use_error(e):
            raise FileInUse(f"æ— æ³•ä¿å­˜ Excelï¼ˆè¢«å ç”¨ï¼‰ï¼š{path}") from e
        raise


def save_docx_safe(doc, path):
    try:
        doc.save(str(path))
    except Exception as e:
        if _is_in_use_error(e):
            raise FileInUse(f"æ— æ³•ä¿å­˜ Wordï¼ˆè¢«å ç”¨ï¼‰ï¼š{path}") from e
        raise


# ===== Word æ±‡æ€»ç”Ÿæˆ =====
NEED_COLS = 11
MIN_ROWS_EACH = 5
PLACEHOLDER = "/"
digit_re = re.compile(r"\d")
HEADER = [
    "åºå·", "æ„ä»¶åç§°åŠéƒ¨ä½",
    "æµ‹ç‚¹1 è¯»æ•°1", "æµ‹ç‚¹1 è¯»æ•°2",
    "æµ‹ç‚¹2 è¯»æ•°1", "æµ‹ç‚¹2 è¯»æ•°2",
    "æµ‹ç‚¹3 è¯»æ•°1", "æµ‹ç‚¹3 è¯»æ•°2",
    "æµ‹ç‚¹4 è¯»æ•°1", "æµ‹ç‚¹4 è¯»æ•°2",
    "æ¶‚å±‚åšåº¦å¹³å‡å€¼"
]


def ensure_cells(row, need=NEED_COLS):
    """
    ç¡®ä¿è¡¨æ ¼è¡ŒåŒ…å«è¶³å¤Ÿçš„å•å…ƒæ ¼ï¼Œä¸è¶³æ—¶è‡ªåŠ¨è¡¥å……ç©ºç™½å•å…ƒæ ¼ã€‚

    é€šè¿‡å¤åˆ¶é¦–ä¸ªå•å…ƒæ ¼çš„æ ¼å¼åˆ›å»ºç©ºç™½å•å…ƒæ ¼ï¼Œé¿å…å› åŸå§‹è¡¨æ ¼åˆ—æ•°ä¸è¶³å¯¼è‡´æ•°æ®æå–å¤±è´¥ï¼Œä¿éšœæ•°æ®ç»“æ„å®Œæ•´æ€§ã€‚

    Args:
        row: Wordè¡¨æ ¼è¡Œå¯¹è±¡ï¼ˆdocx.table.Rowï¼‰
        need: éœ€è¦çš„æœ€å°åˆ—æ•°ï¼Œé»˜è®¤11åˆ—ï¼ˆä¸æ±‡æ€»è¡¨åˆ—æ•°ä¸€è‡´ï¼‰
    """
    while len(row.cells) < need:
        tc = copy.deepcopy(row.cells[0]._tc)  # noqa
        for t in tc.xpath('.//*[local-name()="t"]'): t.text = ''
        row._tr.append(tc)  # noqa


def color_row_red(row):
    """
    å°†è¡¨æ ¼è¡Œçš„æ–‡å­—é¢œè‰²è®¾ç½®ä¸ºçº¢è‰²ï¼Œç”¨äºè¡¨å¤´é«˜äº®æ˜¾ç¤ºã€‚

    é€šè¿‡éå†è¡Œå†…æ‰€æœ‰å•å…ƒæ ¼å’Œæ®µè½ï¼Œç»Ÿä¸€è®¾ç½®æ–‡å­—é¢œè‰²ä¸ºçº¢è‰²ï¼Œå¢å¼ºæ±‡æ€»è¡¨ä¸­è¡¨å¤´ä¸æ•°æ®è¡Œçš„åŒºåˆ†åº¦ã€‚

    Args:
        row: Wordè¡¨æ ¼è¡Œå¯¹è±¡ï¼ˆdocx.table.Rowï¼‰
    """
    for c in row.cells:
        for p in c.paragraphs:
            for run in p.runs:
                run.font.color.rgb = RGBColor(255, 0, 0)


def is_data_table(tbl):
    """
    åˆ¤æ–­Wordè¡¨æ ¼æ˜¯å¦ä¸ºæœ‰æ•ˆæ•°æ®è¡¨æ ¼ï¼ˆå«æµ‹ç‚¹å’Œå¹³å‡å€¼ä¿¡æ¯ï¼‰ã€‚

    é€šè¿‡æ£€æŸ¥è¡¨æ ¼å‰3è¡Œæ˜¯å¦åŒæ—¶åŒ…å«â€œæµ‹ç‚¹1â€å’Œâ€œå¹³å‡å€¼â€å…³é”®è¯ï¼Œç­›é€‰å‡ºå®é™…å­˜å‚¨æ£€æµ‹æ•°æ®çš„è¡¨æ ¼ï¼Œæ’é™¤è¯´æ˜æ€§è¡¨æ ¼ã€‚

    Args:
        tbl: Wordè¡¨æ ¼å¯¹è±¡ï¼ˆdocx.table.Tableï¼‰
    Returns:
        bool: æ˜¯æœ‰æ•ˆæ•°æ®è¡¨æ ¼åˆ™è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
    """
    first_three = " ".join(c.text for r in tbl.rows[:3] for c in r.cells)
    return "æµ‹ç‚¹1" in first_three and "å¹³å‡å€¼" in first_three


def detect_layout(tbl):
    """
    æ£€æµ‹æ•°æ®è¡¨æ ¼çš„åˆ—å¸ƒå±€ï¼Œç¡®å®šæµ‹ç‚¹åˆ—ã€å¹³å‡å€¼åˆ—ä½ç½®åŠæ˜¯å¦ä¸ºé’¢æ¢è¡¨æ ¼ã€‚

    å®šä½å«â€œæµ‹ç‚¹1â€çš„è¡¨å¤´è¡Œï¼Œæå–æµ‹ç‚¹åˆ—ç´¢å¼•å’Œå¹³å‡å€¼åˆ—ç´¢å¼•ï¼›é€šè¿‡æµ‹ç‚¹åˆ—æ•°é‡åˆ¤æ–­æ˜¯å¦ä¸ºé’¢æ¢è¡¨æ ¼ï¼ˆé’¢æ¢å«3ä¸ªæµ‹ç‚¹ï¼‰ã€‚

    Args:
        tbl: Wordè¡¨æ ¼å¯¹è±¡ï¼ˆdocx.table.Tableï¼‰
    Returns:
        tuple: åŒ…å«ä¸‰ä¸ªå…ƒç´ çš„å…ƒç»„ï¼Œåˆ†åˆ«ä¸ºï¼š
            - æµ‹ç‚¹åˆ—ç´¢å¼•åˆ—è¡¨ï¼ˆlist[int]ï¼‰
            - å¹³å‡å€¼åˆ—ç´¢å¼•ï¼ˆintï¼‰
            - æ˜¯å¦ä¸ºé’¢æ¢è¡¨æ ¼ï¼ˆboolï¼Œé’¢æ¢è¡¨æ ¼è¿”å›Trueï¼‰
    """
    hdr = next(r for r in tbl.rows if "æµ‹ç‚¹1" in "".join(c.text for c in r.cells))
    col_vals, col_avg = [], None
    for i, t in enumerate(hdr.cells):
        txt = (t.text or "").strip()
        m = re.match(r"æµ‹ç‚¹(\d+)", txt)
        if m:
            col_vals.append(i)
        elif "å¹³å‡å€¼" in txt and "æ‰€æœ‰" not in txt:
            col_avg = i
    is_beam = len(col_vals) == 3  # æ¢ 3 ç»„ï¼ŒæŸ±/æ”¯æ’‘ 4 ç»„
    return col_vals, col_avg, is_beam


def extract_rows_with_progress(tbl, ti: int, T: int, *, show_progress: bool = True):  # noqa
    """
    ä»æ•°æ®è¡¨æ ¼æå–è¡Œæ•°æ®ï¼Œå¸¦å®æ—¶è¿›åº¦æç¤ºã€‚

    æŒ‰è¡¨å¤´å¸ƒå±€æå–æ„ä»¶åç§°ã€æµ‹ç‚¹å€¼å’Œå¹³å‡å€¼ï¼Œå¯¹é’¢æ¢è¡¨æ ¼è‡ªåŠ¨è¡¥å……ç¬¬4ä¸ªæµ‹ç‚¹ï¼ˆç”¨â€œ/â€å ä½ï¼‰ï¼›é€šè¿‡æ§åˆ¶å°å®æ—¶æ˜¾ç¤ºæå–è¿›åº¦ï¼ˆæŒ‰è¡Œè®¡ç®—ï¼‰ã€‚

    Args:
        tbl: Wordè¡¨æ ¼å¯¹è±¡ï¼ˆdocx.table.Tableï¼‰
        ti: å½“å‰è¡¨æ ¼åœ¨æ€»è¡¨æ ¼ä¸­çš„åºå·ï¼ˆä»1å¼€å§‹ï¼‰
        T: éœ€å¤„ç†çš„æ€»è¡¨æ ¼æ•°é‡
    Returns:
        list[dict]: æå–çš„æ•°æ®è¡Œåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºåŒ…å«ä»¥ä¸‹é”®çš„å­—å…¸ï¼š
            - name: æ„ä»¶åç§°ï¼ˆstrï¼‰
            - vals: æµ‹ç‚¹å€¼åˆ—è¡¨ï¼ˆlist[str]ï¼‰
            - avg: å¹³å‡å€¼ï¼ˆstrï¼‰
            - is_hdr: æ˜¯å¦ä¸ºè¡¨å¤´è¡Œï¼ˆboolï¼‰
    """
    col_vals, col_avg, is_beam = detect_layout(tbl)
    rows, last_comp, last_avg = [], None, ""
    buffer = []

    total = len(tbl.rows)
    last_flush = -1

    for ridx, r in enumerate(tbl.rows):
        if show_progress and ridx // 20 != last_flush:
            last_flush = ridx // 20
            pct = int((ridx + 1) * 100 / max(1, total))
            sys.stdout.write(f"\rğŸ“ è¯»å– Wordï¼šè¡¨ {ti}/{T}ï¼ˆ{pct}%ï¼‰")
            sys.stdout.flush()

        line = " ".join(c.text for c in r.cells)

        if "æµ‹ç‚¹1" in line:
            if buffer:
                rows.extend(buffer);
                buffer.clear()  # noqa
            meas_titles = [f"æµ‹ç‚¹{i + 1}" for i in range(len(col_vals))]
            if is_beam: meas_titles.append("æµ‹ç‚¹4")  # æ¢è¡¥ç¬¬4åˆ—æ ‡é¢˜
            rows.append({"name": "", "vals": meas_titles, "avg": "å¹³å‡å€¼", "is_hdr": True})
            continue

        if not digit_re.search(line):
            continue

        comp = r.cells[1].text.strip()
        vals = [r.cells[i].text.strip() for i in col_vals]
        if is_beam and len(vals) == 3: vals.append("/")

        raw_avg = r.cells[col_avg].text.replace("\n", "").strip()
        avg = raw_avg or last_avg or "/"
        last_avg = avg if raw_avg else last_avg

        buffer.append({"name": comp if comp != last_comp else "",
                       "vals": vals, "avg": avg, "is_hdr": False})
        last_comp = comp

    rows.extend(buffer)
    sys.stdout.write(f"\rğŸ“ è¯»å– Wordï¼šè¡¨ {ti}/{T}ï¼ˆ100%ï¼‰\n");
    sys.stdout.flush()
    return rows


def build_summary_doc_with_progress(rows):
    """
     ç”ŸæˆWordæ±‡æ€»è¡¨ï¼Œå¸¦å®æ—¶è¿›åº¦æç¤ºã€‚

     å°†æå–çš„æ•°æ®è¡Œæ•´ç†ä¸ºè§„èŒƒè¡¨æ ¼ï¼Œè¡¨å¤´æ ‡çº¢ï¼›ä¸è¶³è¡Œæ•°ç”¨å ä½ç¬¦è¡¥å……ï¼Œç»Ÿä¸€å­—ä½“å¤§å°ï¼›é€šè¿‡æ§åˆ¶å°æ˜¾ç¤ºç»„è£…è¿›åº¦ã€‚

     Args:
         rows: æå–çš„æ•°æ®è¡Œåˆ—è¡¨ï¼ˆextract_rows_with_progressè¿”å›ç»“æœï¼‰
     Returns:
         Document: ç”Ÿæˆçš„Wordæ±‡æ€»è¡¨æ–‡æ¡£å¯¹è±¡ï¼ˆdocx.document.Documentï¼‰
     """
    doc = Document()
    tbl = doc.add_table(rows=1, cols=NEED_COLS)
    tbl.style = "Table Grid"
    for i, t in enumerate(HEADER):
        tbl.rows[0].cells[i].text = t
    color_row_red(tbl.rows[0])

    serial, last_comp, buffer = 1, None, []
    total = len(rows)
    step = max(50, total // 100)

    def flush():
        nonlocal serial, buffer
        miss = max(0, MIN_ROWS_EACH - len(buffer))
        for _ in range(miss):
            q = tbl.add_row();
            ensure_cells(q)
            for z in range(2, 10): q.cells[z].text = PLACEHOLDER
            q.cells[10].text = PLACEHOLDER
        serial += 1;
        buffer.clear()

    for i, it in enumerate(rows, start=1):
        if i % step == 0 or i == total:
            pct = int(i * 100 / max(1, total))
            sys.stdout.write(f"\rğŸ“¦ ç»„è£…æ±‡æ€»ï¼š{i}/{total}ï¼ˆ{pct}%ï¼‰")
            sys.stdout.flush()

        if it["is_hdr"] and buffer: flush()

        raw_name = (it.get("name") or "").strip()
        comp = raw_name or last_comp or ""

        if last_comp and comp and comp != last_comp:
            flush();
            last_comp = None

        if it.get("is_hdr"):
            r = tbl.add_row();
            ensure_cells(r);
            color_row_red(r)
            r.cells[1].text = "æ„ä»¶åç§°åŠéƒ¨ä½" if not raw_name else raw_name
            for k, v in enumerate(it["vals"]):
                c = 2 + k * 2
                r.cells[c].text = v
            r.cells[10].text = it["avg"]
            last_comp = comp
            continue

        r = tbl.add_row();
        ensure_cells(r);
        buffer.append(r)
        first = (last_comp is None) or (comp and comp != last_comp)
        if first:
            r.cells[0].text = str(serial)
            r.cells[1].text = raw_name
            last_comp = comp
        for k, v in enumerate(it["vals"]):
            c = 2 + k * 2
            r.cells[c].text = v
            r.cells[c + 1].text = v
        r.cells[10].text = it["avg"]

    flush()
    sys.stdout.write("\n");
    sys.stdout.flush()
    return doc


def set_doc_font_progress(doc, pt=DEFAULT_FONT_PT):
    """
    ç»Ÿä¸€Wordæ–‡æ¡£ä¸­æ‰€æœ‰æ–‡å­—çš„å­—ä½“å¤§å°ï¼Œå¸¦å®æ—¶è¿›åº¦æç¤ºã€‚

    éå†æ–‡æ¡£ä¸­çš„æ‰€æœ‰æ®µè½å’Œè¡¨æ ¼å•å…ƒæ ¼ï¼Œå°†å­—ä½“å¤§å°è®¾ç½®ä¸ºæŒ‡å®šç£…æ•°ï¼ˆé»˜è®¤9ptï¼‰ï¼›é€šè¿‡æ§åˆ¶å°æ˜¾ç¤ºå­—ä½“è®¾ç½®è¿›åº¦ã€‚

    Args:
        doc: Wordæ–‡æ¡£å¯¹è±¡ï¼ˆdocx.document.Documentï¼‰
        pt: å­—ä½“å¤§å°ï¼ˆç£…ï¼‰ï¼Œé»˜è®¤9pt
    """
    cell_pars = 0
    for t in doc.tables:
        for r in t.rows:
            for c in r.cells:
                cell_pars += len(c.paragraphs)
    total = len(doc.paragraphs) + cell_pars
    done = 0
    step = max(200, total // 100)

    for p in doc.paragraphs:
        for run in p.runs: run.font.size = Pt(pt)
        done += 1
        if done % step == 0 or done == total:
            pct = int(done * 100 / max(1, total))
            sys.stdout.write(f"\rğŸ–‹ ç»Ÿä¸€å­—ä½“ï¼š{done}/{total}ï¼ˆ{pct}%ï¼‰");
            sys.stdout.flush()

    for t in doc.tables:
        for r in t.rows:
            for c in r.cells:
                for p in c.paragraphs:
                    for run in p.runs: run.font.size = Pt(pt)
                    done += 1
                    if done % step == 0 or done == total:
                        pct = int(done * 100 / max(1, total))
                        sys.stdout.write(f"\rğŸ–‹ ç»Ÿä¸€å­—ä½“ï¼š{done}/{total}ï¼ˆ{pct}%ï¼‰");
                        sys.stdout.flush()
    sys.stdout.write("\n");
    sys.stdout.flush()


# ===== rows â†’ groupsï¼ˆ8è¯»æ•°+å¹³å‡å€¼ï¼‰=====
def groups_from_your_rows(rows_all_tables):
    """
    å°†æå–çš„åŸå§‹æ•°æ®è¡Œè½¬æ¢ä¸ºæŒ‰æ„ä»¶åˆ†ç»„çš„ç»“æ„åŒ–æ•°æ®ã€‚

    æŒ‰æ„ä»¶åç§°åˆ†ç»„ï¼Œå°†æ¯ç»„æ•°æ®æ•´ç†ä¸ºè§„èŒƒæ ¼å¼ï¼ˆ8ä¸ªè¯»æ•°+1ä¸ªå¹³å‡å€¼ï¼‰ï¼Œè‡ªåŠ¨ç”¨â€œ/â€è¡¥é½ä¸è¶³çš„è¯»æ•°ã€‚

    Args:
        rows_all_tables: æ‰€æœ‰è¡¨æ ¼æå–çš„åŸå§‹æ•°æ®è¡Œåˆ—è¡¨ï¼ˆextract_rows_with_progressè¿”å›ç»“æœï¼‰
    Returns:
        list[dict]: æ„ä»¶æ•°æ®ç»„åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºåŒ…å«ä»¥ä¸‹é”®çš„å­—å…¸ï¼š
            - name: æ„ä»¶åç§°ï¼ˆstrï¼‰
            - data: æ•°æ®è¡Œåˆ—è¡¨ï¼Œæ¯è¡ŒåŒ…å«8ä¸ªè¯»æ•°å’Œ1ä¸ªå¹³å‡å€¼ï¼ˆlist[list[str]]ï¼‰
    """
    groups = [];
    cur = None
    for it in rows_all_tables:
        if it.get("is_hdr"): continue
        name = (it.get("name") or "").strip()
        if name:
            if cur and cur["data"]: groups.append(cur)  # noqa
            cur = {"name": name, "data": []}
        if not cur: continue
        vals8 = []
        for v in it["vals"]:
            v = (v or "/").strip() or "/"
            vals8.extend([v, v])
        while len(vals8) < 8: vals8.append("/")
        avg = (it.get("avg") or "/").strip() or "/"
        cur["data"].append(vals8[:8] + [avg])  # noqa
    if cur and cur["data"]: groups.append(cur)
    return groups


# ===== åˆ†ç±» / è§„åˆ™ =====
CATEGORY_SYNONYMS = {
    "ç½‘æ¶": [
        "ç½‘æ¶", "WJ", "SPACE FRAME", "SPACEFRAME", "GRID", "GRID STRUCTURE",
        "æ¡æ¶ç½‘æ¶", "çƒèŠ‚ç‚¹", "ç½‘å£³", "SJ",
        "XX", "SX", "FG", "ä¸Šå¼¦", "ä¸‹å¼¦", "è…¹æ†"
    ],
    "æ”¯æ’‘": ["æ”¯æ’‘", "WZ", "ZC", "æ”¯æ¶", "æ–œæ’‘", "æ’‘æ†"],
    "é’¢æŸ±": ["é’¢æŸ±", "æŸ±", "GZ", "æ¡†æ¶æŸ±", "ç«‹æŸ±", "HæŸ±"],
    "é’¢æ¢": ["é’¢æ¢", "æ¢", "GL", "è¿ç³»æ¢", "æª©æ¡", "æ¥¼æ¢¯æ¢", "å¹³å°æ¢", "å±‹æ¶æ¢"],
}


def kind_of(name: str) -> str:
    """
    æ ¹æ®æ„ä»¶åç§°åˆ¤æ–­ç±»å‹ï¼ˆé’¢æŸ±/é’¢æ¢/æ”¯æ’‘/å…¶ä»–ï¼‰ã€‚

    åŸºäºé¢„è®¾çš„åŒä¹‰è¯è¡¨åŒ¹é…æ„ä»¶åç§°ä¸­çš„å…³é”®è¯ï¼ˆå¦‚â€œé’¢æŸ±â€æˆ–â€œGZâ€å¯¹åº”é’¢æŸ±ï¼‰ï¼ŒæœªåŒ¹é…åˆ°å…³é”®è¯çš„æ„ä»¶å½’ä¸ºâ€œå…¶ä»–â€ç±»ã€‚

    Args:
        name: æ„ä»¶åç§°å­—ç¬¦ä¸²ï¼ˆstrï¼‰
    Returns:
        str: æ„ä»¶ç±»å‹ï¼Œå¯èƒ½ä¸º"é’¢æŸ±"ã€"é’¢æ¢"ã€"æ”¯æ’‘"æˆ–"å…¶ä»–"
    """
    s_up = name.upper()
    for cat, words in CATEGORY_SYNONYMS.items():
        for w in words:
            if w.isascii():
                if w.upper() in s_up:
                    return cat
            else:
                if w in name:
                    return cat
    return "å…¶ä»–"  # æœªè¯†åˆ« â†’ å…¶ä»–


def floor_of(name: str) -> int:
    """
    ä»æ„ä»¶åç§°ä¸­æå–æ¥¼å±‚å·ï¼Œç‰¹æ®Šæ¥¼å±‚ç”¨å›ºå®šå¤§æ•°å€¼æ ‡è®°ã€‚
    è§„åˆ™æ›´æ–°ï¼š
      - æœºæˆ¿ä¼˜å…ˆäºå±‹é¢ï¼Œé¿å…â€œå±‹é¢æœºæˆ¿å±‚â€è¢«è¯¯åˆ¤ä¸ºå±‹é¢
      - æœºæˆ¿å±‚: 10**6 - 1ï¼›å±‹é¢: 10**6
    """
    s = (name or "").replace("ï¼", "-").replace("â€”", "-").replace("â€“", "-")
    sl = s.lower()
    # â‘  å…ˆæœºæˆ¿ï¼ˆæ›´å…·ä½“ï¼‰
    if re.search(r"(æœºæˆ¿(?:å±‚)?|\bjf\b)", sl):
        return 10 ** 6 - 1
    # â‘¡ å†å±‹é¢
    if re.search(r"(?:å±‹é¢|å±‹é¡¶|é¡¶\s*å±‚)", s) or re.search(r"\b(?:wm|dc)\b", sl):
        return 10 ** 6
    # â‘¢ å¸¸è§„æ•°å­—å±‚
    m = re.search(r"(?i)[FL]\s*(\d+)", s)
    if m: return int(m.group(1))
    m = re.search(r"(?i)(\d+)\s*[FL]", s)
    if m: return int(m.group(1))
    m = re.search(r"(\d+)\s*[å±‚æ¨“æ¥¼]", s)
    if m: return int(m.group(1))
    # â‘£ åœ°ä¸‹/è´Ÿå±‚ â†’ ç»Ÿå½’ 0ï¼ˆæ’åºé  _floor_label_from_nameï¼‰
    if re.search(r"(?i)\bB\s*\d+\b|è´Ÿ\s*\d+\s*å±‚?", s):
        return 0
    return 0



def _floor_label_from_name(name: str) -> str:
    """è¿”å›æ ‡ç­¾ï¼šB2 / 5F / æœºæˆ¿å±‚ / å±‹é¢ ...ï¼ˆæœºæˆ¿ä¼˜å…ˆäºå±‹é¢ï¼‰"""
    s = (name or "").replace("ï¼", "-").replace("â€”", "-").replace("â€“", "-")
    sl = s.lower()
    # â‘  æœºæˆ¿å…ˆåˆ¤
    if re.search(r"(æœºæˆ¿(?:å±‚)?|\bjf\b)", sl):
        return "æœºæˆ¿å±‚"
    # â‘¡ å†å±‹é¢
    if re.search(r"å±‹é¢|é¡¶å±‚", s) or re.search(r"\b(?:wm|dc)\b", sl):
        return "å±‹é¢"
    m = re.search(r"(?i)B\s*(\d+)", s)
    if m: return f"B{int(m.group(1))}"
    m = re.search(r"(\d+)\s*[Ffå±‚æ¨“æ¥¼]?", s)
    if m: return f"{int(m.group(1))}F"
    return "F?"



def _floor_sort_key_by_label(label: str):
    """ç”Ÿæˆæ¥¼å±‚æ ‡ç­¾çš„æ’åºé”®ã€‚"""
    m = re.fullmatch(r"B(\d+)", label)
    if m:
        return (0, -int(m.group(1)))
    m = re.fullmatch(r"(\d+)F", label)
    if m:
        return (1, int(m.group(1)))
    if label == "æœºæˆ¿å±‚":
        return (2, 0)
    if label == "å±‹é¢":
        return (3, 0)
    return (4, 0)


def segment_index(floor: int, breaks: list[int]) -> int:
    """
    æ ¹æ®æ¥¼å±‚æ–­ç‚¹è¿”å›åˆ†æ®µç´¢å¼•ã€‚
    æ›´æ–°ï¼š
      - æœºæˆ¿å±‚å•ç‹¬æˆæ®µï¼Œä½äºæ•°å­—å±‚ä¹‹åã€å±‹é¢ä¹‹å‰
      - å±‹é¢åœ¨æœ€æœ«æ®µ
      - è‹¥æ— æ–­ç‚¹ï¼Œæœºæˆ¿å±‚ä¸å±‹é¢ä¹Ÿèƒ½ç¨³å®šåˆ†å¼€ï¼ˆç´¢å¼• 1 / 2ï¼‰
    """
    # æ²¡æœ‰æ–­ç‚¹æ—¶ï¼š0=æ•°å­—&åœ°ä¸‹ï¼Œ1=æœºæˆ¿å±‚ï¼Œ2=å±‹é¢
    if not breaks:
        if floor == 10**6 - 1:  # æœºæˆ¿å±‚
            return 1
        if floor >= 10**6:      # å±‹é¢
            return 2
        return 0

    # æœ‰æ–­ç‚¹æ—¶ï¼šæ•°å­—å±‚ â†’ æœºæˆ¿å±‚( len(breaks) ) â†’ å±‹é¢( len(breaks)+1 )
    if floor == 10**6 - 1:      # æœºæˆ¿å±‚
        return len(breaks)
    if floor >= 10**6:          # å±‹é¢
        return len(breaks) + 1

    # å¸¸è§„æ•°å­—å±‚ï¼šè½åˆ°ç¬¬ä¸€ä¸ª >= æ–­ç‚¹ çš„æ®µ
    for i, b in enumerate(breaks):
        if floor <= b:
            return i
    return len(breaks)  # é«˜äºæœ€å¤§æ–­ç‚¹çš„æ•°å­—å±‚ï¼ˆæé«˜å±‚ï¼‰ä»è½åœ¨æœ€åä¸€ä¸ªæ•°å­—æ®µ



def expand_blocks(groups, block_size=PER_LINE_PER_BLOCK):
    """
    å°†æ„ä»¶æ•°æ®ç»„æ‹†åˆ†ä¸ºå›ºå®šå¤§å°çš„æ•°æ®å—ï¼ˆé»˜è®¤5è¡Œ/å—ï¼‰ï¼Œä¸è¶³è¡Œæ•°ç”¨â€œ/â€è¡¥é½ã€‚

    æŒ‰æŒ‡å®šå—å¤§å°ï¼ˆé»˜è®¤5è¡Œï¼‰æ‹†åˆ†æ¯ç»„æ•°æ®ï¼Œç¡®ä¿æ¯ä¸ªå—ç»“æ„ç»Ÿä¸€ï¼Œé€‚é…Excelæ¨¡æ¿ä¸­â€œæ¯ç»„æ•°æ®å 5è¡Œâ€çš„æ ¼å¼è¦æ±‚ã€‚

    Args:
        groups: æ„ä»¶æ•°æ®ç»„åˆ—è¡¨ï¼ˆgroups_from_your_rowsè¿”å›ç»“æœï¼‰
        block_size: æ¯ä¸ªæ•°æ®å—çš„è¡Œæ•°ï¼Œé»˜è®¤5è¡Œ
    Returns:
        list[dict]: æ•°æ®å—åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºåŒ…å«ä»¥ä¸‹é”®çš„å­—å…¸ï¼š
            - name: æ„ä»¶åç§°ï¼ˆstrï¼‰
            - data: 5è¡Œæ•°æ®ï¼ˆæ¯è¡Œ9åˆ—ï¼Œlist[list[str]]ï¼‰
    """
    blocks = []
    for g in groups:
        rows = list(g["data"])
        for k in range(0, len(rows), block_size):
            sub = rows[k:k + block_size]
            while len(sub) < block_size: sub.append(['/'] * 9)
            blocks.append({"name": g["name"], "data": sub})
    return blocks


# ===== Excel sheet å¤åˆ¶ä¸è®¾ç½® =====
def clone_sheet_keep_print(wb, tpl_name: str, title: str):
    """
    å¤åˆ¶Excelå·¥ä½œè¡¨å¹¶ä¿ç•™æ‰“å°æ ¼å¼å’Œè§†å›¾è®¾ç½®ï¼Œç¡®ä¿æ–°è¡¨ä¸æ¨¡æ¿æ ¼å¼ä¸€è‡´ã€‚

    å¤åˆ¶å†…å®¹åŒ…æ‹¬è§†å›¾ï¼ˆç¼©æ”¾ã€å†»ç»“çª—æ ¼ï¼‰ã€æ‰“å°åŒºåŸŸã€é¡µé¢è®¾ç½®ï¼ˆæ–¹å‘ã€çº¸å¼ å¤§å°ï¼‰ã€é¡µè¾¹è·ã€è¡Œåˆ—å®½ç­‰ï¼Œä¿éšœæ ¼å¼ç»Ÿä¸€æ€§ã€‚

    Args:
        wb: Excelå·¥ä½œç°¿å¯¹è±¡ï¼ˆopenpyxl.workbook.Workbookï¼‰
        tpl_name: æ¨¡æ¿å·¥ä½œè¡¨åç§°ï¼ˆstrï¼‰
        title: æ–°å·¥ä½œè¡¨åç§°ï¼ˆstrï¼‰
    Returns:
        openpyxl.worksheet.worksheet.Worksheet: æ–°å¤åˆ¶çš„å·¥ä½œè¡¨å¯¹è±¡
    """
    tpl = wb[tpl_name]
    ws = wb.copy_worksheet(tpl)
    ws.title = title
    ws.sheet_view.view = "pageBreakPreview"
    try:
        ws.freeze_panes = tpl.freeze_panes
    except:
        pass
    try:
        ws.print_area = tpl.print_area
    except:
        pass
    try:
        ws.print_titles = tpl.print_titles
    except:
        pass
    for attr in (
            "orientation", "paperSize", "fitToWidth", "fitToHeight", "scale", "firstPageNumber", "useFirstPageNumber"):
        try:
            setattr(ws.page_setup, attr, getattr(tpl.page_setup, attr))
        except:
            pass
    for attr in ("left", "right", "top", "bottom", "header", "footer"):
        try:
            setattr(ws.page_margins, attr, getattr(tpl.page_margins, attr))
        except:
            pass
    for col, dim in tpl.column_dimensions.items():
        if dim.width is not None:
            ws.column_dimensions[col].width = dim.width
    for row, dim in tpl.row_dimensions.items():
        if dim.height is not None:
            ws.row_dimensions[row].height = dim.height
    return ws


def ensure_total_pages(wb, base: str, total_needed: int):
    """
    ç¡®ä¿Excelä¸­æœ‰è¶³å¤Ÿçš„æŒ‡å®šç±»å‹å·¥ä½œè¡¨ï¼Œä¸è¶³æ—¶è‡ªåŠ¨ä»åŸºç¡€è¡¨å¤åˆ¶è¡¥å……ã€‚

    ç­›é€‰å¹¶æ’åºå·²æœ‰åŒç±»å‹å·¥ä½œè¡¨ï¼Œè‹¥æ•°é‡ä¸è¶³ï¼Œä»¥åŸºç¡€è¡¨ä¸ºæ¨¡æ¿å¤åˆ¶æ–°è¡¨å¹¶æŒ‰åºå·å‘½åï¼ˆå¦‚â€œé’¢æŸ±ï¼ˆ2ï¼‰â€ï¼‰ã€‚

    Args:
        wb: Excelå·¥ä½œç°¿å¯¹è±¡ï¼ˆopenpyxl.workbook.Workbookï¼‰
        base: åŸºç¡€å·¥ä½œè¡¨åç§°ï¼ˆå¦‚"é’¢æŸ±"ï¼Œstrï¼‰
        total_needed: éœ€è¦çš„å·¥ä½œè¡¨æ€»æ•°ï¼ˆintï¼‰
    Returns:
        list[str]: æ’åºåçš„å·¥ä½œè¡¨åç§°åˆ—è¡¨
    """
    names = [s for s in wb.sheetnames if s == base or re.match(rf'^{re.escape(base)}ï¼ˆ\d+ï¼‰$', s)]
    names = sorted(names, key=lambda n: 0 if n == base else int(re.findall(r'ï¼ˆ(\d+)ï¼‰', n)[0]))
    have = len(names)
    start = have + 1
    for _ in range(max(0, total_needed - have)):
        nm = f"{base}ï¼ˆ{start}ï¼‰"
        clone_sheet_keep_print(wb, base, nm)
        names.append(nm);
        start += 1
    return names


def ensure_total_pages_from(wb, tpl_name: str, new_base: str, total_needed: int):
    """
    ä¸ºâ€œå…¶ä»–â€ç±»æ„ä»¶ç¡®ä¿è¶³å¤Ÿçš„å·¥ä½œè¡¨ï¼Œå¤ç”¨å·²æœ‰è¡¨æˆ–ä»æŒ‡å®šæ¨¡æ¿å¤åˆ¶ã€‚

    é€‚ç”¨äºæ— ä¸“ç”¨æ¨¡æ¿çš„ç±»åˆ«ï¼Œç­›é€‰å·²æœ‰åŒç±»å‹å·¥ä½œè¡¨ï¼Œä¸è¶³æ—¶ä»æŒ‡å®šæ¨¡æ¿ï¼ˆå¦‚â€œé’¢æŸ±â€ï¼‰å¤åˆ¶æ–°è¡¨å¹¶å‘½åã€‚

    Args:
        wb: Excelå·¥ä½œç°¿å¯¹è±¡ï¼ˆopenpyxl.workbook.Workbookï¼‰
        tpl_name: æ¨¡æ¿å·¥ä½œè¡¨åç§°ï¼ˆå¦‚"é’¢æŸ±"ï¼Œstrï¼‰
        new_base: æ–°ç±»åˆ«åŸºç¡€åç§°ï¼ˆå¦‚"å…¶ä»–"ï¼Œstrï¼‰
        total_needed: éœ€è¦çš„å·¥ä½œè¡¨æ€»æ•°ï¼ˆintï¼‰
    Returns:
        list[str]: æ’åºåçš„å·¥ä½œè¡¨åç§°åˆ—è¡¨
    """
    # å¤ç”¨å·²æœ‰â€œå…¶ä»–ï¼ˆnï¼‰â€ç­‰ï¼›ä¸è¶³åˆ™ä» tpl_name å¤åˆ¶
    names = [s for s in wb.sheetnames if s == new_base or re.match(rf'^{re.escape(new_base)}ï¼ˆ\d+ï¼‰$', s)]
    names = sorted(names, key=lambda n: 0 if n == new_base else int(re.findall(r'ï¼ˆ(\d+)ï¼‰', n)[0]))
    have = len(names)
    start = have + 1
    for _ in range(max(0, total_needed - have)):
        nm = f"{new_base}ï¼ˆ{start}ï¼‰" if start > 1 else new_base
        clone_sheet_keep_print(wb, tpl_name, nm)
        if nm not in names:
            names.append(nm)
        start += 1
    return names

# ========= Î¼ åˆ¤å®š & Î¼ é¡µåˆ›å»º & æ¸…ç† =========

def _normalize_digits(s: str) -> str:
    """æŠŠå…¨è§’/å¸¦é€—å·/ç©ºæ ¼/ç‚¹çš„æ•°å­—ç»Ÿä¸€æˆ ASCII è¿ç»­æ•°å­—ä¸²ï¼š'ï¼”,070.0' â†’ '40700'ã€‚"""
    s = unicodedata.normalize("NFKC", str(s or ""))
    parts = re.findall(r"\d+", s)
    return "".join(parts)


def _is_mu_block(block: dict) -> bool:
    # """åˆ¤æ–­ä¸€ä¸ªå—æ˜¯å¦å« Î¼ å€¼ã€‚
    #
    # ä»…æ£€æŸ¥æ¯è¡Œå‰ 8 ä¸ªè¯»æ•°æ ¼ï¼š
    #   * è‹¥æŸæ ¼ä¸ºçº¯ \d{4,} æ•°å­—ä¸² â†’ Î¼ï¼›
    #   * è‹¥æŸæ ¼èƒ½è§£æä¸ºæ•°å€¼ä¸”ç»å¯¹å€¼ â‰¥1000ï¼Œä¸”ä¸å«å•ä½/æ–‡å­— â†’ Î¼ã€‚
    # """
    for row in block.get("data", []):
        cells = row[:8] if isinstance(row, (list, tuple)) else []
        for v in cells:
            if v in (None, "/", "ï¼"):
                continue
            s = unicodedata.normalize("NFKC", str(v)).strip()
            if re.fullmatch(r"\d{4,}", s):
                return True
            if re.fullmatch(r"[\d.,]+", s):
                try:
                    if abs(float(s.replace(",", ""))) >= 1000:
                        return True
                except Exception:
                    pass
    return False


def _ensure_mu_pages_shared(wb, base: str, mu_tpl: str, start_idx: int, count: int) -> list[str]:
    """
    åŸºäº Î¼ æ¯ç‰ˆï¼ˆå¦‚ 'é’¢æ¢Î¼'ï¼‰æ‰¹é‡ç”Ÿæˆç¼–å·é¡µï¼Œåºå·ä» start_idx+1 èµ·ã€‚
    è‹¥ start_idx ä¸º 0ï¼Œåˆ™å¤ç”¨æ¨¡æ¿é¡µä½œä¸ºé¦–å¼ ã€‚
    è¿”å›ç”Ÿæˆï¼ˆæˆ–å·²æœ‰ï¼‰çš„ Î¼ é¡µååˆ—è¡¨ï¼š['é’¢æ¢Î¼', 'é’¢æ¢Î¼ï¼ˆ2ï¼‰', ...]
    """
    pages = []
    use_tpl_first = start_idx == 0 and mu_tpl in wb.sheetnames
    for idx in range(start_idx + 1, start_idx + count + 1):
        if use_tpl_first and idx == start_idx + 1:
            pages.append(mu_tpl)
            continue
        nm = f"{base}Î¼ï¼ˆ{idx}ï¼‰"
        if nm not in wb.sheetnames:
            if mu_tpl not in wb.sheetnames:
                raise RuntimeError(f"ç¼ºå°‘ Î¼ æ¨¡æ¿ï¼š{mu_tpl}")
            clone_sheet_keep_print(wb, mu_tpl, nm)
        pages.append(nm)
    return pages


def cleanup_unused_mu_templates(wb, used_pages: list[str]):
    """
    æ¸…æ‰æœ¬æ¬¡æ²¡ç”¨åˆ°çš„â€œè£¸ Î¼ æ¨¡æ¿é¡µâ€ï¼ˆå¦‚ 'é’¢æ¢Î¼'ï¼‰ã€‚
    """
    used = set(used_pages or [])
    base_candidates = ["é’¢æŸ±Î¼", "é’¢æ¢Î¼", "æ”¯æ’‘Î¼", "ç½‘æ¶Î¼", "é’¢æŸ± Î¼", "é’¢æ¢ Î¼", "æ”¯æ’‘ Î¼", "ç½‘æ¶ Î¼"]
    for base in base_candidates:
        if base in wb.sheetnames and base not in used:
            try:
                wb.remove(wb[base])
            except Exception:
                pass
# ========= Î¼ åˆ†æµ + å…±ç”¨ç¼–å·ï¼ˆé€šç”¨åˆ†é¡µå™¨ï¼‰ =========
def split_mu_blocks(blocks):
    normal, mu = [], []
    for b in blocks:
        (mu if _is_mu_block(b) else normal).append(b)
    return normal, mu

def pages_needed(blocks):
    return math.ceil(len(blocks) / BLOCKS_PER_SHEET) if blocks else 0

def ensure_pages_slices_for_cat_muaware(wb, cat: str, blocks_by_bucket: dict[int, list]):
    """
    Î¼ é€»è¾‘çš„é€šç”¨åˆ†é¡µå™¨ï¼ˆä¿®æ­£ç‰ˆï¼‰ï¼š
      - åŒä¸€æ¡¶å†…ï¼šå…ˆæ™®é€šé¡µã€å Î¼ é¡µï¼›åŒé¡µä¸æ··
      - åºå·å…±ç”¨ï¼šæ™®é€šä¸ Î¼ è·¨æ¡¶è¿ç»­ç¼–å·
      - åªä¸ºéœ€è¦çš„æ™®é€šé¡µåˆ›å»º sheetï¼Œä¸ä¼šå› ä¸º Î¼ é¡µè€Œâ€œè¡¥é€ â€æ™®é€šé¡µ
    è¿”å›ï¼špages_slicesã€blocks_slicesï¼ˆæŒ‰æ¡¶é¡ºåºçš„åˆ—è¡¨ï¼‰
    """
    buckets = sorted(blocks_by_bucket.keys())
    pages_slices = []
    blocks_slices = []

    # ä¸¤å¥—è®¡æ•°ï¼šä¸€ä¸ªç”¨äºâ€œç¼–å·â€ï¼ˆæ™®é€š+Î¼ï¼‰ï¼Œä¸€ä¸ªç”¨äºâ€œæ™®é€šé¡µå®é™…å·²åˆ›å»ºæ•°â€
    total_all_pages = 0            # æ™®é€š + Î¼ï¼Œå†³å®š Î¼ é¡µçš„èµ·å§‹åºå·
    normal_pages_created = 0       # ä»…æ™®é€šé¡µï¼Œå†³å®š ensure_total_pages çš„ç›®æ ‡æ•°

    for i in buckets:
        all_blocks = blocks_by_bucket.get(i, []) or []

        # æ‹†åˆ†æ™®é€š/Î¼
        normal_blocks = []
        mu_blocks = []
        for b in all_blocks:
            (mu_blocks if _is_mu_block(b) else normal_blocks).append(b)

        need_n = math.ceil(len(normal_blocks) / BLOCKS_PER_SHEET) if normal_blocks else 0
        need_m = math.ceil(len(mu_blocks) / BLOCKS_PER_SHEET) if mu_blocks else 0

        # 1) æ™®é€šé¡µï¼šåªæŒ‰â€œæ™®é€šé¡µå·²åˆ›å»ºæ•° + æœ¬æ¡¶æ™®é€šéœ€æ±‚â€æ¥ç¡®ä¿
        if need_n:
            normal_full = ensure_total_pages(wb, cat, normal_pages_created + need_n)
            # å–å‡ºâ€œæœ¬æ¡¶æ–°åˆ†é…â€çš„é‚£ä¸€æ®µ
            normal_batch = normal_full[normal_pages_created : normal_pages_created + need_n]
            normal_pages_created += need_n
        else:
            normal_batch = []

        # 2) Î¼ é¡µï¼šç¼–å·è¦æ¥åœ¨â€œå·²æœ‰æ€»é¡µæ•° + æœ¬æ¡¶æ™®é€šé¡µæ•°â€ä¹‹å
        #    ä½†ä¸éœ€è¦ä¸ºäº†ç¼–å·å»åˆ›å»ºé¢å¤–çš„â€œæ™®é€šç©ºé¡µâ€
        mu_batch = []
        if need_m:
            start_idx_for_mu = total_all_pages + need_n  # å…ˆç®—ä¸ŠåŒæ¡¶æ™®é€šé¡µ
            mu_batch = _ensure_mu_pages_shared(
                wb, base=cat, mu_tpl=f"{cat}Î¼",
                start_idx=start_idx_for_mu, count=need_m
            )

        # 3) æ›´æ–°â€œæ€»é¡µæ•°â€è®¡æ•°ï¼ˆæ™®é€š+Î¼ï¼‰
        total_all_pages += (need_n + need_m)

        # 4) æ±‡æ€»æœ¬æ¡¶
        pages_slices.append(normal_batch + mu_batch)
        blocks_slices.append(normal_blocks + mu_blocks)

    return pages_slices, blocks_slices

# ===== å¿«é€Ÿæ¢æµ‹æ–‡æ¡£ä¸­åŒ…å«çš„æ„ä»¶ç±»åˆ«ï¼ˆä¾›å‰ç«¯é™é»˜è¯†åˆ«ç”¨ï¼‰ =====

def probe_categories_from_docx(src: Union[str, Path]) -> dict:
    """è½»é‡è¯†åˆ« Wordï¼Œè¿”å›ç±»åˆ«é¡ºåºä¸æ•°é‡ï¼Œå¹¶å†™å…¥ç¼“å­˜ã€‚"""
    p = Path(str(src)).resolve()
    if not p.exists():
        raise FileNotFoundError(f"æœªæ‰¾åˆ° Word æºæ–‡ä»¶ï¼š{p}")

    cache_src = _PROBE_CACHE.get("src")
    if cache_src and Path(str(cache_src)).resolve() == p:
        grouped_cached = _PROBE_CACHE.get("grouped") or {}
        cats_cached = _PROBE_CACHE.get("categories") or []
        counts_cached = {c: len(grouped_cached.get(c, [])) for c in cats_cached}
        for k in CATEGORY_ORDER:
            counts_cached.setdefault(k, 0)
        return {"categories": list(cats_cached), "counts": counts_cached}

    groups_all_tables, all_rows = read_groups_from_doc(p, progress=False)
    grouped = defaultdict(list)
    for g in groups_all_tables:
        grouped[kind_of(g["name"])].append(g)
    categories_present = [cat for cat in CATEGORY_ORDER if grouped.get(cat)]

    _PROBE_CACHE.update({
        "src": str(p),
        "grouped": grouped,
        "all_rows": all_rows,
        "categories": categories_present,
    })

    counts = {cat: len(grouped.get(cat, [])) for cat in categories_present}
    for k in CATEGORY_ORDER:
        counts.setdefault(k, 0)
    return {"categories": list(categories_present), "counts": counts}



def enforce_mu_font(wb):
    """
    éå†Excelæ‰€æœ‰å•å…ƒæ ¼ï¼Œå°†å«â€œÎ¼â€å­—ç¬¦çš„å•å…ƒæ ¼å­—ä½“å¼ºåˆ¶è®¾ä¸ºTimes New Romanã€‚

    è§£å†³â€œÎ¼â€ç¬¦å·åœ¨éƒ¨åˆ†å­—ä½“ä¸‹æ˜¾ç¤ºå¼‚å¸¸çš„é—®é¢˜ï¼Œä¿ç•™åŸå­—ä½“çš„å¤§å°ã€åŠ ç²—ç­‰å…¶ä»–å±æ€§ã€‚

    Args:
        wb: Excelå·¥ä½œç°¿å¯¹è±¡ï¼ˆopenpyxl.workbook.Workbookï¼‰
    """
    for ws in wb.worksheets:
        for row in ws.iter_rows():
            for cell in row:
                v = cell.value
                if isinstance(v, str) and "Î¼" in v:
                    f = cell.font
                    cell.font = Font(
                        name="Times New Roman",
                        sz=f.sz, bold=f.bold, italic=f.italic, vertAlign=f.vertAlign,
                        underline=f.underline, strike=f.strike, color=f.color,
                        charset=f.charset, scheme=f.scheme, outline=f.outline
                    )


# ===== æ•°æ®åŒºå®šä½ / å†™å…¥ =====
def detect_anchors(ws):
    """
    æ£€æµ‹Excelå·¥ä½œè¡¨çš„æ•°æ®é”šç‚¹ï¼Œç¡®å®šåç§°åˆ—ã€æ•°æ®åˆ—å’Œæ•°æ®èµ·å§‹è¡Œä½ç½®ã€‚

    é€šè¿‡æŸ¥æ‰¾â€œè¯»æ•°1â€å®šä½è¯»æ•°æ ‡é¢˜è¡Œï¼Œè®¡ç®—æ•°æ®èµ·å§‹è¡Œï¼›é€šè¿‡â€œæ„ä»¶åç§°â€å…³é”®è¯è°ƒæ•´åç§°åˆ—ï¼Œé€šè¿‡â€œè¯»æ•°1â€è°ƒæ•´æ•°æ®åˆ—ã€‚

    Args:
        ws: Excelå·¥ä½œè¡¨å¯¹è±¡ï¼ˆopenpyxl.worksheet.worksheet.Worksheetï¼‰
    Returns:
        dict: é”šç‚¹ä¿¡æ¯å­—å…¸ï¼ŒåŒ…å«ä»¥ä¸‹é”®ï¼š
            - name_col: åç§°åˆ—ç´¢å¼•ï¼ˆintï¼‰
            - data_col: æ•°æ®åˆ—èµ·å§‹ç´¢å¼•ï¼ˆintï¼‰
            - data_row: æ•°æ®èµ·å§‹è¡Œç´¢å¼•ï¼ˆintï¼‰
            - read_row: è¯»æ•°æ ‡é¢˜è¡Œç´¢å¼•ï¼ˆintï¼‰
    """
    read_row = None
    for r in range(1, 60):
        for c in range(1, 40):
            if "è¯»æ•°1" in str(ws.cell(row=r, column=c).value or ""):
                read_row = r;
                break
        if read_row: break
    data_start_row = (read_row + 1) if read_row else 7
    name_col = 2
    for r in range(1, (read_row or 15) + 1):
        for c in range(1, 30):
            if "æ„ä»¶åç§°" in str(ws.cell(row=r, column=c).value or ""):
                name_col = c;
                break
        if name_col != 2: break
    data_col = None
    if read_row:
        for c in range(1, 40):
            if "è¯»æ•°1" in str(ws.cell(row=read_row, column=c).value or ""):
                data_col = c;
                break
    data_col = data_col or 5
    return {"name_col": name_col, "data_col": data_col, "data_row": data_start_row, "read_row": read_row or 6}


def keep_align(cell, value):
    """
    å‘Excelå•å…ƒæ ¼å†™å…¥å€¼å¹¶ä¿ç•™åŸæœ‰å¯¹é½æ ¼å¼ï¼Œé¿å…æ ¼å¼é”™ä¹±ã€‚

    è¯»å–å•å…ƒæ ¼åŸæœ‰å¯¹é½æ–¹å¼ï¼ˆæ°´å¹³/å‚ç›´å¯¹é½ã€è‡ªåŠ¨æ¢è¡Œç­‰ï¼‰ï¼Œå†™å…¥å€¼åé‡æ–°åº”ç”¨è¿™äº›æ ¼å¼ã€‚

    Args:
        cell: Excelå•å…ƒæ ¼å¯¹è±¡ï¼ˆopenpyxl.cell.cell.Cellï¼‰
        value: å¾…å†™å…¥çš„å€¼ï¼ˆstrï¼‰
    """
    old = cell.alignment or Alignment()
    cell.value = value
    cell.alignment = Alignment(
        horizontal=old.horizontal,
        vertical=old.vertical,
        wrap_text=old.wrap_text,
        textRotation=old.textRotation,
        indent=old.indent,
        shrinkToFit=old.shrinkToFit
    )


def write_block(ws, anchors, pos, item):
    """
    å°†æ•°æ®å—å†™å…¥Excelå·¥ä½œè¡¨çš„æŒ‡å®šä½ç½®ï¼Œä¿ç•™æ ¼å¼å¯¹é½ã€‚

    æ ¹æ®é”šç‚¹ä¿¡æ¯è®¡ç®—èµ·å§‹è¡Œï¼Œå†™å…¥æ„ä»¶åç§°å’Œ5è¡Œæ•°æ®ï¼Œç¡®ä¿ä¸æ¨¡æ¿æ ¼å¼ä¸€è‡´ã€‚

    Args:
        ws: Excelå·¥ä½œè¡¨å¯¹è±¡ï¼ˆopenpyxl.worksheet.worksheet.Worksheetï¼‰
        anchors: é”šç‚¹ä¿¡æ¯å­—å…¸ï¼ˆdetect_anchorsè¿”å›ç»“æœï¼‰
        pos: æ•°æ®å—åœ¨å·¥ä½œè¡¨ä¸­çš„ä½ç½®ï¼ˆ0-4ï¼Œintï¼‰
        item: æ•°æ®å—å¯¹è±¡ï¼ˆexpand_blocksè¿”å›çš„å•ä¸ªå…ƒç´ ï¼‰
    """
    r0 = anchors["data_row"] + pos * PER_LINE_PER_BLOCK
    name_col = anchors["name_col"];
    data_col = anchors["data_col"]
    keep_align(ws.cell(row=r0, column=name_col), item["name"])
    for dr in range(PER_LINE_PER_BLOCK):
        for dc in range(9):
            ws.cell(row=r0 + dr, column=data_col + dc).value = item["data"][dr][dc]


def slash_block(ws, anchors, pos):
    """
    ç”¨â€œ/â€å¡«å……Excelå·¥ä½œè¡¨ä¸­æŒ‡å®šä½ç½®çš„æ•°æ®å—ï¼Œç”¨äºè¡¥é½æœªå¡«æ»¡çš„åŒºåŸŸã€‚

    åœ¨æŒ‡å®šä½ç½®å†™å…¥â€œ/â€å ä½ç¬¦ï¼Œä¿ç•™å•å…ƒæ ¼åŸæœ‰å¯¹é½æ ¼å¼ï¼Œç¡®ä¿è¡¨æ ¼æ ¼å¼ç»Ÿä¸€ã€‚

    Args:
        ws: Excelå·¥ä½œè¡¨å¯¹è±¡ï¼ˆopenpyxl.worksheet.worksheet.Worksheetï¼‰
        anchors: é”šç‚¹ä¿¡æ¯å­—å…¸ï¼ˆdetect_anchorsè¿”å›ç»“æœï¼‰
        pos: æ•°æ®å—ä½ç½®ï¼ˆ0-4ï¼Œintï¼‰
    """
    r0 = anchors["data_row"] + pos * PER_LINE_PER_BLOCK
    name_col = anchors["name_col"];
    data_col = anchors["data_col"]
    keep_align(ws.cell(row=r0, column=name_col), "/")
    for dr in range(PER_LINE_PER_BLOCK):
        for dc in range(9):
            ws.cell(row=r0 + dr, column=data_col + dc).value = "/"


def slash_tail(ws, anchors, used_pos):
    """
    ç”¨â€œ/â€å¡«å……å·¥ä½œè¡¨ä¸­æœªä½¿ç”¨çš„æ•°æ®å—ä½ç½®ï¼Œä»å·²ç”¨ä½ç½®åˆ°æœ€åã€‚

    ç¡®ä¿å·¥ä½œè¡¨æ•°æ®åŒºåŸŸæ ¼å¼ç»Ÿä¸€ï¼Œæœªä½¿ç”¨çš„ä½ç½®æ˜ç¡®æ ‡è®°ä¸ºâ€œ/â€ã€‚

    Args:
        ws: Excelå·¥ä½œè¡¨å¯¹è±¡ï¼ˆopenpyxl.worksheet.worksheet.Worksheetï¼‰
        anchors: é”šç‚¹ä¿¡æ¯å­—å…¸ï¼ˆdetect_anchorsè¿”å›ç»“æœï¼‰
        used_pos: å·²ä½¿ç”¨çš„æ•°æ®å—ä½ç½®ç´¢å¼•ï¼ˆintï¼‰
    """
    for rem in range(used_pos, BLOCKS_PER_SHEET):
        slash_block(ws, anchors, rem)



# ===== å…ƒä¿¡æ¯å›ºå®šåæ ‡ =====
def top_left_of_merged(ws, r, c):
    """
    æŸ¥æ‰¾åˆå¹¶å•å…ƒæ ¼çš„å·¦ä¸Šè§’å•å…ƒæ ¼åæ ‡ï¼Œç¡®ä¿å€¼å†™å…¥æ­£ç¡®ä½ç½®ã€‚

    éå†å·¥ä½œè¡¨ä¸­çš„åˆå¹¶åŒºåŸŸï¼Œè¿”å›æŒ‡å®šå•å…ƒæ ¼æ‰€å±åˆå¹¶åŒºåŸŸçš„å·¦ä¸Šè§’è¡Œå·å’Œåˆ—å·ã€‚

    Args:
        ws: Excelå·¥ä½œè¡¨å¯¹è±¡ï¼ˆopenpyxl.worksheet.worksheet.Worksheetï¼‰
        r: è¡Œå·ï¼ˆintï¼‰
        c: åˆ—å·ï¼ˆintï¼‰
    Returns:
        tuple: å·¦ä¸Šè§’å•å…ƒæ ¼çš„è¡Œå·å’Œåˆ—å·ï¼ˆint, intï¼‰
    """
    for rng in ws.merged_cells.ranges:
        if rng.min_row <= r <= rng.max_row and rng.min_col <= c <= rng.max_col:
            return rng.min_row, rng.min_col
    return r, c

# ===== éäº¤äº’ï¼šå•æ—¥æ¨¡å¼å¯¼å‡ºï¼ˆä¾› UI ç›´æ¥è°ƒç”¨ï¼‰ =====
from pathlib import Path
from typing import Union
from openpyxl import load_workbook

def export_single_day_noninteractive(
    src: Union[str, Path],
    meta: dict | None = None,
    single_date: str | None = None,
    *,
    support_strategy: str = "number",
    net_strategy: str = "number",
) -> dict:
    """
    éäº¤äº’å¯¼å‡ºï¼ˆé”å®š Mode 3 / å•æ—¥ï¼‰ã€‚
    è¿”å›: {"excel": Path, "word": Path|None}
    """
    # 0) æ ¡éªŒ
    src = Path(str(src)).resolve()
    if not src.exists():
        raise FileNotFoundError(f"æœªæ‰¾åˆ° Word æºæ–‡ä»¶ï¼š{src}")

    # 1) è§£æ Word
    grouped, categories_present = prepare_from_word(src)

    # 2) é€‰æ‹© Excel æ¨¡æ¿ï¼ˆæœ‰æ”¯æ’‘ç‰ˆï¼‰
    template_path = None
    for name in ("XLSX_WITH_SUPPORT_DEFAULT", "XLSX_TEMPLATE_WITH_SUPPORT", "DEFAULT_XLSX_WITH_SUPPORT"):
        if name in globals() and globals()[name]:
            template_path = Path(globals()[name])
            break
    if not template_path or not Path(template_path).exists():
        raise FileNotFoundError("æœªæ‰¾åˆ° Excel æ¨¡æ¿å¸¸é‡ï¼ˆXLSX_WITH_SUPPORT_DEFAULT / XLSX_TEMPLATE_WITH_SUPPORT / DEFAULT_XLSX_WITH_SUPPORTï¼‰ã€‚")

    wb = load_workbook(str(template_path))

    # 3) æ‰§è¡Œæ¨¡å¼ï¼šé”å®š Mode 3
    #    â€”â€” æ³¨å…¥ä¸€æ¬¡æ€§â€œéäº¤äº’æ—¥æœŸâ€ï¼Œä¾› run_mode è¯»å–å¹¶è·³è¿‡ ask()
    prev_flag = globals().get("NONINTERACTIVE_MODE3_DATE", None)
    globals()["NONINTERACTIVE_MODE3_DATE"] = single_date if single_date is not None else ""
    try:
        used_pages = run_mode("3", wb, grouped, categories_present)
    finally:
        # run_mode å†…éƒ¨ä¼š popï¼Œè¿™é‡Œå†å…œåº•æ¸…æ‰
        globals().pop("NONINTERACTIVE_MODE3_DATE", None)
        if prev_flag is not None:
            globals()["NONINTERACTIVE_MODE3_DATE"] = prev_flag

    # 4) å›ºå®šå…ƒä¿¡æ¯ã€å­—ä½“ä¸æ¸…ç†
    meta = meta or {}
    apply_meta_fixed(wb, categories_present, meta)
    enforce_mu_font(wb)
    cleanup_unused_sheets(wb, categories_present)

    # 5) ä¿å­˜åˆ°åŒç›®å½•ï¼Œé¿å…è¦†ç›–
    def _unique_name(p: Path) -> Path:
        if not p.exists():
            return p
        stem, suf = p.stem, p.suffix
        i = 1
        while True:
            cand = p.with_name(f"{stem}({i}){suf}")
            if not cand.exists():
                return cand
            i += 1

    out_xlsx = _unique_name(src.parent / "æ±‡æ€»åŸå§‹è®°å½•.xlsx")
    wb.save(str(out_xlsx))

    # 6) å¯é€‰ï¼šç”Ÿæˆ Word æ±‡æ€»ï¼ˆå®‰å…¨è°ƒç”¨ï¼‰
    word_out = None
    maybe_func = globals().get("export_word_summary", None)
    if callable(maybe_func):
        try:
            word_out = maybe_func(src, grouped)
        except Exception:
            word_out = None

    return {"excel": out_xlsx, "word": word_out}


def apply_meta_fixed(wb, categories_present, meta: dict):
    """
    å‘Excelå·¥ä½œè¡¨å†™å…¥å›ºå®šå…ƒä¿¡æ¯ï¼ˆå·¥ç¨‹åç§°ã€å§”æ‰˜ç¼–å·ï¼‰åˆ°æŒ‡å®šä½ç½®ã€‚

    ä»…å¤„ç†ç›®æ ‡ç±»å‹å·¥ä½œè¡¨ï¼Œå°†å·¥ç¨‹åç§°å†™å…¥C3ã€å§”æ‰˜ç¼–å·å†™å…¥L3ï¼Œæ”¯æŒåˆå¹¶å•å…ƒæ ¼ã€‚

    Args:
        wb: Excelå·¥ä½œç°¿å¯¹è±¡ï¼ˆopenpyxl.workbook.Workbookï¼‰
        categories_present: å­˜åœ¨çš„æ„ä»¶ç±»å‹åˆ—è¡¨ï¼ˆlist[str]ï¼‰
        meta: å…ƒä¿¡æ¯å­—å…¸ï¼Œå«"proj"ï¼ˆå·¥ç¨‹åç§°ï¼‰å’Œ"order"ï¼ˆå§”æ‰˜ç¼–å·ï¼‰é”®
    """
    for ws in wb.worksheets:
        if not any(ws.title.startswith(p) for p in categories_present): continue

        def _set_rc(r, c, v):
            if not v: return
            r0, c0 = top_left_of_merged(ws, r, c)
            ws.cell(row=r0, column=c0).value = v

        _set_rc(3, 3, meta.get("proj"))  # C3
        _set_rc(3, 12, meta.get("order"))  # L3


def apply_meta_on_pages(wb, pages: list[str], date_str: str):
    """
    å‘æŒ‡å®š Excel å·¥ä½œè¡¨å†™å…¥æ—¥æœŸå…ƒä¿¡æ¯ï¼Œæ”¯æŒå…¬å¼æ£€æµ‹å’Œæ¸…é™¤ã€‚

    å†™å…¥é€»è¾‘ï¼š
    1. å®šä½åˆ°ç¬¬32è¡Œç¬¬1åˆ—ï¼ˆæˆ–å…¶åˆå¹¶å•å…ƒæ ¼çš„å·¦ä¸Šè§’ï¼‰
    2. æ£€æµ‹è¯¥å•å…ƒæ ¼æ˜¯å¦åŒ…å«å…¬å¼å¼•ç”¨
    3. å¦‚æœåŒ…å«å…¬å¼ï¼Œå…ˆæ¸…é™¤å…¬å¼å†å†™å…¥å€¼ï¼ˆé¿å…å¤šä¸ªsheetå…±äº«åŒä¸€æ•°æ®æºï¼‰
    4. å†™å…¥æ—¥æœŸå€¼å¹¶ä¿ç•™å•å…ƒæ ¼å¯¹é½æ ¼å¼
    5. è¾“å‡ºè°ƒè¯•æ—¥å¿—ä»¥ä¾¿è¿½è¸ªå†™å…¥è¿‡ç¨‹

    Args:
        wb: Excel å·¥ä½œç°¿å¯¹è±¡ï¼ˆopenpyxl.workbook.Workbookï¼‰
        pages: å·¥ä½œè¡¨åç§°åˆ—è¡¨ï¼ˆlist[str]ï¼‰
        date_str: æ—¥æœŸå­—ç¬¦ä¸²ï¼ˆstrï¼‰ï¼Œå¦‚"2025å¹´1æœˆ1æ—¥"
    """
    if not pages:
        return
    value = (date_str or "").strip()
    # è°ƒè¯•æ—¥å¿—ï¼šæ˜¾ç¤ºå³å°†å†™å…¥çš„æ—¥æœŸå’Œé¡µé¢åˆ—è¡¨
    if value:
        print(f"\nğŸ“… [apply_meta_on_pages] å†™å…¥æ—¥æœŸ: '{value}' åˆ° {len(pages)} ä¸ªsheet")
    for name in pages:
        if name not in wb.sheetnames:
            continue
        ws = wb[name]
        r0, c0 = top_left_of_merged(ws, 32, 1)
        cell = ws.cell(row=r0, column=c0)

        # è¯»å–å½“å‰å•å…ƒæ ¼çš„å€¼å’Œç±»å‹
        old_value = cell.value
        has_formula = False

        # æ£€æµ‹æ˜¯å¦åŒ…å«å…¬å¼ï¼ˆExcelå…¬å¼ä»¥"="å¼€å¤´ï¼‰
        if old_value and isinstance(old_value, str) and old_value.startswith('='):
            has_formula = True
            print(f"âš ï¸  [{name}] å•å…ƒæ ¼({r0},{c0})åŒ…å«å…¬å¼: {old_value}")
            # å…ˆæ¸…ç©ºå•å…ƒæ ¼ï¼Œæ–­å¼€å…¬å¼å¼•ç”¨
            cell.value = None

        # å†™å…¥æ—¥æœŸå€¼ï¼ˆä¿ç•™å¯¹é½æ ¼å¼ï¼‰
        keep_align(cell, value)

        # éªŒè¯å†™å…¥ç»“æœ
        actual_value = ws.cell(row=r0, column=c0).value
        if has_formula:
            print(f"âœ“  [{name}] å·²æ¸…é™¤å…¬å¼å¹¶å†™å…¥: '{actual_value}' åˆ° ({r0},{c0})")
        elif value:
            # ä»…åœ¨æœ‰å€¼æ—¶è¾“å‡ºæ—¥å¿—
            print(f"   [{name}] å†™å…¥: '{actual_value}' åˆ° ({r0},{c0})")


# ===== è§„èŒƒåŒ– =====
def normalize_date(text: str) -> str:
    """
    å°†ç”¨æˆ·è¾“å…¥çš„ç¯å¢ƒæ¸©åº¦å­—ç¬¦ä¸²è§„èŒƒåŒ–ä¸ºâ€œXâ„ƒâ€æˆ–â€œX.Xâ„ƒâ€æ ¼å¼ã€‚

    ä»è¾“å…¥ä¸­æå–æ•°å­—éƒ¨åˆ†ï¼ˆå¿½ç•¥â€œâ„ƒâ€â€œåº¦â€ç­‰ç¬¦å·ï¼‰ï¼Œæ•´æ•°æ¸©åº¦å»å°æ•°ç‚¹ï¼Œå°æ•°æ¸©åº¦ä¿ç•™æœ‰æ•ˆæ•°å­—ã€‚
    è‹¥æ— æ³•æå–æœ‰æ•ˆæ•°å­—ï¼Œåˆ™è¿”å›åŸå§‹å­—ç¬¦ä¸²ã€‚

    Args:
        text: ç”¨æˆ·è¾“å…¥çš„ç¯å¢ƒæ¸©åº¦å­—ç¬¦ä¸²ï¼ˆå¦‚â€œ24â€â€œ24â„ƒâ€â€œ24.5åº¦â€ï¼‰
    Returns:
        str: æ ‡å‡†åŒ–çš„æ¸©åº¦å­—ç¬¦ä¸²ï¼ˆå¦‚â€œ24â„ƒâ€â€œ24.5â„ƒâ€ï¼‰
    """
    s = (text or "").strip()
    if not s: return ""
    if re.fullmatch(r"\d{8}", s):
        y, m, d = int(s[:4]), int(s[4:6]), int(s[6:8]);
        return f"{y}å¹´{m}æœˆ{d}æ—¥"
    s2 = s.replace("å¹´", " ").replace("æœˆ", " ").replace("æ—¥", " ")
    for ch in ".-/ï¼Œ,": s2 = s2.replace(ch, " ")
    nums = re.findall(r"\d+", s2)
    if len(nums) >= 3:
        y, m, d = map(int, nums[:3]);
        return f"{y}å¹´{m}æœˆ{d}æ—¥"
    return s



def _normalize_date_token(tok: str, base_year: int) -> str:
    """å°†å•ä¸ªæ—¥æœŸ token è§„èŒƒä¸º"YYYY-MM-DD"ï¼Œå¤±è´¥è¿”å›ç©ºä¸²ã€‚"""
    if not tok:
        return ""
    tok = tok.strip()
    tok = tok.replace("å¹´", "-").replace("æœˆ", "-").replace("æ—¥", "")
    tok = tok.replace("/", "-").replace(".", "-")
    tok = re.sub(r"\s+", "-", tok)
    if re.fullmatch(r"\d{8}", tok):
        y = int(tok[:4]);
        mth = int(tok[4:6]);
        d = int(tok[6:])
    else:
        m = re.fullmatch(r"(\d{4})-(\d{1,2})-(\d{1,2})", tok)
        if m:
            y, mth, d = map(int, m.groups())
        else:
            m = re.fullmatch(r"(\d{1,2})-(\d{1,2})", tok)
            if not m:
                return ""
            y = base_year
            mth, d = map(int, m.groups())
    if not (1 <= mth <= 12 and 1 <= d <= 31):
        return ""
    return f"{y:04d}-{mth:02d}-{d:02d}"


def _parse_dates_simple(input_str: str):
    """ç®€å•è§£æå¤šä¸ªæ—¥æœŸï¼Œè¿”å› (æ—¥æœŸåˆ—è¡¨, æ— æ•ˆtokenåˆ—è¡¨)ã€‚"""
    # å…è®¸ç©ºæ ¼/è‹±æ–‡é€—å·/ä¸­æ–‡é€—å·/ä¸­æ–‡é¡¿å·ä½œä¸ºåˆ†éš”
    tokens = [t for t in re.split(r"[,\sï¼Œã€]+", input_str.strip()) if t]

    res, ignored = [], []
    seen = set()
    base_year = None
    cur_year = datetime.now().year

    i = 0
    while i < len(tokens):
        tok = tokens[i]
        consumed = 1

        # å…ˆå°è¯•æŠŠå½“å‰ token å½“æˆä¸€ä¸ªå®Œæ•´æ—¥æœŸï¼ˆæ”¯æŒ 8/27ã€8-27ã€2025-8-27ã€2025å¹´8æœˆ27æ—¥ ç­‰ç­‰ï¼‰
        norm = _normalize_date_token(tok, base_year or cur_year)

        if not norm:
            # å°è¯• Y M D è¿™ç§è¢«ç©ºæ ¼/é€—å·æ‹†å¼€çš„æƒ…å†µï¼š2025 8 27
            if re.fullmatch(r"\d{4}", tok) and i + 2 < len(tokens) \
                    and tokens[i + 1].isdigit() and tokens[i + 2].isdigit():
                norm = _normalize_date_token(
                    f"{tok}-{tokens[i + 1]}-{tokens[i + 2]}",
                    base_year or cur_year
                )
                consumed = 3

            # å°è¯• M Dï¼š8 27ï¼ˆåŸºäº base_year æˆ–å½“å‰å¹´ï¼‰
            elif tok.isdigit() and i + 1 < len(tokens) and tokens[i + 1].isdigit():
                norm = _normalize_date_token(
                    f"{tok}-{tokens[i + 1]}",
                    base_year or cur_year
                )
                consumed = 2

        if norm:
            # é”å®š base_yearï¼Œåç»­ M-D èµ°åŒä¸€å¹´
            if base_year is None:
                base_year = int(norm[:4])
            # å»é‡ï¼šåŒä¸€å¤©ä¸é‡å¤è®¡å…¥
            if norm not in seen:
                res.append(norm)
                seen.add(norm)
        else:
            # è®°å½•æ— æ³•è§£æçš„åŸå§‹ tokenï¼ˆæˆ–ç»„åˆï¼‰
            ignored.extend(tokens[i:i + consumed])

        i += consumed

    return res, ignored

    # ===== äº¤äº’ =====
HELP_HOME = f"""
==================== åŸå§‹è®°å½•è‡ªåŠ¨å¡«å†™ç¨‹åº | å¸®åŠ©ä¸­å¿ƒï¼ˆ{VERSION}ï¼‰ ====================

ä¸€ã€åŸºæœ¬æµç¨‹
  1) åœ¨â€œè¯·è¾“å…¥ Word æºè·¯å¾„â€å¤„è¾“å…¥ .docx æ–‡ä»¶è·¯å¾„ï¼ˆè¾“å…¥ help æ‰“å¼€æœ¬å¸®åŠ©ï¼‰ï¼›
  2) ç¨‹åºè¯»å–æºæ–‡ä»¶å¹¶ç”Ÿæˆã€Šæ±‡æ€»åŸå§‹è®°å½•.docxã€‹ï¼›
  3) é€‰æ‹©æ¨¡å¼ 1 / 2 / 3 / 4ï¼ŒæŒ‰å‘å¯¼å®Œæˆåˆ†é…ä¸å‡ºè¡¨ã€‚

äºŒã€å…¨å±€æ“ä½œ
  â€¢ ä»»æ„æ­¥éª¤è¾“å…¥å°å†™ q è¿”å›ä¸Šä¸€æ­¥ï¼›ä»…åœ¨è·¯å¾„è¾“å…¥ç•Œé¢è¾“å…¥å¤§å†™ Q é€€å‡ºç¨‹åºã€‚
  â€¢ åœ¨è·¯å¾„è¾“å…¥ç•Œé¢è¾“å…¥ help æ‰“å¼€æœ¬å¸®åŠ©ï¼›å›è½¦è¿”å›è·¯å¾„è¾“å…¥ã€‚

ä¸‰ã€è¾“å…¥è§„èŒƒï¼ˆç¨‹åºè‡ªåŠ¨æ ‡å‡†åŒ–ï¼‰
  â€¢ æ—¥æœŸï¼š
      æ”¯æŒä»¥ä¸‹ä»»æ„å½¢å¼ï¼Œè‡ªåŠ¨è§„èŒƒï¼šYYYY-MM-DD / YYYY/MM/DD / YYYY.MM.DD /
      YYYY MM DD / YYYYMMDD / M-D / M/D / M.D / M D / YYYYå¹´MæœˆDæ—¥ã€‚
  â€¢ ç‰¹æ®ŠæŒ‡ä»¤ï¼š
      *   è¡¨ç¤ºâ€œå…¨éƒ¨æ¥æ”¶â€ï¼ˆå¦‚ç½‘æ¶å­ç±»ç¼–å·èŒƒå›´ï¼‰ï¼›
      lk  è¡¨ç¤ºâ€œç•™ç©ºï¼Œä¸æ¥æ”¶â€ï¼ˆä»…ç½‘æ¶èŒƒå›´å½•å…¥æ—¶å¯ç”¨ï¼‰ï¼›
      a   åœ¨â€œåˆ†é…ç¡®è®¤â€é˜¶æ®µè¡¨ç¤ºâ€œå°†æœªåˆ†é…æ„ä»¶å¹¶å…¥æœ€åä¸€å¤©â€ã€‚

å››ã€åˆ†ç±»ä¸æ¨¡æ¿ã€æ’åºè§„åˆ™
  â€¢ å·²å†…ç½®ç±»åˆ«ï¼šé’¢æŸ±ã€é’¢æ¢ã€æ”¯æ’‘ï¼ˆWZï¼‰ã€ç½‘æ¶ï¼ˆå« XX/FG/SX/æ³›ç§°ï¼‰ï¼›æœªè¯†åˆ«å½’â€œå…¶ä»–â€ï¼ˆå¤ç”¨é’¢æŸ±æ¨¡æ¿ï¼‰ã€‚
  â€¢ Î¼ å€¼è¯†åˆ«ï¼šä»»ä¸€è¯»æ•°å‡ºç°â€œâ‰¥4 ä½è¿ç»­æ•°å­—â€æˆ–â€œç»å¯¹å€¼â‰¥1000 çš„çº¯æ•°å€¼â€å³åˆ¤å®šä¸º Î¼ã€‚
      - åŒä¸€æ¡¶å†…ï¼šå…ˆå†™æ™®é€šé¡µï¼Œåå†™ Î¼ é¡µï¼›åŒé¡µä¸æ··ã€‚
      - è·¨æ¡¶ç¼–å·è¿ç»­ï¼ˆæ™®é€š+Î¼ ç»Ÿä¸€æµæ°´å·ï¼‰ï¼Œä¸ä¸º Î¼ é¢å¤–è¡¥å»ºâ€œç©ºæ™®é€šé¡µâ€ã€‚
      - æœªä½¿ç”¨çš„â€œè£¸ Î¼ æ¨¡æ¿é¡µâ€ï¼ˆå¦‚â€œé’¢æ¢Î¼â€ï¼‰ä¼šåœ¨å‡ºè¡¨åè‡ªåŠ¨æ¸…ç†ã€‚
  â€¢ é¡µæ± å‘½åï¼šæ²¿ç”¨æ¨¡æ¿é¡µåï¼Œä¸å°†æ—¥æœŸ/æ¥¼å±‚æ‹¼å…¥ Sheet åç§°ã€‚
  â€¢ æ¥¼å±‚æ’åºï¼šåœ°ä¸‹ B* â†’ æ•°å­—å±‚ï¼ˆ1Fâ†‘ï¼‰â†’ æœºæˆ¿å±‚ â†’ å±‹é¢ï¼›åŒå±‚å†…æŒ‰â€œWZ ç¼–å· â†’ åç§°ä¸­çš„æ•°å­— â†’ å­—å…¸åºâ€ã€‚

äº”ã€æ”¯æ’‘/ç½‘æ¶åˆ†æ¡¶ç­–ç•¥ï¼ˆMode 1/2/3 å°†å…ˆè¯¢é—®ï¼‰
  1 = æŒ‰ç¼–å·ï¼ˆWZ å·/ç½‘æ¶å­ç±»ç¼–å·ï¼‰ï¼› 2 = æŒ‰æ¥¼å±‚ï¼ˆä¸é’¢æŸ±/é’¢æ¢ä¸€è‡´ï¼‰ã€‚
  æ³¨ï¼šç½‘æ¶æ”¯æŒä¸ºå„å­ç±»å•ç‹¬é…ç½®èŒƒå›´ï¼›åŒä¸€æ—¥æœŸå†™å…¥åŒä¸€å¼ â€œç½‘æ¶â€è¡¨ã€‚

å…­ã€ä½¿ç”¨æç¤º
  â€¢ æ— è®ºæˆåŠŸæˆ–å¤±è´¥ï¼Œæµç¨‹ç»“æŸéƒ½ä¼šå›åˆ°è·¯å¾„è¾“å…¥ï¼›ä»…åœ¨è·¯å¾„è¾“å…¥å¤„è¾“å…¥å¤§å†™ Q æ‰ä¼šé€€å‡ºã€‚
  â€¢ è¿è¡Œå‰åŠ¡å¿…å…³é—­ç›¸å…³ Word/Excel æ–‡ä»¶ï¼Œé¿å…â€œæ–‡ä»¶å ç”¨â€å¯¼è‡´è¯»å†™å¤±è´¥ã€‚

â€”â€” å¸¸è§é—®é¢˜å¿«é€Ÿæ’æŸ¥ â€”â€”
1) â€œExcel è¢«å ç”¨/æ— æ³•ä¿å­˜â€ï¼šå…³é—­æ¨¡æ¿æˆ–ç›®æ ‡æ–‡ä»¶åé‡è¯•ã€‚
2) â€œæ‰¾ä¸åˆ°æ–‡ä»¶â€ï¼šæ£€æŸ¥è·¯å¾„æ˜¯å¦å«å¼•å·/ç©ºæ ¼ï¼›æˆ–ä½¿ç”¨é»˜è®¤è·¯å¾„ã€‚
3) â€œæœªè¯†åˆ«åˆ°æ•°æ®è¡¨â€ï¼šæº Word è¡¨æ ¼éœ€åŒ…å«â€œæµ‹ç‚¹1â€â€œå¹³å‡å€¼â€è¡¨å¤´ã€‚
4) â€œç¼ºå°‘ Î¼ æ¨¡æ¿â€ï¼šç¡®ä¿æ¨¡æ¿åŒ…å«æ‰€éœ€ Î¼ é¡µï¼ˆé’¢æŸ±Î¼/é’¢æ¢Î¼/æ”¯æ’‘Î¼/ç½‘æ¶Î¼ï¼‰ã€‚
5) â€œç½‘æ¶å…¨éƒ¨è¿›å…¥ Î¼ é¡µâ€ï¼šæ ¸æŸ¥æ˜¯å¦ç¡®æœ‰ 4 ä½æ•°æˆ– â‰¥1000 çš„è¯»æ•°è§¦å‘é˜ˆå€¼ã€‚

å¦‚éœ€æŸ¥çœ‹å„æ¨¡å¼è¯´æ˜ï¼Œè¯·åœ¨ä¸‹æ–¹è¾“å…¥ 1 / 2 / 3 / 4ï¼›å›è½¦æˆ– q è¿”å›ã€‚

=====================================================================
"""

HELP_TEXTS = {
    "1": r"""====================  Mode 1 | æŒ‰æ—¥æœŸåˆ†æ¡¶ï¼ˆé»˜è®¤ç¨³å¥ï¼‰  ====================

é€‚ç”¨åœºæ™¯
  å°†å…¨éƒ¨æ„ä»¶åˆ†é…è‡³å¤šä¸ªæ—¥æœŸï¼›æ”¯æŒâ€œåé¢çš„æ—¥æœŸä¼˜å…ˆâ€ï¼ˆé»˜è®¤ï¼‰æˆ–â€œå‰é¢çš„æ—¥æœŸä¼˜å…ˆâ€ã€‚

æ“ä½œæµç¨‹
  1) é€‰æ‹©æ¨¡å¼ï¼šè¾“å…¥ 1ï¼›
  2) å­˜åœ¨â€œæ”¯æ’‘/ç½‘æ¶â€æ—¶ï¼Œå…ˆé€‰æ‹©åˆ†æ¡¶ç­–ç•¥ï¼š1=æŒ‰ç¼–å·ï¼Œ2=æŒ‰æ¥¼å±‚ï¼›
  3) ï¼ˆä»…ç½‘æ¶ï¼‰ä¸ºå„å­ç±»å½•å…¥ç¼–å·èŒƒå›´ï¼šå›è½¦=æ²¿ç”¨ä¸Šæ¬¡ï¼›*=å…¨éƒ¨ï¼›lk=ç•™ç©ºä¸æ¥æ”¶ï¼›
  4) è¾“å…¥ 1â€“10 ä¸ªæ—¥æœŸï¼Œç¨‹åºè‡ªåŠ¨å»é‡å¹¶è§„èŒƒåŒ–æ ¼å¼ï¼›
  5) å†²çªå¤„ç†ï¼šå›è½¦=â€œåé¢çš„æ—¥æœŸä¼˜å…ˆâ€ï¼ˆé»˜è®¤ï¼‰ï¼Œn=â€œå‰é¢çš„æ—¥æœŸä¼˜å…ˆâ€ï¼›
  6) é¢„è§ˆåˆ†é…ï¼šå›è½¦=ç¡®è®¤ç”Ÿæˆï¼Œn=å–æ¶ˆï¼Œa=å°†æœªåˆ†é…å¹¶å…¥æœ€åä¸€å¤©ï¼›
  7) æ‰§è¡Œå‡ºè¡¨ï¼šæ‰¹é‡å†™å…¥é¡µæ± åŠå…ƒä¿¡æ¯ï¼ˆæ—¥æœŸï¼‰ã€‚

è¦ç‚¹è¯´æ˜
  â€¢ Î¼ åˆ¤å®šä¾æ®â€œâ‰¥4 ä½æ•°å­—â€æˆ–â€œç»å¯¹å€¼â‰¥1000â€çš„è¯»æ•°ï¼›åŒæ¡¶å†…â€œæ™®é€šåœ¨å‰ã€Î¼ åœ¨åâ€ï¼Œè·¨æ¡¶æµæ°´å·è¿ç»­ã€‚
  â€¢ æ”¯æ’‘/ç½‘æ¶æŒ‰æ‰€é€‰ç­–ç•¥å‚ä¸æ’åºï¼Œå’Œé’¢æŸ±/é’¢æ¢å¹¶è¡Œå¤„ç†ã€‚

è¿”å›/é€€å‡ºï¼šä»»æ„æ­¥éª¤è¾“å…¥ q è¿”å›ä¸Šä¸€æ­¥ï¼›ä»…è·¯å¾„è¾“å…¥ç•Œé¢è¾“å…¥å¤§å†™ Q é€€å‡ºã€‚
=====================================================================
""",

    "2": r"""====================  Mode 2 | æŒ‰æ¥¼å±‚æ–­ç‚¹ï¼ˆæŒ‰å±‚å‡ºæŠ¥ï¼‰  ====================

é€‚ç”¨åœºæ™¯
  é€šè¿‡æ–­ç‚¹å°†æ¥¼å±‚åˆ’åˆ†ä¸ºå¤šä¸ªâ€œæ¥¼å±‚æ¡¶â€ï¼ˆå¦‚ 1Fâ€“3Fã€4Fâ€“6Fã€B3â€“B1ã€æœºæˆ¿å±‚ã€å±‹é¢ï¼‰ï¼Œ
  æ¯ä¸ªæ¥¼å±‚æ¡¶å¯¹åº”ä¸€ä¸ªæ—¥æœŸï¼ˆå¯ç›¸åŒï¼‰

æ“ä½œæµç¨‹
  1) é€‰æ‹©æ¨¡å¼ï¼šè¾“å…¥ 2ï¼›
  2) å¦‚å­˜åœ¨â€œæ”¯æ’‘/ç½‘æ¶â€ï¼Œå…ˆé€‰æ‹©åˆ†æ¡¶ç­–ç•¥ï¼ˆ1=ç¼–å· / 2=æ¥¼å±‚ï¼‰ï¼›
  3) å½•å…¥æ–­ç‚¹ï¼ˆå¦‚â€œ5 10â€ï¼‰ï¼Œæ’åºè§„åˆ™å›ºå®šä¸ºâ€œB* â†’ 1Fâ†‘ â†’ æœºæˆ¿å±‚ â†’ å±‹é¢â€ï¼›
  4) ä¸ºæ¯ä¸ªæ¡¶è®¾ç½®æ—¥æœŸï¼›
  5) é¢„è§ˆå¹¶ç¡®è®¤ï¼Œæ‰§è¡Œå‡ºè¡¨ã€‚

è¦ç‚¹è¯´æ˜
  â€¢ Î¼/æ™®é€šåˆ†æµåŠç¼–å·è§„åˆ™ä¸å…¨å±€ä¸€è‡´ï¼›Sheet å‘½åä¸åŒ…å«æ—¥æœŸ/æ¥¼å±‚ä¿¡æ¯ã€‚

è¿”å›/é€€å‡ºï¼šä»»æ„æ­¥éª¤è¾“å…¥ q è¿”å›ä¸Šä¸€æ­¥ï¼›ä»…è·¯å¾„è¾“å…¥ç•Œé¢è¾“å…¥å¤§å†™ Q é€€å‡ºã€‚
=====================================================================
""",

    "3": r"""====================  Mode 3 | å•æ—¥æ¨¡å¼ï¼ˆå¿«é€Ÿåˆ¶è¡¨ï¼‰  ====================

é€‚ç”¨åœºæ™¯
  å…¨é‡æ•°æ®åŒä¸€æ—¥æœŸå‡ºæŠ¥ï¼Œæˆ–éœ€è¦å¿«é€Ÿç”Ÿæˆæˆè¡¨ã€‚

æ“ä½œæµç¨‹
  1) é€‰æ‹©æ¨¡å¼ï¼šè¾“å…¥ 3ï¼›
  2) å¦‚å­˜åœ¨â€œæ”¯æ’‘/ç½‘æ¶â€ï¼Œå…ˆé€‰æ‹©åˆ†æ¡¶ç­–ç•¥ï¼ˆ1=ç¼–å· / 2=æ¥¼å±‚ï¼‰ï¼›
  3) ç¨‹åºæŒ‰â€œæ¯é¡µ 5 ç»„ Ã— æ¯ç»„ 5 è¡Œâ€è‡ªåŠ¨åˆ†é¡µï¼Œå†™å…¥é¡µæ± ä¸å…ƒä¿¡æ¯ã€‚

è¦ç‚¹è¯´æ˜
  â€¢ Î¼ åˆ¤å®šä¸ç¼–å·è§„åˆ™åŒå…¨å±€ï¼›åŒæ¡¶â€œæ™®é€šå…ˆã€Î¼åâ€ï¼Œæµæ°´å·è·¨æ¡¶è¿ç»­ã€‚
  â€¢ æ”¯æ’‘/ç½‘æ¶éµå¾ªæ‰€é€‰ç­–ç•¥å¹¶å…¥æ•´ä½“æ’åºã€‚

è¿”å›/é€€å‡ºï¼šä»»æ„æ­¥éª¤è¾“å…¥ q è¿”å›ä¸Šä¸€æ­¥ï¼›ä»…è·¯å¾„è¾“å…¥ç•Œé¢è¾“å…¥å¤§å†™ Q é€€å‡ºã€‚
=====================================================================
""",

    "4": r"""================  Mode 4 | æ¥¼å±‚ Ã— æ—¥æœŸ åˆ‡ç‰‡ï¼ˆå‡åˆ† / é…é¢ï¼‰  ================

é€‚ç”¨åœºæ™¯
  åŒä¸€æ¥¼å±‚éœ€è¦åˆ†é…åˆ°å¤šå¤©ï¼›æ”¯æŒâ€œå‡åˆ†â€æˆ–â€œæ¯æ—¥ä¸Šé™ï¼ˆé…é¢ï¼‰â€ã€‚

æ ¸å¿ƒæ¦‚å¿µ
  â€¢ å…±ç”¨è®¡åˆ’ï¼šä¸ºä¸€æ‰¹æ¥¼å±‚é…ç½®åŒä¸€ç»„æ—¥æœŸä¸æ¯æ—¥ä¸Šé™ï¼ˆç•™ç©ºä¸Šé™=å‡åˆ†ï¼‰ï¼›
  â€¢ é»˜è®¤è®¡åˆ’ï¼ˆ*ï¼‰ï¼šå¯¹â€œæœªå•ç‹¬é…ç½®â€çš„æ¥¼å±‚ç”Ÿæ•ˆçš„å…œåº•æ–¹æ¡ˆï¼›
  â€¢ è‹¥ä»æœ‰å‰©ä½™ï¼šå¯ç»Ÿä¸€æŒ‡å®šæ—¥æœŸ/æ¸©åº¦ï¼Œæˆ–è¿”å› Mode 1 ç»§ç»­åˆ†æ¡¶ã€‚

æ“ä½œæµç¨‹
  1) é€‰æ‹©æ¨¡å¼ï¼šè¾“å…¥ 4ï¼›
  2) é€‰æ‹©æœ¬æ¬¡æ¶‰åŠçš„æ¥¼å±‚ï¼ˆå›è½¦=å…¨éƒ¨è¯†åˆ«åˆ°çš„æ¥¼å±‚ï¼›æ”¯æŒ B2/5F/æœºæˆ¿å±‚/å±‹é¢ç­‰æ ‡ç­¾ï¼‰ï¼›
  3) æ˜¯å¦é‡‡ç”¨â€œå…±ç”¨è®¡åˆ’â€ï¼šy=å¯¹æ‰€é€‰æ¥¼å±‚å…±ç”¨ï¼›å›è½¦=é€æ¥¼å±‚è®¾ç½®ï¼›
  4) å¦‚ä»æœ‰æœªé…ç½®æ¥¼å±‚ï¼šå¯åˆ›å»ºé»˜è®¤è®¡åˆ’ï¼ˆ*ï¼‰ï¼›
  5) æ‰§è¡Œåˆ‡ç‰‡ã€åˆ†é¡µä¸å†™å…¥ï¼›å¿…è¦æ—¶é€‰æ‹©å…œåº•ç­–ç•¥ã€‚

è¦ç‚¹è¯´æ˜
  â€¢ æ¥¼å±‚æ’åºåŠ Î¼/æ™®é€šåˆ†æµã€ç¼–å·è§„åˆ™ç»§æ‰¿å…¨å±€é€»è¾‘ã€‚
  â€¢ ç½‘æ¶/æ”¯æ’‘ç»§ç»­æŒ‰æ‰€é€‰ç­–ç•¥å‚ä¸åˆ‡ç‰‡å’Œå‡ºè¡¨ã€‚

è¿”å›/é€€å‡ºï¼šä»»æ„æ­¥éª¤è¾“å…¥ q è¿”å›ä¸Šä¸€æ­¥ï¼›ä»…è·¯å¾„è¾“å…¥ç•Œé¢è¾“å…¥å¤§å†™ Q é€€å‡ºã€‚
=====================================================================
""",
}





def tutorial_browser():
    """æ˜¾ç¤ºæ¨¡å¼æ•™ç¨‹æµè§ˆå™¨ã€‚"""
    print(HELP_HOME)
    viewed = False
    while True:
        prompt = "è¿˜è¦æŸ¥çœ‹å…¶ä»–æ¨¡å¼ï¼Ÿè¾“å…¥ 1/2/3/4ï¼Œå›è½¦æˆ– q è¿”å›ã€‚\nâ†’ " if viewed else "æŸ¥çœ‹å“ªä¸ªæ¨¡å¼ï¼Ÿè¾“å…¥ 1/2/3/4ï¼Œå›è½¦æˆ– q è¿”å›è·¯å¾„è¾“å…¥ã€‚\nâ†’ "
        sel = input(prompt).strip()
        if sel in ("", "q"):
            return
        if sel in HELP_TEXTS:
            print(HELP_TEXTS[sel])
            viewed = True
        else:
            print("ä»…æ¥å— 1/2/3/4 æˆ–å›è½¦/qã€‚")


def prompt_path(prompt, default: Path) -> Path:
    """
    äº¤äº’å¼è·å–ç”¨æˆ·è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼ŒéªŒè¯æ–‡ä»¶å­˜åœ¨æ€§å¹¶è¿”å›æœ‰æ•ˆè·¯å¾„ã€‚

    æç¤ºç”¨æˆ·è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼Œæ”¯æŒç›´æ¥å›è½¦ä½¿ç”¨é»˜è®¤è·¯å¾„ï¼›è‡ªåŠ¨å¤„ç†è·¯å¾„ä¸­çš„å¼•å·ï¼›
    è‹¥è¾“å…¥è·¯å¾„æ— æ•ˆï¼ˆæ–‡ä»¶ä¸å­˜åœ¨ï¼‰ï¼Œåˆ™æ˜¾ç¤ºé”™è¯¯æç¤ºå¹¶é‡æ–°è¯·æ±‚è¾“å…¥ï¼Œç¡®ä¿è¿”å›æœ‰æ•ˆæ–‡ä»¶è·¯å¾„ã€‚

    Args:
        prompt: è·¯å¾„è¾“å…¥æç¤ºä¿¡æ¯ï¼ˆstrï¼‰
        default: é»˜è®¤æ–‡ä»¶è·¯å¾„ï¼ˆPathå¯¹è±¡ï¼‰
    Returns:
        Path: ç»è¿‡éªŒè¯çš„æœ‰æ•ˆæ–‡ä»¶è·¯å¾„
    """
    while True:
        raw = ask(f"{prompt}ï¼ˆå›è½¦é»˜è®¤ï¼š{default}ï¼‰")
        if raw.lower() == "help":
            tutorial_browser()
            continue
        p = Path(raw.strip('"')) if raw else default
        if p.exists() and p.is_file():
            return p
        print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶ï¼š{p}")


def prompt_floor_breaks(label: str):
    """
    äº¤äº’å¼è·å–æ¥¼å±‚æ–­ç‚¹åˆ—è¡¨ï¼Œæ”¯æŒæ— æ•ˆè¾“å…¥å¹¶è¿”å›ç©ºå€¼å¤„ç†ã€‚

    æç¤ºç”¨æˆ·è¾“å…¥ç©ºæ ¼åˆ†éš”çš„æ¥¼å±‚æ–­ç‚¹ï¼ˆå¦‚"5 10"ï¼‰ï¼Œæ”¯æŒç›´æ¥å›è½¦è¡¨ç¤ºä¸åˆ†æ®µï¼›
    è‡ªåŠ¨è¿‡æ»¤é‡å¤å€¼å¹¶æŒ‰å‡åºæ’åºï¼›è‹¥è¾“å…¥æ— æ•ˆï¼ˆéæ•°å­—ï¼‰åˆ™è¿”å›ç©ºåˆ—è¡¨ã€‚

    Args:
        label: æç¤ºä¿¡æ¯å‰ç¼€ï¼ˆstrï¼‰
    Returns:
        list[int]: æ’åºåçš„æ¥¼å±‚æ–­ç‚¹åˆ—è¡¨ï¼ˆç©ºåˆ—è¡¨è¡¨ç¤ºä¸åˆ†æ®µï¼‰
    """
    txt = ask(f"{label} æ–­ç‚¹æ¥¼å±‚ï¼ˆç©ºæ ¼åˆ†éš”ï¼Œå¦‚ 5 10ï¼›å›è½¦=ä¸åˆ†æ®µï¼‰ï¼š")
    if not txt: return []
    try:
        return sorted({int(x) for x in txt.split()})
    except:
        return []


# ===== æ—¥æœŸåˆ†æ¡¶ï¼ˆæ³›åŒ–åˆ°ä»»æ„ç±»åˆ«ï¼‰ =====
import re
from typing import List, Tuple

def _parse_int_ranges(expr: str) -> List[Tuple[int, int]]:
    """
    å°†æ¥¼å±‚é€‰æ‹©è¡¨è¾¾å¼è§£æä¸ºä¸€ç»„é—­åŒºé—´ [(lo, hi), ...]ã€‚

    çº¦å®šï¼ˆä¸éœ€æ±‚å¯¹é½ï¼‰ï¼š
      - ç•™ç©ºï¼ˆç©ºå­—ç¬¦ä¸²/åªç©ºç™½ï¼‰ => **ä¸è¦ä»»ä½•æ¥¼å±‚**ï¼ˆè¿”å›ä¸€ä¸ªæ°¸ä¸å‘½ä¸­çš„â€œç©ºé›†å“¨å…µâ€åŒºé—´ï¼‰
      - '*'ï¼ˆæˆ–å…¨è§’'ï¼Š'ï¼‰      => **å…¨éƒ½è¦**ï¼ˆè¿”å› []ï¼Œè®©ä¸Šæ¸¸çš„ç©ºåˆ—è¡¨=å…¨åŒ…å«é€»è¾‘ç”Ÿæ•ˆï¼‰

    æ”¯æŒï¼š
      - æ•°å­—ï¼š'3'
      - èŒƒå›´ï¼š'1-10'ï¼ˆè¿å­—ç¬¦å¯ä¸º - â€” â€“ ~ è‡³ åˆ°ï¼‰
      - ç‰¹æ®Šæ¥¼å±‚ï¼šæœºæˆ¿/æœºæˆ¿å±‚/JF/jfï¼Œå±‹é¢/å±‹é¡¶å±‚/é¡¶å±‚/WM/wm/roof
      - æ··åˆï¼š'10-æœºæˆ¿'ã€'æœºæˆ¿-å±‹é¢'ã€'8-å±‹é¢'
      - å¤šåˆ†éš”ï¼šç©ºæ ¼/è‹±æ–‡é€—å·/ä¸­æ–‡é€—å·/é¡¿å·/åˆ†å·ç­‰
    """
    text = (expr or "").strip()

    # â€”â€” è§„åˆ™ï¼šç•™ç©º = ä¸è¦ä»»ä½•æ¥¼å±‚ï¼ˆè¿”å›ä¸€ä¸ªæ°¸ä¸å‘½ä¸­çš„åŒºé—´ï¼Œé¿å…ä¸Šæ¸¸æŠŠç©ºå½“â€œå…¨åŒ…å«â€ï¼‰â€”â€”
    if not text:
        return [(1, 0)]  # lo > hiï¼Œä»»ä½•æ¥¼å±‚éƒ½ä¸ä¼šå‘½ä¸­ï¼›ä¸”åˆ—è¡¨éç©ºï¼Œä¸ä¼šè§¦å‘â€œå…¨åŒ…å«â€

    # '*' = å…¨éƒ½è¦ï¼šä¿æŒç©ºåˆ—è¡¨ï¼Œè®©ä¸Šæ¸¸çš„â€œç©º=å…¨åŒ…å«â€ç”Ÿæ•ˆ
    if text in ("*", "ï¼Š"):
        return []

    # å…ˆåš NFKC è§„èŒƒåŒ– + ç»Ÿä¸€è¿å­—ç¬¦
    text = unicodedata.normalize("NFKC", text)
    # æŠŠå„ç§â€œçœ‹èµ·æ¥åƒè¿å­—ç¬¦/æ³¢æµªçº¿/ä¸­æ–‡è‡³åˆ°â€ç»Ÿä¸€æˆ '-'
    text = re.sub(r"[ï¼â€”â€“âˆ’~ï½ã€œè‡³åˆ°]", "-", text)

    # ***** ç‰¹æ®Šæ¥¼å±‚æ˜ å°„ *****
    JF_VAL = 10**6 - 1    # æœºæˆ¿
    WM_VAL = 10**6        # å±‹é¢/å±‹é¡¶å±‚/é¡¶å±‚
    SPECIAL_MAP = {
        "æœºæˆ¿": JF_VAL, "æœºæˆ¿å±‚": JF_VAL, "jf": JF_VAL,
        "å±‹é¢": WM_VAL, "å±‹é¡¶å±‚": WM_VAL, "é¡¶å±‚": WM_VAL,
        "wm": WM_VAL, "roof": WM_VAL,
    }

    def norm_token(tok: str) -> str:
        return tok.strip().lower()

    # åˆ†è¯ï¼šç©ºæ ¼ã€è‹±æ–‡/ä¸­æ–‡é€—å·ã€é¡¿å·ã€åˆ†å·
    tokens = [t for t in re.split(r"[,\uFF0C\u3001;\uFF1B\s]+", text) if t.strip()]

    ranges: List[Tuple[int, int]] = []

    # å„ç±»æ­£åˆ™
    re_int = re.compile(r"^\s*\d+\s*$")
    # ç»Ÿä¸€ååªéœ€è¦åŒ¹é… '-'
    re_num_num = re.compile(r"^\s*(\d+)\s*-\s*(\d+)\s*$")
    re_a_sp   = re.compile(r"^\s*(\d+)\s*-\s*([^\d\s]+)\s*$")
    re_sp_b   = re.compile(r"^\s*([^\d\s]+)\s*-\s*(\d+)\s*$")
    re_sp_sp  = re.compile(r"^\s*([^\d\s]+)\s*-\s*([^\d\s]+)\s*$")

    def sp_val(s: str):
        key = norm_token(s).replace("ï¼ˆ", "(").replace("ï¼‰", ")")
        return SPECIAL_MAP.get(key)

    for raw in tokens:
        tok = raw.strip()
        if not tok:
            continue

        # å•ä¸ªæ•°å­—
        if re_int.match(tok):
            v = int(tok)
            ranges.append((v, v))
            continue

        # æ•°å­—-æ•°å­—
        m = re_num_num.match(tok)
        if m:
            a, b = int(m.group(1)), int(m.group(2))
            if a > b:
                a, b = b, a
            ranges.append((a, b))
            continue

        # æ•°å­—-ç‰¹æ®Š
        m = re_a_sp.match(tok)
        if m:
            a = int(m.group(1))
            rb = sp_val(m.group(2))
            if rb is not None:
                lo, hi = (a, rb) if a <= rb else (rb, a)
                ranges.append((lo, hi))
                continue

        # ç‰¹æ®Š-æ•°å­—
        m = re_sp_b.match(tok)
        if m:
            la = sp_val(m.group(1))
            b = int(m.group(2))
            if la is not None:
                lo, hi = (la, b) if la <= b else (b, la)
                ranges.append((lo, hi))
                continue

        # ç‰¹æ®Š-ç‰¹æ®Š
        m = re_sp_sp.match(tok)
        if m:
            la, lb = sp_val(m.group(1)), sp_val(m.group(2))
            if la is not None and lb is not None:
                lo, hi = (la, lb) if la <= lb else (lb, la)
                ranges.append((lo, hi))
                continue

        # å•ä¸ªç‰¹æ®Šè¯
        sv = sp_val(tok)
        if sv is not None:
            ranges.append((sv, sv))
            continue

        # æœªè¯†åˆ«ç‰‡æ®µï¼Œå‹å¥½æç¤ºï¼ˆä¸å½±å“è¿è¡Œï¼‰
        print(f"[hint] æœªè¯†åˆ«ç‰‡æ®µï¼š{raw}ï¼ˆå·²å¿½ç•¥ï¼Œä¸å‚ä¸æ¥¼å±‚ç­›é€‰ï¼‰")

    # å¦‚æœä»ç„¶ä»€ä¹ˆéƒ½æ²¡è§£æåˆ°ï¼šç»™ä¸€ä¸ªâ€œç©ºé›†å“¨å…µâ€ï¼Œé¿å…è¢«è¯¯è®¤ä¸ºâ€œå…¨åŒ…å«â€
    if not ranges:
        return [(1, 0)]

    # åˆå¹¶åŒºé—´
    ranges.sort(key=lambda x: (x[0], x[1]))
    merged: List[Tuple[int, int]] = []
    for lo, hi in ranges:
        if not merged:
            merged.append((lo, hi))
        else:
            mlo, mhi = merged[-1]
            if lo <= mhi:         # é‡å /ç›¸é‚»éƒ½å¹¶
                merged[-1] = (mlo, max(mhi, hi))
            else:
                merged.append((lo, hi))
    return merged



def parse_rule(text: str):
    """
    è§£ææ•°æ®åˆ†å‘è§„åˆ™å­—ç¬¦ä¸²ä¸ºç»“æ„åŒ–è§„åˆ™å­—å…¸ã€‚

    æ”¯æŒä¸¤ç§è§„åˆ™ç±»å‹ï¼š
    - å¯ç”¨æ‰€æœ‰æ•°æ®ï¼šè¾“å…¥â€œ*â€â€œallâ€â€œå…¨éƒ¨â€â€œæ‰€æœ‰â€æ—¶ï¼Œè¿”å›å¯ç”¨çŠ¶æ€ä¸”ç©ºèŒƒå›´ï¼ˆè¡¨ç¤ºæ¥æ”¶æ‰€æœ‰æ•°æ®ï¼‰
    - èŒƒå›´è§„åˆ™ï¼šå…¶ä»–è¾“å…¥è§£æä¸ºæ•´æ•°èŒƒå›´åˆ—è¡¨ï¼ˆé€šè¿‡_parse_int_rangeså¤„ç†ï¼‰

    Args:
        text: è§„åˆ™å­—ç¬¦ä¸²ï¼ˆå¦‚â€œ*â€â€œ1-3 5â€â€œå…¨éƒ¨â€ï¼‰
    Returns:
        dict: è§„åˆ™å­—å…¸ï¼ŒåŒ…å«ï¼š
            - enabled: æ˜¯å¦å¯ç”¨è¯¥è§„åˆ™ï¼ˆboolï¼‰
            - ranges: è§£æåçš„èŒƒå›´åˆ—è¡¨ï¼ˆlist[tuple[int, int]]ï¼Œç©ºåˆ—è¡¨è¡¨ç¤ºå…¨éƒ¨ï¼‰
    """
    s = (text or "").strip()
    if not s:
        return {"enabled": False, "ranges": []}
    if _is_explicit_all_token(s):
        return {"enabled": True, "ranges": [], "explicit_all": True}
    return {"enabled": True, "ranges": _parse_int_ranges(s)}


def _is_lk(s: str) -> bool:
    """å¤§å°å†™åŠå…¨è§’åŠè§’å‡è¯†åˆ« 'lk'ã€‚"""
    return unicodedata.normalize('NFKC', (s or '')).strip().lower() == 'lk'


_STAR_TOKENS = {"*", "å…¨éƒ¨", "æ‰€æœ‰"}


def _is_explicit_all_token(value) -> bool:
    """åˆ¤å®šè¾“å…¥æ˜¯å¦è¡¨ç¤ºæ˜¾å¼çš„â€œå…¨éƒ¨æ¥æ”¶â€ã€‚"""
    if not isinstance(value, str):
        return False
    token = unicodedata.normalize("NFKC", value or "").strip()
    if not token:
        return False
    if token in _STAR_TOKENS:
        return True
    return token.casefold() == "all"


def _in_ranges(val: int, ranges):
    """
    åˆ¤æ–­å€¼æ˜¯å¦åœ¨æŒ‡å®šçš„èŒƒå›´åˆ—è¡¨å†…ï¼Œæ”¯æŒç©ºèŒƒå›´è¡¨ç¤ºâ€œå…¨éƒ¨åŒ…å«â€ã€‚

    èŒƒå›´åˆ—è¡¨ä¸ºç©ºæ—¶é»˜è®¤åŒ…å«æ‰€æœ‰å€¼ï¼›å¦åˆ™æ£€æŸ¥å€¼æ˜¯å¦è½åœ¨ä»»ä¸€èŒƒå›´çš„é—­åŒºé—´å†…ã€‚

    Args:
        val: å¾…åˆ¤æ–­çš„æ•´æ•°ï¼ˆå¦‚æ¥¼å±‚å·ã€æ”¯æ’‘ç¼–å·ï¼‰
        ranges: èŒƒå›´å…ƒç»„åˆ—è¡¨ï¼ˆå¦‚[(1,3), (5,7)]ï¼‰ï¼Œç©ºåˆ—è¡¨è¡¨ç¤ºå…¨éƒ¨
    Returns:
        bool: åœ¨èŒƒå›´å†…è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
    """
    if ranges is None: return False
    if ranges == []: return True  # noqa
    for a, b in ranges:
        if a <= val <= b: return True
    return False


def net_part(name: str) -> str:
    """
    è¿”å› 'XX' / 'FG' / 'SX' / 'GEN'ï¼ˆæ³›ç§°ï¼‰ä¹‹ä¸€ï¼›å¤§å°å†™ä¸æ•æ„Ÿï¼Œå…¼å®¹ä¸­æ–‡åˆ«åã€‚
    """
    s = name.upper()
    # å½¢å¦‚ XX1 / XX-12 / XX_003ï¼Œæˆ–ä¸­æ–‡åˆ«å
    if re.search(r"(?<![A-Z0-9])XX(?=[-_]?\d+)|ä¸‹\s*å¼¦", s):
        return "XX"
    if re.search(r"(?<![A-Z0-9])FG(?=[-_]?\d+)|è…¹\s*æ†", s):
        return "FG"
    if re.search(r"(?<![A-Z0-9])SX(?=[-_]?\d+)|ä¸Š\s*å¼¦", s):
        return "SX"
    if re.search(r"\bWJ\b|ç½‘\s*æ¶|SPACE\s*FRAME|GRID", s):
        return "GEN"
    return "GEN"


def _net_no(name: str):
    """
    ä»ç½‘æ¶æ„ä»¶åé‡Œæå–ç¼–å·ï¼ˆXX12 / FG-03 / SX_7 / ç½‘æ¶-15 ç­‰ï¼‰ã€‚

    ä»…åœ¨æ˜ç¡®å‰ç¼€æˆ–æ³›ç§°å­˜åœ¨æ—¶æ‰è§£æç¼–å·ï¼Œé¿å…è¯¯åƒå…¶ä»–æ•°å­—ã€‚
    """
    s = name.upper()
    part = net_part(name)
    if part in ("XX", "FG", "SX"):
        m = re.search(rf"{part}\s*[-_]?(\d+)", s)
        return int(m.group(1)) if m else None
    m = re.search(r"(?:WJ|ç½‘æ¶|SPACE\s*FRAME|GRID)\s*[-_]?(\d+)", s)
    return int(m.group(1)) if m else None


def _wz_no(name: str):
    """
    ä»æ”¯æ’‘æ„ä»¶åç§°ä¸­æå–ç¼–å·ï¼ˆå¦‚ä»â€œWZ3â€â€œæ”¯æ’‘-5â€ä¸­æå–3ã€5ï¼‰ã€‚

æ”¯æŒå…³é”®è¯åŒ¹é…ï¼š
- å«â€œWZâ€æˆ–â€œZCâ€å‰ç¼€ï¼ˆå¦‚â€œWZ12â€â€œZC-8â€ï¼‰
- å«â€œæ”¯æ’‘â€å…³é”®è¯ï¼ˆå¦‚â€œæ”¯æ’‘6â€â€œæ–œæ’‘-3â€ï¼‰
æå–å¤±è´¥æ—¶è¿”å›Noneã€‚

Args:
    name: æ”¯æ’‘æ„ä»¶åç§°å­—ç¬¦ä¸²ï¼ˆå¦‚â€œWZ5â€â€œæ”¯æ’‘-10â€ï¼‰
Returns:
    int | None: æå–çš„ç¼–å·ï¼Œå¤±è´¥åˆ™è¿”å›None
"""
    m = re.search(r"(?i)\b(?:WZ|ZC)\s*[-â€“â€”]?\s*(\d+)\b", name)
    if m: return int(m.group(1))
    m = re.search(r"æ”¯æ’‘\s*[-â€“â€”]?\s*(\d+)", name)
    return int(m.group(1)) if m else None


def _match_keywords(name: str, kws):
    """
    åˆ¤æ–­æ„ä»¶åç§°æ˜¯å¦åŒ…å«ä»»æ„å…³é”®è¯ï¼ˆå¿½ç•¥å¤§å°å†™ï¼‰ã€‚

    å…³é”®è¯åˆ—è¡¨ä¸ºç©ºæ—¶é»˜è®¤åŒ¹é…æ‰€æœ‰åç§°ï¼›å¦åˆ™æ£€æŸ¥åç§°æ˜¯å¦å«ä»»ä¸€å…³é”®è¯ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰ã€‚

    Args:
        name: æ„ä»¶åç§°å­—ç¬¦ä¸²
        kws: å…³é”®è¯åˆ—è¡¨ï¼ˆå¦‚["3å±‚", "ä¸œç«‹é¢"]ï¼‰
    Returns:
        bool: åŒ…å«ä»»ä¸€å…³é”®è¯è¿”å›Trueï¼Œå¦åˆ™è¿”å›Falseï¼ˆå…³é”®è¯ä¸ºç©ºæ—¶è¿”å›Trueï¼‰
    """
    if not kws: return True
    s = name.lower()
    return any(k.lower() in s for k in kws)


def prompt_mode():
    """æ¨¡å¼é€‰æ‹©ï¼Œæ”¯æŒ q è¿”å›ã€‚"""
    txt = ask("æ¨¡å¼é€‰æ‹©ï¼š1) æŒ‰æ—¥æœŸåˆ†æ¡¶  2) æŒ‰æ¥¼å±‚æ–­ç‚¹  3) å•æ—¥æ¨¡å¼  4) æ¥¼å±‚+æ—¥æœŸé…é¢")
    if txt in ("", "1"):
        return "1"
    if txt in ("2", "3", "4"):
        return txt
    return "1"


def prompt_bucket_priority():
    """è¯¢é—®è§„åˆ™é‡å ä¼˜å…ˆçº§ã€‚"""
    ans = ask("è§„åˆ™é‡å å°†æŒ‰ã€åé¢çš„å¤©ã€‘ä¼˜å…ˆå¹¶è‡ªåŠ¨åšå·®ï¼ˆå›è½¦=æ˜¯ / n=å¦ï¼‰ï¼š", lower=True)
    return ans != 'n'


def prompt_later_priority():
    """ä¾›é€‚é…å±‚è¦†ç›–çš„â€œåæ¡¶ä¼˜å…ˆâ€è¯¢é—®æ¥å£ã€‚"""
    return prompt_bucket_priority()


def prompt_auto_merge_remains(*_, **__):
    """ä¾›é€‚é…å±‚è¦†ç›–çš„â€œæ˜¯å¦è‡ªåŠ¨å¹¶å…¥å‰©ä½™æ„ä»¶â€æ¥å£ã€‚é»˜è®¤è¿”å› ``None``ã€‚"""
    return None


def prompt_keywords_for_bucket(*_, **__):
    """ä¾›é€‚é…å±‚è¦†ç›–çš„å…³é”®è¯è¾“å…¥æ¥å£ã€‚é»˜è®¤è¿”å› ``None`` è¡¨ç¤ºç»§ç»­äº¤äº’è¯¢é—®ã€‚"""
    return None


def prompt_support_strategy_for_bucket():
    """åœ¨éœ€è¦æ”¯æ’‘åˆ†æ¡¶ç­–ç•¥æ—¶è¯¢é—®ä¸€æ¬¡ã€‚"""
    global support_bucket_strategy
    if support_bucket_strategy is None:
        ans = ask("æ”¯æ’‘åˆ†æ¡¶æ–¹å¼ï¼š1) æŒ‰ç¼–å· 2) æŒ‰æ¥¼å±‚ï¼ˆå›è½¦=1ï¼‰")
        support_bucket_strategy = "floor" if ans == "2" else "number"
    return support_bucket_strategy


def prompt_net_strategy_for_bucket():
    """åœ¨éœ€è¦ç½‘æ¶åˆ†æ¡¶ç­–ç•¥æ—¶è¯¢é—®ä¸€æ¬¡ã€‚"""
    global net_bucket_strategy
    if net_bucket_strategy is None:
        ans = ask("ç½‘æ¶åˆ†æ¡¶æ–¹å¼ï¼š1) æŒ‰ç¼–å·  2) æŒ‰æ¥¼å±‚ï¼ˆå›è½¦=1ï¼‰")
        net_bucket_strategy = "floor" if ans == "2" else "number"
    return net_bucket_strategy


def detect_net_parts_for_category(grouped, cat="ç½‘æ¶"):
    """æ£€æµ‹æœ¬æ¬¡è¿è¡Œå®é™…å‡ºç°çš„ç½‘æ¶å­ç±»é›†åˆã€‚"""
    parts = set()
    for g in grouped.get(cat, []):
        parts.add(net_part(g["name"]))
    return parts or {"GEN"}


def prompt_date_buckets(categories_present, grouped):
    """
    äº¤äº’å¼æ”¶é›†æ—¥æœŸæ¡¶é…ç½®ï¼Œæ”¯æŒ1-10å¤©çš„æ£€æµ‹æ•°æ®åˆ†å‘è§„åˆ™ã€‚

    ä¸ºæ¯å¤©é…ç½®ï¼š
    - æ—¥æœŸï¼ˆè‡ªåŠ¨æ ‡å‡†åŒ–ä¸ºâ€œYYYYå¹´MMæœˆDDæ—¥â€ï¼‰
    - ç¯å¢ƒæ¸©åº¦ï¼ˆè‡ªåŠ¨æ ‡å‡†åŒ–ä¸ºâ€œXâ„ƒâ€ï¼‰
    - å„æ„ä»¶ç±»å‹çš„æ¥æ”¶è§„åˆ™ï¼ˆæ¥¼å±‚/ç¼–å·èŒƒå›´ï¼‰
    - å…³é”®è¯ç­›é€‰ï¼ˆå¯é€‰ï¼‰

    Args:
        categories_present: å­˜åœ¨çš„æ„ä»¶ç±»å‹åˆ—è¡¨ï¼ˆå¦‚["é’¢æŸ±", "æ”¯æ’‘"]ï¼‰
        grouped: æŒ‰ç±»å‹åˆ†ç»„çš„æ„ä»¶æ•°æ®ï¼Œç”¨äºæ£€æµ‹ç½‘æ¶å­ç±»
    Returns:
        list[dict]: æ—¥æœŸæ¡¶é…ç½®åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ å«æ—¥æœŸã€ç¯å¢ƒã€è§„åˆ™ç­‰ä¿¡æ¯
    """
    while True:
        n_txt = ask("å…±æœ‰å‡ å¤©ï¼ˆ1-10ï¼Œå›è½¦=1ï¼‰ï¼š")
        if not n_txt: n = 1; break
        if n_txt.isdigit() and 1 <= int(n_txt) <= 10:
            n = int(n_txt);
            break
        print("è¯·è¾“å…¥ 1-10 ä¹‹é—´çš„æ•´æ•°ã€‚")
    buckets = []

    # é¢„å…ˆæ£€æµ‹ç½‘æ¶å­ç±»ï¼Œå‘ç”¨æˆ·æç¤ºå‡ºç°çš„åç§°
    net_parts_present = set()
    if "ç½‘æ¶" in categories_present:
        net_parts_present = detect_net_parts_for_category(grouped, "ç½‘æ¶")
        if net_parts_present:
            name_map = {"XX": "XX", "FG": "FG", "SX": "SX", "GEN": "æ³›ç§°"}
            pretty = "ã€".join(name_map.get(p, p) for p in sorted(net_parts_present))
            print(f"ğŸ•¸ æœ¬æ¬¡è¯†åˆ«åˆ°çš„ç½‘æ¶åç§°ï¼š{pretty}")

    for i in range(1, n + 1):
        print(f"\nâ€”â€” ç¬¬ {i} å¤© â€”â€”")
        d = ask("ğŸ“… æ—¥æœŸï¼ˆ20250101 / 2025å¹´1æœˆ1æ—¥ / 2025 1 1 / 2025.1.1 / 2025-1-1 / 1-1 / 01-01ï¼‰ï¼š")

        rules = {}
        for cat in categories_present:
            if cat == "æ”¯æ’‘":
                prompt_support_strategy_for_bucket()
                if support_bucket_strategy == "floor":
                    txt = ask("ğŸ¦¾ æ”¯æ’‘ æ¥¼å±‚è§„åˆ™ï¼ˆä¾‹ï¼š1-3 5 7-10 å±‹é¢ï¼›ç•™ç©º=ä¸æ¥æ”¶ï¼›*=ä¸é™ï¼‰ï¼š")
                else:
                    txt = ask("ğŸ¦¾ æ”¯æ’‘ ç¼–å·èŒƒå›´ï¼ˆä¾‹ï¼š1-12 20-25ï¼›ç•™ç©º=ä¸æ¥æ”¶ï¼›*=ä¸é™ï¼‰ï¼š")
                rules[cat] = parse_rule(txt)
            elif cat == "ç½‘æ¶":
                prompt_net_strategy_for_bucket()
                present_parts = net_parts_present
                sub_rules = {}
                if net_bucket_strategy == "number":
                    prev_rule = None
                    for part in sorted(present_parts - {"GEN"}):
                        placeholder = "åŒä¸Š" if prev_rule else "ä¸æ¥æ”¶"
                        txt = ask(f"ğŸ•¸ ç½‘æ¶-{part} ç¼–å·èŒƒå›´ï¼ˆä¾‹ï¼š1-12 20-25ï¼›ç•™ç©º={placeholder}ï¼›*=æ‰€æœ‰ï¼›lk=ä¸æ¥æ”¶ï¼‰ï¼š")
                        if _is_lk(txt):
                            sub_rules[part] = {"enabled": False, "ranges": []}
                        elif txt == "":
                            sub_rules[part] = prev_rule or {"enabled": False, "ranges": []}
                        else:
                            sub_rules[part] = parse_rule(txt)
                        print(f"âœ… å·²è®¾ç½® ç½‘æ¶-{part}: {sub_rules[part]}")
                        prev_rule = sub_rules[part] if txt != "" else prev_rule
                    if "GEN" in present_parts:
                        placeholder = "åŒä¸Š" if prev_rule else "ä¸æ¥æ”¶"
                        txt = ask(f"ğŸ•¸ ç½‘æ¶-æ³›ç§° ç¼–å·èŒƒå›´ï¼ˆç•™ç©º={placeholder}ï¼›*=æ‰€æœ‰ï¼›lk=ä¸æ¥æ”¶ï¼‰ï¼š")
                        if _is_lk(txt):
                            sub_rules["GEN"] = {"enabled": False, "ranges": []}
                        elif txt == "":
                            sub_rules["GEN"] = prev_rule or {"enabled": False, "ranges": []}
                        else:
                            sub_rules["GEN"] = parse_rule(txt)
                        print(f"âœ… å·²è®¾ç½® ç½‘æ¶-æ³›ç§°: {sub_rules['GEN']}")
                        prev_rule = sub_rules["GEN"] if txt != "" else prev_rule
                else:
                    for part in sorted(present_parts - {"GEN"}):
                        txt = ask(f"ğŸ•¸ ç½‘æ¶-{part} æ¥¼å±‚è§„åˆ™ï¼ˆä¾‹ï¼š1-3 5 7-10 å±‹é¢ï¼›ç•™ç©º=ä¸æ¥æ”¶ï¼›*=ä¸é™ï¼›lk=ä¸æ¥æ”¶ï¼‰ï¼š")
                        if _is_lk(txt):
                            rule = {"enabled": False, "ranges": []}
                        else:
                            rule = parse_rule(txt)
                        sub_rules[part] = rule
                        print(f"âœ… å·²è®¾ç½® ç½‘æ¶-{part}: {rule}")
                    if "GEN" in present_parts:
                        txt = ask("ğŸ•¸ ç½‘æ¶-æ³›ç§° æ¥¼å±‚è§„åˆ™ï¼ˆç•™ç©º=ä¸æ¥æ”¶ï¼›*=ä¸é™ï¼›lk=ä¸æ¥æ”¶ï¼‰ï¼š")
                        if _is_lk(txt):
                            rule = {"enabled": False, "ranges": []}
                        else:
                            rule = parse_rule(txt)
                        sub_rules["GEN"] = rule
                        print(f"âœ… å·²è®¾ç½® ç½‘æ¶-æ³›ç§°: {rule}")
                rules[cat] = {"strategy": net_bucket_strategy, "parts": sub_rules}
            else:
                txt = ask(f"ğŸ— {cat} æ¥¼å±‚è§„åˆ™ï¼ˆä¾‹ï¼š1-3 5 7-10 å±‹é¢ï¼›ç•™ç©º=ä¸æ¥æ”¶ï¼›*=ä¸é™ï¼‰ï¼š")
                rules[cat] = parse_rule(txt)
        kws_prefilled = prompt_keywords_for_bucket(
            bucket_index=i - 1,
            rules=rules,
            categories_present=categories_present,
        )
        if kws_prefilled is None:
            kws_txt = ask("ğŸ” å…³é”®è¯ï¼ˆå¯å¤šä¸ªï¼Œç©ºæ ¼/é€—å·åˆ†éš”ï¼›ç•™ç©º=æ— éœ€ï¼‰ï¼š")
            kws = [k for k in re.split(r"[,\sï¼Œ]+", kws_txt) if k] if kws_txt else []
        else:
            if isinstance(kws_prefilled, str):
                kws = [k for k in re.split(r"[,\sï¼Œ]+", kws_prefilled) if k]
            else:
                kws = [str(k).strip() for k in kws_prefilled if str(k).strip()]
        buckets.append({
            "date_raw": d,
            "date": normalize_date(d) if d else "",
            "rules": rules,
            "kws": kws
        })
    return buckets


def assign_by_buckets(cat_groups: dict, buckets, later_priority=True):
    """
    å°†æ„ä»¶æ•°æ®ç»„æŒ‰æ—¥æœŸæ¡¶è§„åˆ™åˆ†é…åˆ°å¯¹åº”å¤©æ•°ï¼Œæ”¯æŒè§„åˆ™é‡å å¤„ç†ã€‚

    åˆ†é…é€»è¾‘ï¼š
    1. æŒ‰æ„ä»¶ç±»å‹éå†æ•°æ®ç»„
    2. æ ¹æ®æ—¥æœŸæ¡¶è§„åˆ™ï¼ˆæ¥¼å±‚/ç¼–å·èŒƒå›´+å…³é”®è¯ï¼‰åŒ¹é…æ•°æ®
    3. è§„åˆ™é‡å æ—¶æŒ‰â€œåå®šä¹‰æ¡¶ä¼˜å…ˆâ€ï¼ˆå¯é€šè¿‡å‚æ•°å…³é—­ï¼‰
    è¿”å›åˆ†é…ç»“æœå’ŒæœªåŒ¹é…çš„æ•°æ®ã€‚

    Args:
        cat_groups: æŒ‰ç±»å‹åˆ†ç»„çš„æ„ä»¶æ•°æ®ï¼ˆé”®ä¸ºç±»å‹ï¼Œå€¼ä¸ºæ•°æ®ç»„åˆ—è¡¨ï¼‰
        buckets: æ—¥æœŸæ¡¶é…ç½®åˆ—è¡¨ï¼ˆprompt_date_bucketsè¿”å›ç»“æœï¼‰
        later_priority: è§„åˆ™é‡å æ—¶æ˜¯å¦åå®šä¹‰æ¡¶ä¼˜å…ˆï¼Œé»˜è®¤True
    Returns:
        tuple: åŒ…å«ä¸¤ä¸ªå…ƒç´ çš„å…ƒç»„ï¼š
            - cat_byb: æŒ‰ç±»å‹å’Œæ¡¶åˆ†é…çš„ç»“æœï¼ˆdict[ç±»å‹][æ¡¶ç´¢å¼•] = æ•°æ®ç»„åˆ—è¡¨ï¼‰
            - remain_by_cat: æœªåˆ†é…çš„æ•°æ®ï¼ˆdict[ç±»å‹] = æ•°æ®ç»„åˆ—è¡¨ï¼‰
    """
    # è¾“å‡ºï¼šcat_byb[cat][bucket_index] = [groups...];  remain_by_cat[cat] = [groups...]
    cat_byb = {cat: {i: [] for i in range(len(buckets))} for cat in cat_groups}
    assigned = {cat: set() for cat in cat_groups}
    order = range(len(buckets) - 1, -1, -1) if later_priority else range(len(buckets))
    sup_strategy = (support_bucket_strategy or "number") if support_bucket_strategy else "number"
    sup_strategy = sup_strategy.lower()
    net_strategy_default = (net_bucket_strategy or "number") if net_bucket_strategy else "number"
    net_strategy_default = net_strategy_default.lower()
    for cat, groups in cat_groups.items():
        for idx, g in enumerate(groups):
            # è®¡ç®—åŒ¹é…
            fl = floor_of(g["name"])
            wzno = _wz_no(g["name"]) if cat == "æ”¯æ’‘" and sup_strategy == "number" else None
            for bi in order:
                b = buckets[bi]
                bucket_rules = (b or {}).get("rules") or {}
                rule = bucket_rules.get(cat)
                if not rule:
                    continue
                if cat != "ç½‘æ¶" and not rule.get("enabled"):
                    continue
                ok = False  # noqa
                if cat == "æ”¯æ’‘":
                    if sup_strategy == "number":
                        rng = rule["ranges"]
                        ok_num = True if rng == [] else (wzno is not None and _in_ranges(wzno, rng))
                        ok = ok_num
                    else:
                        ok = _in_ranges(fl, rule["ranges"])
                elif cat == "ç½‘æ¶":
                    parts = (rule or {}).get("parts") or {}
                    part = net_part(g["name"])
                    part_rule = parts.get(part) or parts.get("GEN")
                    if not (part_rule and part_rule.get("enabled")):
                        continue
                    bucket_net_strategy = (rule.get("strategy") or net_strategy_default).lower()
                    if bucket_net_strategy == "number":
                        no = _net_no(g["name"])
                        ok = (no is not None) and _in_ranges(no, part_rule["ranges"])
                    else:
                        ok = _in_ranges(fl, part_rule["ranges"])
                else:
                    ok = _in_ranges(fl, rule["ranges"])
                kws_list = b.get("kws") if isinstance(b, dict) else None
                if ok and _match_keywords(g["name"], kws_list):
                    cat_byb[cat][bi].append(g)
                    assigned[cat].add(idx)
                    break

    remain_by_cat = {cat: [g for i, g in enumerate(groups) if i not in assigned[cat]]
                     for cat, groups in cat_groups.items()}
    return cat_byb, remain_by_cat


def _to_bool(x):
    if isinstance(x, bool):
        return x
    s = str(x).strip().lower()
    return s in {"1", "true", "y", "yes", "on"}


class Mode1ConfigProvider:
    """å‰ç«¯é…ç½®é€‚é…å±‚ï¼Œæä¾› Mode 1 æ‰€éœ€çš„ç»“æ„åŒ–é…ç½®ã€‚"""

    def __init__(
            self,
            buckets,
            support_strategy,
            net_strategy,
            later_priority,
            auto_merge_rest,
            meta=None,
    ):
        self.raw_buckets = list(buckets or [])
        self.support_strategy = (support_strategy or "number").lower()
        self.net_strategy = (net_strategy or "number").lower()
        self.later_priority = _to_bool(later_priority)
        self.auto_merge_rest = _to_bool(auto_merge_rest)
        self.meta = dict(meta or {})
        self._normalized_buckets = [self._normalize_bucket(b) for b in self.raw_buckets]

    def _normalize_bucket(self, bucket):
        data = dict(bucket or {})
        date_raw = data.get("date_raw") or data.get("date") or ""
        kws = self._normalize_keywords(data.get("kws"))
        normalized = {
            "date_raw": date_raw,
            "date": normalize_date(date_raw) if date_raw else "",
            "rules": {},
            "kws": kws,
        }
        rules_in = data.get("rules") or data.get("parts") or {}
        for cat, rule in rules_in.items():
            if cat == "ç½‘æ¶":
                normalized["rules"][cat] = self._normalize_net_rule(rule)
            else:
                normalized_rule = self._normalize_simple_rule(rule)
                if normalized_rule:
                    normalized["rules"][cat] = normalized_rule
        return normalized

    def _normalize_keywords(self, kws):
        if not kws:
            return []
        if isinstance(kws, str):
            parts = [k for k in re.split(r"[,\sï¼Œ]+", kws) if k]
            return parts
        parts = []
        for item in kws:
            s = str(item).strip()
            if s:
                parts.append(s)
        return parts

    def _normalize_simple_rule(self, rule_data):
        if rule_data is None:
            return {"enabled": False, "ranges": []}
        explicit_all = False
        if isinstance(rule_data, dict):
            enabled_flag = rule_data.get("enabled")
            if enabled_flag is None:
                enabled_flag = True
            ranges_raw = rule_data.get("ranges")
            explicit_all = bool(
                rule_data.get("explicit_all")
                or _is_explicit_all_token(rule_data.get("raw"))
                or _is_explicit_all_token(rule_data.get("text"))
                or _is_explicit_all_token(ranges_raw if isinstance(ranges_raw, str) else "")
                )
        else:
            enabled_flag = True
            ranges_raw = rule_data
            explicit_all = _is_explicit_all_token(rule_data if isinstance(rule_data, str) else "")
        ranges = self._coerce_ranges(ranges_raw)
        if ranges is None:
            return {"enabled": False, "ranges": []}
        if ranges == [(1, 0)]:
            return {"enabled": False, "ranges": []}
        if ranges == [] and not explicit_all:
            return {"enabled": False, "ranges": []}
        return {"enabled": bool(enabled_flag), "ranges": ranges}

    def _coerce_ranges(self, ranges_raw):
        if isinstance(ranges_raw, list):
            return list(ranges_raw)
        s = unicodedata.normalize("NFKC", str(ranges_raw or "")).strip()
        if not s:
            return None
        if _is_lk(s):
            # è¿”å›ä¸€ä¸ªâ€œç©ºé›†å“¨å…µâ€ï¼Œä¸Šå±‚æ®æ­¤æŠŠ enabled ç½® False
            return [(1, 0)]
        if _is_explicit_all_token(s):
            return []
        return _parse_int_ranges(s)

    def _normalize_net_rule(self, rule):
        data = dict(rule or {})
        strategy = (data.get("strategy") or self.net_strategy or "number").lower()
        parts_in = data.get("parts") or {}
        parts_out = {}
        for part, part_rule in parts_in.items():
            parts_out[part] = self._normalize_simple_rule(part_rule)
        return {"strategy": strategy, "parts": parts_out}

    def get_buckets(self):
        """è¿”å›æ·±æ‹·è´çš„è§„èŒƒåŒ–æ¡¶é…ç½®ï¼Œä¾› run_mode ä½¿ç”¨ã€‚"""
        return copy.deepcopy(self._normalized_buckets)


def run_mode1_with_provider(src_docx, out_dir, provider: "Mode1ConfigProvider"):
    """ä»¥é€‚é…å±‚æä¾›çš„æ•°æ®è¿è¡Œ Mode1ï¼Œæ— éœ€äº¤äº’ã€‚"""

    if provider is None:
        raise ValueError("provider ä¸èƒ½ä¸ºç©º")

    src = Path(str(src_docx)).resolve()
    if not src.exists():
        raise FileNotFoundError(f"æœªæ‰¾åˆ° Word æºæ–‡ä»¶ï¼š{src}")

    out_dir = Path(out_dir) if out_dir is not None else src.parent
    out_dir = out_dir.resolve()
    out_dir.mkdir(parents=True, exist_ok=True)

    cache_src = _PROBE_CACHE.get("src")
    grouped = None
    categories_present = None
    if cache_src and Path(str(cache_src)).resolve() == src:
        grouped = _PROBE_CACHE.get("grouped") or defaultdict(list)
        categories_present = list(_PROBE_CACHE.get("categories") or [])
    if grouped is None or categories_present is None:
        grouped, categories_present = prepare_from_word(src)
    else:
        if not isinstance(grouped, defaultdict):
            tmp = defaultdict(list)
            for k, v in (grouped or {}).items():
                tmp[k] = list(v)
            grouped = tmp

    tpl_path = XLSX_WITH_SUPPORT_DEFAULT
    if not tpl_path.exists():
        raise FileNotFoundError(f"Excel æ¨¡æ¿ä¸å­˜åœ¨ï¼š{tpl_path}")
    wb = load_workbook_safe(tpl_path)

    buckets = provider.get_buckets()

    _pd = globals().get("prompt_date_buckets")
    _ps = globals().get("prompt_support_strategy_for_bucket")
    _pn = globals().get("prompt_net_strategy_for_bucket")
    _pl = globals().get("prompt_later_priority")
    _pa = globals().get("prompt_auto_merge_remains")
    _pk = globals().get("prompt_keywords_for_bucket")

    def __pd(*_, **__):
        return buckets

    def __ps(*_, **__):
        set_support_strategy(provider.support_strategy)
        return provider.support_strategy

    def __pn(*_, **__):
        set_net_strategy(provider.net_strategy)
        return provider.net_strategy

    def __pl(*_, **__):
        return provider.later_priority

    def __pa(*_, **__):
        return provider.auto_merge_rest

    def __pk(*_, **__):
        return []

    try:
        globals()["prompt_date_buckets"] = __pd
        globals()["prompt_support_strategy_for_bucket"] = __ps
        globals()["prompt_net_strategy_for_bucket"] = __pn
        globals()["prompt_later_priority"] = __pl
        globals()["prompt_auto_merge_remains"] = __pa
        globals()["prompt_keywords_for_bucket"] = __pk

        used_pages = run_mode(
            "1",
            wb,
            categories_present=categories_present,
            grouped_preloaded=grouped,
        )
    finally:
        if _pd is not None:
            globals()["prompt_date_buckets"] = _pd
        if _ps is not None:
            globals()["prompt_support_strategy_for_bucket"] = _ps
        if _pn is not None:
            globals()["prompt_net_strategy_for_bucket"] = _pn
        if _pl is not None:
            globals()["prompt_later_priority"] = _pl
        if _pa is not None:
            globals()["prompt_auto_merge_remains"] = _pa
        if _pk is not None:
            globals()["prompt_keywords_for_bucket"] = _pk
        set_support_strategy(None)
        set_net_strategy(None)

    meta = provider.meta or {}
    apply_meta_fixed(wb, categories_present, meta)
    enforce_mu_font(wb)
    cleanup_unused_sheets(wb, used_pages, bases=tuple(CATEGORY_ORDER))

    def _unique_out_path(dest_dir: Path, stem: str) -> Path:
        cand = dest_dir / f"{stem}.xlsx"
        if not cand.exists():
            return cand
        i = 1
        while True:
            cand = dest_dir / f"{stem}({i}).xlsx"
            if not cand.exists():
                return cand
            i += 1

    final_path = _unique_out_path(out_dir, f"{TITLE}_æŠ¥å‘Šç‰ˆ")
    save_workbook_safe(wb, final_path)

    word_out = src.with_name("æ±‡æ€»åŸå§‹è®°å½•.docx")
    if not word_out.exists():
        all_rows = _PROBE_CACHE.get("all_rows")
        if all_rows:
            try:
                doc_out = build_summary_doc_with_progress(all_rows)
                set_doc_font_progress(doc_out, DEFAULT_FONT_PT)
                save_docx_safe(doc_out, word_out)
            except Exception:
                pass
    return final_path, word_out


def set_support_strategy(strategy: str | None):
    """è®¾ç½®å…¨å±€æ”¯æ’‘åˆ†æ¡¶ç­–ç•¥ã€‚"""
    global support_bucket_strategy
    if strategy is None:
        support_bucket_strategy = None
        return
    val = str(strategy).strip().lower()
    if val not in {"number", "floor"}:
        raise ValueError("support_strategy å¿…é¡»æ˜¯ 'number' æˆ– 'floor'")
    support_bucket_strategy = val


def set_net_strategy(strategy: str | None):
    """è®¾ç½®å…¨å±€ç½‘æ¶åˆ†æ¡¶ç­–ç•¥ã€‚"""
    global net_bucket_strategy
    if strategy is None:
        net_bucket_strategy = None
        return
    val = str(strategy).strip().lower()
    if val not in {"number", "floor"}:
        raise ValueError("net_strategy å¿…é¡»æ˜¯ 'number' æˆ– 'floor'")
    net_bucket_strategy = val


def merge_remains_into_last_bucket(cats_by_bucket: dict, remain_by_cat: dict):
    """æŠŠæœªåˆ†é…çš„æ•°æ®å¹¶å…¥æœ€åä¸€ä¸ªæ¡¶ã€‚"""
    if not cats_by_bucket:
        return
    last_idx = None
    for bucket_map in cats_by_bucket.values():
        if bucket_map:
            cur_max = max(bucket_map.keys())
            last_idx = cur_max if last_idx is None else max(last_idx, cur_max)
    if last_idx is None:
        last_idx = 0
    for cat, remain in (remain_by_cat or {}).items():
        bucket_map = cats_by_bucket.setdefault(cat, {})
        if last_idx not in bucket_map:
            bucket_map[last_idx] = []
        bucket_map[last_idx].extend(remain)
        if hasattr(remain, "clear"):
            remain.clear()


def preview_buckets_generic(cat_byb, remain_by_cat, buckets, categories_present):
    """
     é¢„è§ˆæ—¥æœŸæ¡¶åˆ†é…ç»“æœï¼Œè¯¢é—®ç”¨æˆ·æ˜¯å¦ç¡®è®¤ç”Ÿæˆï¼Œæ”¯æŒæœªåˆ†é…æ•°æ®å¤„ç†ã€‚

     æ˜¾ç¤ºæ¯å¤©å„ç±»å‹æ„ä»¶çš„åˆ†é…æ•°é‡åŠæœªåˆ†é…æ•°æ®ï¼›æä¾›é€‰é¡¹ï¼š
     - å›è½¦ï¼šç¡®è®¤ç”Ÿæˆ
     - nï¼šå–æ¶ˆæ“ä½œ
     - aï¼šå°†æœªåˆ†é…æ•°æ®å¹¶å…¥æœ€åä¸€å¤©

     Args:
         cat_byb: æŒ‰ç±»å‹å’Œæ¡¶åˆ†é…çš„ç»“æœ
         remain_by_cat: æœªåˆ†é…æ•°æ®
         buckets: æ—¥æœŸæ¡¶é…ç½®åˆ—è¡¨
         categories_present: å­˜åœ¨çš„æ„ä»¶ç±»å‹åˆ—è¡¨
     Returns:
         tuple: åŒ…å«ä¸¤ä¸ªå…ƒç´ çš„å…ƒç»„ï¼š
             - æ˜¯å¦ç¡®è®¤ç”Ÿæˆï¼ˆboolï¼‰
             - æ˜¯å¦å°†æœªåˆ†é…æ•°æ®å¹¶å…¥æœ€åä¸€å¤©ï¼ˆboolï¼‰
     """
    print("\nğŸ§¾ é¢„è§ˆï¼š")
    for i, b in enumerate(buckets, start=1):
        parts = []
        for cat in categories_present:
            parts.append(f"{cat} {len(cat_byb[cat][i - 1])}")
        print(f"  ç¬¬{i}å¤© ã€”{b['date'] or b['date_raw'] or 'æœªå¡«æ—¥æœŸ'}ã€• â†’ " + "ã€".join(parts))
    if any(remain_by_cat[cat] for cat in categories_present):
        print("  âš ï¸ æœªåˆ†é…ï¼š", end="")
        print("ã€".join(f"{cat} {len(remain_by_cat[cat])}" for cat in categories_present if remain_by_cat[cat]))
    ans = ask("ç¡®è®¤ç”Ÿæˆå—ï¼Ÿ(å›è½¦=æ˜¯ / n=å¦ / a=æŠŠæœªåˆ†é…å¹¶å…¥æœ€åä¸€å¤©)ï¼š", lower=True)
    return (ans != "n"), (ans == "a")


def expand_blocks_by_bucket(cat_byb):
    """
    å°†æŒ‰æ—¥æœŸæ¡¶åˆ†é…çš„æ„ä»¶æ•°æ®ç»„æ‹†åˆ†ä¸ºæ ‡å‡†æ•°æ®å—ï¼ˆ5è¡Œ/å—ï¼‰ã€‚

    å¯¹æ¯ä¸ªç±»å‹ã€æ¯ä¸ªæ—¥æœŸæ¡¶çš„æ•°æ®ç»„åº”ç”¨expand_blockså‡½æ•°ï¼Œç¡®ä¿æ•°æ®å—ç»“æ„ç»Ÿä¸€ï¼Œé€‚é…Excelæ¨¡æ¿ã€‚

    Args:
        cat_byb: æŒ‰ç±»å‹å’Œæ¡¶åˆ†é…çš„ç»“æœï¼ˆassign_by_bucketsè¿”å›çš„cat_bybï¼‰
    Returns:
        dict: æŒ‰ç±»å‹å’Œæ¡¶ç»„ç»‡çš„æ•°æ®å—å­—å…¸ï¼ˆdict[ç±»å‹][æ¡¶ç´¢å¼•] = æ•°æ®å—åˆ—è¡¨ï¼‰
    """
    # è¿”å›ï¼šblocks_by_cat[cat][bucket_index] = [blocks...]
    return {cat: {bi: expand_blocks(lst, PER_LINE_PER_BLOCK) for bi, lst in byb.items()}
            for cat, byb in cat_byb.items()}


def ensure_pages_slices_for_cat(wb, cat: str, blocks_by_bucket_for_cat: dict):
    """
    ä¸ºæŒ‡å®šç±»å‹çš„æ¯ä¸ªæ—¥æœŸæ¡¶ç¡®ä¿è¶³å¤Ÿçš„å·¥ä½œè¡¨ï¼Œè¿”å›æŒ‰æ¡¶åˆ’åˆ†çš„å·¥ä½œè¡¨åˆ‡ç‰‡ã€‚

    è®¡ç®—æ¯ä¸ªæ¡¶æ‰€éœ€å·¥ä½œè¡¨æ•°é‡ï¼ˆæŒ‰5å—/é¡µï¼‰ï¼Œä¸è¶³æ—¶è‡ªåŠ¨å¤åˆ¶è¡¥å……ï¼š
    - å¸¸è§„ç±»å‹ï¼ˆé’¢æŸ±/é’¢æ¢/æ”¯æ’‘ï¼‰ä»è‡ªèº«åŸºç¡€è¡¨å¤åˆ¶
    - â€œå…¶ä»–â€ç±»å‹ä»é’¢æŸ±æ¨¡æ¿å¤åˆ¶
    è¿”å›æŒ‰æ¡¶åˆ†ç»„çš„å·¥ä½œè¡¨åç§°åˆ—è¡¨ã€‚

    Args:
        wb: Excelå·¥ä½œç°¿å¯¹è±¡
        cat: æ„ä»¶ç±»å‹ï¼ˆå¦‚â€œé’¢æŸ±â€â€œå…¶ä»–â€ï¼‰
        blocks_by_bucket_for_cat: è¯¥ç±»å‹æŒ‰æ¡¶ç»„ç»‡çš„æ•°æ®å—å­—å…¸
    Returns:
        list[list[str]]: æŒ‰æ¡¶åˆ’åˆ†çš„å·¥ä½œè¡¨åç§°åˆ—è¡¨ï¼ˆæ¯ä¸ªå…ƒç´ ä¸ºä¸€ä¸ªæ¡¶çš„å·¥ä½œè¡¨ï¼‰
    """

    def need_pages(lst):
        return math.ceil(len(lst) / BLOCKS_PER_SHEET) if lst else 0

    page_need_each = [need_pages(blocks_by_bucket_for_cat.get(i, [])) for i in range(len(blocks_by_bucket_for_cat))]
    total_need = sum(page_need_each)
    if total_need == 0:
        return [[] for _ in page_need_each]
    if cat == "å…¶ä»–":
        pages_all = ensure_total_pages_from(wb, "é’¢æŸ±", "å…¶ä»–", total_need)
    else:
        pages_all = ensure_total_pages(wb, cat, total_need)
    slices = [];
    p = 0
    for n in page_need_each:
        slices.append(pages_all[p:p + n]);
        p += n
    return slices


def make_target_order_generic(pages_slices_by_cat, categories_present):
    """
    ç”Ÿæˆå·¥ä½œè¡¨çš„ç›®æ ‡é¡ºåºï¼ŒæŒ‰â€œæ—¥æœŸæ¡¶â†’ç±»å‹ä¼˜å…ˆçº§â€æ’åºã€‚

    æ’åºè§„åˆ™ï¼š
    1. æŒ‰æ—¥æœŸæ¡¶è½®æ¬¡åˆ†ç»„
    2. åŒè½®æ¬¡å†…æŒ‰CATEGORY_ORDERï¼ˆé’¢æŸ±â†’é’¢æ¢â†’æ”¯æ’‘â†’ç½‘æ¶â†’å…¶ä»–ï¼‰æ’åº
    ç¡®ä¿å·¥ä½œè¡¨æŒ‰æ£€æµ‹æµç¨‹å’Œç±»å‹é€»è¾‘æœ‰åºæ’åˆ—ã€‚

    Args:
        pages_slices_by_cat: æŒ‰ç±»å‹å’Œæ¡¶åˆ’åˆ†çš„å·¥ä½œè¡¨åˆ‡ç‰‡å­—å…¸
        categories_present: å­˜åœ¨çš„æ„ä»¶ç±»å‹åˆ—è¡¨
    Returns:
        list[str]: æ’åºåçš„å·¥ä½œè¡¨åç§°åˆ—è¡¨
    """
    rounds = 0
    for cat in categories_present:
        rounds = max(rounds, len(pages_slices_by_cat.get(cat, [])))
    target = []
    for i in range(rounds):
        for cat in CATEGORY_ORDER:
            if cat not in categories_present: continue
            sl = pages_slices_by_cat[cat][i] if i < len(pages_slices_by_cat[cat]) else []
            target += sl
    return target


# ===== Excel å†™å…¥å¸¦è¿›åº¦ =====
class Prog:
    def __init__(self, total: int, label: str = "å†™å…¥ Excel"):
        self.total = max(1, total)
        self.done = 0
        self.label = label

    def tick(self, k=1):
        self.done += k
        pct = int(self.done * 100 / self.total)
        sys.stdout.write(f"\rğŸ“Š {self.label}ï¼š{self.done}/{self.total}ï¼ˆ{pct}%ï¼‰")
        sys.stdout.flush()

    def finish(self):
        sys.stdout.write("\n");
        sys.stdout.flush()


def fill_blocks_to_pages(wb, pages_slice, blocks, prog: Prog | None = None):
    """
    å¼ºæ ¡éªŒç‰ˆï¼ˆé¡µå†…ä¹Ÿæ‹¦ï¼‰ï¼šåªè¦å‘ç°â€œå½“å‰å—â€çš„ ç±»åˆ«/Î¼ ä¸â€œå½“å‰é¡µâ€ä¸ä¸€è‡´ï¼Œ
    - è‹¥å½“å‰é¡µè¿˜æ²¡å†™ï¼ˆpos==0ï¼‰ï¼šè·³è¿‡è¿™å¼ é¡µæ‰¾ä¸‹ä¸€å¼ ï¼›
    - è‹¥å½“å‰é¡µå·²å†™è¿‡ï¼ˆpos>0ï¼‰ï¼šå…ˆè¡¥æ–œæ æ”¶å°¾ï¼Œæ¢åˆ°ä¸‹ä¸€å¼ å†å†™ã€‚
    """
    if not pages_slice:
        return

    max_cap = len(pages_slice) * BLOCKS_PER_SHEET
    if len(blocks) > max_cap:
        sys.stdout.write(f"\nâš ï¸ å†™å…¥å— {len(blocks)} è¶…å‡ºå¯ç”¨å®¹é‡ {max_cap}ï¼ˆå°†è‡ªåŠ¨æˆªæ–­ï¼Œä¸ä¼šä¸²é¡µï¼‰ã€‚\n")

    page_idx, pos = 0, 0
    i = 0
    while i < len(blocks) and page_idx < len(pages_slice):
        ws = wb[pages_slice[page_idx]]
        title = ws.title

        if STRICT_CROSS_CAT_GUARD:
            sheet_cat = _sheet_cat_from_title(title)
            sheet_is_mu = _is_mu_title(title)
            block_cat = kind_of((blocks[i].get("name") if blocks[i] else "") or "")
            block_is_mu = _is_mu_block(blocks[i])

            mismatch = (sheet_cat and block_cat and sheet_cat != block_cat) or (sheet_is_mu != block_is_mu)
            if mismatch:
                # é¡µé¦–ï¼šè¿™å¼ ç›´æ¥è·³è¿‡ï¼›é¡µä¸­ï¼šå…ˆæ”¶å°¾å†æ¢é¡µ
                if pos != 0:
                    slash_tail(ws, detect_anchors(ws), pos)
                page_idx += 1
                pos = 0
                continue

        # å†™å…¥å½“å‰å—
        anc = detect_anchors(ws)
        write_block(ws, anc, pos, blocks[i])
        if prog:
            prog.tick(1)
        pos += 1
        i += 1

        # æ¢é¡µ
        if pos == BLOCKS_PER_SHEET:
            page_idx += 1
            pos = 0

    # å°¾é¡µè¡¥â€œ/â€
    if page_idx < len(pages_slice) and pos != 0:
        ws = wb[pages_slice[page_idx]]
        slash_tail(ws, detect_anchors(ws), pos)




def cleanup_unused_sheets(wb, used_names, bases=("é’¢æŸ±", "é’¢æ¢", "æ”¯æ’‘", "ç½‘æ¶", "å…¶ä»–")):
    """
    æ¸…ç†Excelä¸­æœªä½¿ç”¨çš„æŒ‡å®šç±»å‹å·¥ä½œè¡¨ï¼Œå‡å°‘å†—ä½™ã€‚

    ä»…ä¿ç•™å·²ä½¿ç”¨çš„ç›®æ ‡ç±»å‹å·¥ä½œè¡¨ï¼ˆé’¢æŸ±/é’¢æ¢/æ”¯æ’‘/ç½‘æ¶/å…¶ä»–ï¼‰ï¼Œé¿å…æ¨¡æ¿ä¸­å¤šä½™å·¥ä½œè¡¨å¹²æ‰°ã€‚
    ç¡®ä¿è‡³å°‘ä¿ç•™ä¸€ä¸ªå·¥ä½œè¡¨ï¼ˆé˜²æ­¢å·¥ä½œç°¿ä¸ºç©ºï¼‰ã€‚

    Args:
        wb: Excelå·¥ä½œç°¿å¯¹è±¡
        used_names: å·²ä½¿ç”¨çš„å·¥ä½œè¡¨åç§°åˆ—è¡¨
        bases: ç›®æ ‡ç±»å‹åŸºç¡€åç§°åˆ—è¡¨
    """
    # å¦‚æœæ²¡æœ‰ä»»ä½•å·¥ä½œè¡¨è¢«ä½¿ç”¨ï¼Œåˆ™ä¸è¿›è¡Œæ¸…ç†ï¼Œé¿å…è¯¯åˆ æ¨¡æ¿é¡µ
    if not used_names:
        return
    used = set(used_names)
    to_remove = []
    for ws in list(wb.worksheets):
        if any(ws.title == b or ws.title.startswith(f"{b}ï¼ˆ") for b in bases):
            if ws.title not in used:
                to_remove.append(ws)
    if len(to_remove) >= len(wb.worksheets):
        to_remove = to_remove[:-1]
    for ws in to_remove:
        wb.remove(ws)


def _distribute_by_dates(items, date_entries):
    """æŒ‰æ—¥æœŸåˆ—è¡¨å°†é¡¹ç›®åˆ†é…åˆ°å„å¤©ã€‚"""
    res = []
    if not date_entries:
        return res
    if date_entries[0][1] is not None:  # é…é¢æ¨¡å¼
        cursor = 0
        total = len(items)
        n = len(date_entries)
        for i, (d, limit) in enumerate(date_entries):
            remaining = max(0, total - cursor)
            if remaining <= 0:
                res.append((d, []))
                continue
            if limit is None or limit <= 0:
                take = remaining
            elif i < n - 1:
                take = min(int(limit), remaining)
            else:
                take = remaining
            res.append((d, items[cursor:cursor + take]))
            cursor += take
    else:  # å‡åˆ†
        days = len(date_entries)
        per = math.ceil(len(items) / days) if days else 0
        cursor = 0
        for i, (d, _) in enumerate(date_entries):
            if i < days - 1:
                take = min(per, len(items) - cursor)
            else:
                take = len(items) - cursor
            res.append((d, items[cursor:cursor + take]))
            cursor += take
    return res

def _prompt_dates_and_limits():
    """äº¤äº’è·å–æ—¥æœŸå’Œæ¯æ—¥æ•°é‡ã€‚"""
    while True:
        txt = ask(
            "æ—¥æœŸï¼ˆç©ºæ ¼/é€—å·åˆ†éš”ï¼›æ”¯æŒ 20250101 / 2025å¹´1æœˆ1æ—¥ / 2025 1 1 / 2025.1.1 / 2025-1-1 / 1-1 / 01-01ï¼Œ\n"
            "å¹´ä»½é»˜è®¤å–é¦–ä¸ªæ—¥æœŸçš„å¹´æˆ–å½“å‰å¹´ï¼‰ï¼šä¾‹å¦‚ 2025-08-27 8-28 2025å¹´1æœˆ1æ—¥\nâ†’ "
        )
        if any(ch in txt for ch in "ï¼›;ï¼Œã€/\\|"):
            print("åªæ¥å—é€—å·æˆ–ç©ºæ ¼åˆ†éš”ï¼Œè¯·é‡è¾“ã€‚")
            continue
        dates, ig = _parse_dates_simple(txt)
        if not dates:
            print("è¯·è¾“å…¥è‡³å°‘ä¸€ä¸ªåˆæ³•æ—¥æœŸã€‚")
            continue
        if ig:
            print("å·²å¿½ç•¥ï¼š" + "ã€".join(ig))
        break
    while True:
        txt = ask("æ¯æ—¥æ•°é‡ï¼ˆæŒ‰æ—¥æœŸé¡ºåºï¼›ç©º=å‡åˆ†ï¼›å¡«æ•´æ•°=é…é¢ï¼‰\nâ†’ ")
        if txt == "":
            limits = [None] * len(dates)
            break
        tokens = [t for t in re.split(r"[ ,]+", txt) if t]
        if all(t.isdigit() and int(t) > 0 for t in tokens):
            if len(tokens) == 1:
                limits = [int(tokens[0])] * len(dates)
                break
            if len(tokens) == len(dates):
                limits = [int(t) for t in tokens]
                break
        print(f"è¯·è¾“å…¥{len(dates)}ä¸ªæ­£æ•´æ•°æˆ–ç•™ç©ºã€‚")

    return list(zip(dates, limits))


def _summarize_plan(tag, plan, all_floors=None):
    """è¾“å‡ºæ¥¼å±‚è®¡åˆ’æ‘˜è¦ï¼Œä¾¿äºç”¨æˆ·ç¡®è®¤ã€‚"""

    def fmt(entry):
        ds = " ".join(normalize_date(x[0]) for x in entry)
        ls = ",".join(str(x[1]) if x[1] is not None else "-" for x in entry)
        return f"{ds} â†’ {ls}"

    specified = [f for f in plan if f != "*"]
    if specified:
        print("å·²å•ç‹¬é…ç½®ï¼š")
        for f in sorted(specified, key=_floor_sort_key_by_label):
            print(f"  {f} â†’ {fmt(plan[f])}")
    if "*" in plan:
        print("é»˜è®¤é…ç½®ï¼š")
        print(f"  * â†’ {fmt(plan['*'])}")
    if all_floors:
        miss = [f for f in all_floors if f not in plan and "*" not in plan]
        if miss:
            miss_txt = " ".join(sorted(miss, key=_floor_sort_key_by_label))
            print(f"æœªè¦†ç›–çš„æ¥¼å±‚ï¼š{miss_txt} ï¼ˆç¨åç»Ÿä¸€å¤„ç†/å›è½åˆ°æ—¥æœŸåˆ†æ¡¶ï¼‰")


def _prompt_plan_for_floors(floors, shared=True):
    """é’ˆå¯¹ç»™å®šæ¥¼å±‚é›†åˆäº¤äº’ç”Ÿæˆè®¡åˆ’ã€‚"""
    floors = sorted(set(floors), key=_floor_sort_key_by_label)
    if floors:
        print("å·²è¯†åˆ«æ¥¼å±‚ï¼š" + " ".join(floors))
    # Step1 æ¥¼å±‚
    while True:
        txt = ask("é€‚ç”¨æ¥¼å±‚ï¼ˆå›è½¦=å…¨éƒ¨ï¼‰ï¼šç¤ºä¾‹ 5F, 6F, B2, å±‹é¢ æˆ– 5 6 B2\nâ†’ ")
        if any(ch in txt for ch in "ï¼›;ï¼Œã€/\\|"):
            print("åªæ¥å—é€—å·æˆ–ç©ºæ ¼åˆ†éš”ï¼Œè¯·é‡è¾“ã€‚")
            continue
        if not txt:
            sel = None
            break
        tokens = [t for t in re.split(r"[ ,]+", txt) if t]
        seen, sel, ig = set(), [], []
        for t in tokens:
            lb = _floor_label_from_name(t)
            if lb != "F?" and lb in floors and lb not in seen:
                sel.append(lb);
                seen.add(lb)
            else:
                ig.append(t)
        if ig:
            print("å·²å¿½ç•¥ï¼š" + "ã€".join(ig))
        if sel:
            break
        print("æ²¡æœ‰åˆæ³•æ¥¼å±‚ï¼Œè¯·é‡è¾“ã€‚")
    targets = floors if sel is None else sel
    if shared:
        print("ä¸‹é¢è¾“å…¥çš„æ—¥æœŸä¸æ¯æ—¥ä¸Šé™ï¼Œå°†è‡ªåŠ¨åº”ç”¨åˆ°ä»¥ä¸Šæ‰€æœ‰æ¥¼å±‚")
        date_entries = _prompt_dates_and_limits()
        if sel is None:
            return {"*": date_entries}
        return {f: date_entries for f in targets}
    plan = {}
    for f in targets:
        print(f"{f}ï¼š")
        plan[f] = _prompt_dates_and_limits()
    return plan


def prompt_mode4_plan(floors_by_cat, categories_present):
    """æ¨¡å¼4äº¤äº’ï¼Œåˆ†åˆ«ä¸ºå„ç±»åˆ«è·å–æ¥¼å±‚è®¡åˆ’ã€‚"""
    print("å„ç±»åˆ«æ¥¼å±‚ï¼š")
    for cat in categories_present:
        fls = sorted(floors_by_cat.get(cat, set()), key=_floor_sort_key_by_label)
        print(f"{cat}: {(' '.join(fls)) if fls else '/'}")
    plans = {}
    for cat in categories_present:
        fls = floors_by_cat.get(cat, set())
        if not fls:
            continue
        print(f"\n[{cat}]")
        share = ask("è¿™äº›æ¥¼å±‚ç”¨åŒä¸€å¥—æ—¥æœŸ/æ•°é‡å—ï¼Ÿï¼ˆy=æ˜¯ï¼Œå›è½¦=åˆ†åˆ«è®¾ç½®ï¼‰\nâ†’ ") == "y"
        plans[cat] = _prompt_plan_for_floors(fls, shared=share)
        # â€”â€” æ–°å¢ï¼šç»™æœªæŒ‡å®šæ¥¼å±‚å…œåº• â€”â€”
        all_floors = sorted(floors_by_cat.get(cat, set()), key=_floor_sort_key_by_label)
        plan_for_cat = plans[cat]
        specified = {f for f in plan_for_cat.keys() if f != "*"}
        if "*" not in plan_for_cat and len(specified) < len(all_floors):
            miss = [f for f in all_floors if f not in specified]
            print(f"ğŸ‘‰ {cat} è¿˜æœ‰æœªé…ç½®æ¥¼å±‚ï¼š{' '.join(miss)}")
            ans = ask(
                "è¦ä¸è¦ç»™â€œæœªé…ç½®â€çš„æ¥¼å±‚ç”¨ä¸€å¥—é€šç”¨çš„æ—¥æœŸ/æ•°é‡ï¼Ÿï¼ˆy=æ˜¯ï¼Œå›è½¦=è·³è¿‡ï¼›æœªé…ç½®çš„æ¥¼å±‚ç¨åä¼šå†ç»Ÿä¸€è¯¢é—®æˆ–å›è½åˆ°æ—¥æœŸåˆ†æ¡¶ï¼‰",
                lower=True
            )
            if ans == "y":
                plan_for_cat["*"] = _prompt_dates_and_limits()
        _summarize_plan(cat, plan_for_cat, all_floors)
    return plans


def mode4_run(wb, grouped, categories_present):
    """æ‰§è¡Œæ¨¡å¼4ï¼šæŒ‰æ¥¼å±‚å’Œæ—¥æœŸå†™å…¥Excelã€‚"""
    write_dates = bool(globals().get("NONINTERACTIVE_MODE4_WRITE_DATES", True))
    injected_support = (globals().get("NONINTERACTIVE_MODE4_SUPPORT_STRATEGY") or "").lower()
    if injected_support in {"number", "floor"}:
        set_support_strategy(injected_support)
    injected_net = (globals().get("NONINTERACTIVE_MODE4_NET_STRATEGY") or "").lower()
    if injected_net in {"number", "floor"}:
        set_net_strategy(injected_net)
    cf_groups = defaultdict(list)
    floors_by_cat = defaultdict(set)
    for cat in categories_present:
        for g in grouped[cat]:
            fl = _floor_label_from_name(g["name"])
            cf_groups[(cat, fl)].append(g)
            floors_by_cat[cat].add(fl)
    plan_dict = globals().get("NONINTERACTIVE_MODE4_PLAN")
    if not plan_dict:
        plan_dict = prompt_mode4_plan(floors_by_cat, categories_present)

    blocks_by_cat_bucket = {cat: defaultdict(list) for cat in CATEGORY_ORDER}
    buckets = []  # list[{date}]
    date_idx = {}
    leftover_by_cat = defaultdict(list)

    for (cat, fl), items in cf_groups.items():
        items.sort(key=lambda x: (
            int(re.search(r"\d+", x["name"]).group()) if re.search(r"\d+", x["name"]) else 10 ** 9, x["name"]))
        plan_for_cat = plan_dict.get(cat, {})
        plan = plan_for_cat.get(fl) or plan_for_cat.get("*")
        if not plan:
            leftover_by_cat[cat].extend(items)
            continue
        for date, slice_items in _distribute_by_dates(items, plan):
            if not slice_items:
                continue
            if date not in date_idx:
                date_idx[date] = len(buckets)
                buckets.append({"date": date})
            idx = date_idx[date]
            blocks_by_cat_bucket[cat][idx].extend(expand_blocks(slice_items, PER_LINE_PER_BLOCK))
    # â€”â€” å…œåº• â€”â€”
    left_total = sum(len(v) for v in leftover_by_cat.values())
    if left_total:
        injected_fallback = (globals().get("NONINTERACTIVE_MODE4_FALLBACK") or "").lower()
        default_entries = globals().get("NONINTERACTIVE_MODE4_DEFAULT") or []
        handled_noninteractive = False
        if injected_fallback:
            if injected_fallback == "default":
                if not default_entries:
                    raise RuntimeError("Mode4 fallback=default ä½†æœªæä¾› default_entries")
                for cat in CATEGORY_ORDER:
                    if not leftover_by_cat.get(cat):
                        continue
                    for date, slice_items in _distribute_by_dates(leftover_by_cat[cat], default_entries):
                        if not slice_items:
                            continue
                        if date not in date_idx:
                            date_idx[date] = len(buckets)
                            buckets.append({"date": date})
                        idx = date_idx[date]
                        blocks_by_cat_bucket[cat][idx].extend(expand_blocks(slice_items, PER_LINE_PER_BLOCK))
                    leftover_by_cat[cat] = []
                leftover_by_cat = defaultdict(list)
                handled_noninteractive = True
            elif injected_fallback == "append_last":
                if not buckets:
                    raise RuntimeError("Mode4 append_last éœ€è¦è‡³å°‘ä¸€ä¸ªæ—¥æœŸæ¡¶")
                last_idx = len(buckets) - 1
                for cat in CATEGORY_ORDER:
                    if not leftover_by_cat.get(cat):
                        continue
                    blocks = expand_blocks(leftover_by_cat[cat], PER_LINE_PER_BLOCK)
                    blocks_by_cat_bucket[cat][last_idx].extend(blocks)
                leftover_by_cat = defaultdict(list)
                handled_noninteractive = True
            elif injected_fallback == "error":
                raise RuntimeError("Mode4 æœªåˆ†é…æ¥¼å±‚æœªæŒ‡å®šå¤„ç†æ–¹æ¡ˆï¼ˆfallback=errorï¼‰")
        if not handled_noninteractive:
            print(f"âš ï¸ è¿˜æœ‰ {left_total} ç»„æœªåˆ†é…ã€‚")
            ans = ask("æ˜¯å¦ç»™æœªæŒ‡å®šæ¥¼å±‚å¥—ç”¨ã€é»˜è®¤æ—¥æœŸ/æ•°é‡ã€‘ï¼Ÿ(y=æ˜¯ / å›è½¦=å¦â†’å›è½åˆ°æ—¥æœŸåˆ†æ¡¶)", lower=True)
            if ans == "y":
                default_entries = _prompt_dates_and_limits()
                for cat in CATEGORY_ORDER:
                    if not leftover_by_cat.get(cat):
                        continue
                    for date, slice_items in _distribute_by_dates(leftover_by_cat[cat], default_entries):
                        if not slice_items:
                            continue
                        if date not in date_idx:
                            date_idx[date] = len(buckets)
                            buckets.append({"date": date})
                        idx = date_idx[date]
                        blocks_by_cat_bucket[cat][idx].extend(expand_blocks(slice_items, PER_LINE_PER_BLOCK))
                    leftover_by_cat[cat] = []
            else:
                grouped_left = {c: leftover_by_cat[c] for c in CATEGORY_ORDER if leftover_by_cat.get(c)}
                if grouped_left:
                    buckets2 = prompt_date_buckets(list(grouped_left.keys()), grouped_left)
                    later_first = prompt_bucket_priority()
                    cat_byb, remain_by_cat = assign_by_buckets(grouped_left, buckets2, later_first)
                    ok, auto_last = preview_buckets_generic(cat_byb, remain_by_cat, buckets2, list(grouped_left.keys()))
                    if ok:
                        if auto_last:
                            last = len(buckets2) - 1
                            for c in grouped_left.keys():
                                cat_byb[c][last].extend(remain_by_cat[c])
                                remain_by_cat[c] = []
                        blocks_by_cat_bucket2 = expand_blocks_by_bucket(cat_byb)
                        for i, bk in enumerate(buckets2):
                            date = bk["date"]
                            if date not in date_idx:
                                date_idx[date] = len(buckets)
                                buckets.append({"date": date})
                            idx = date_idx[date]
                            for c in grouped_left.keys():
                                blocks_by_cat_bucket[c][idx].extend(blocks_by_cat_bucket2[c].get(i, []))
                        leftover_by_cat = remain_by_cat
                    else:
                        print("âŒ å·²å–æ¶ˆå…œåº•åˆ†é…ã€‚")

    unassigned = sum(len(v) for v in leftover_by_cat.values())

    # â€”â€” æ—¥æœŸæŒ‰å‡åºæ’åº â€”â€”
    order = sorted(range(len(buckets)), key=lambda i: buckets[i]["date"])
    buckets = [buckets[i] for i in order]
    for cat in CATEGORY_ORDER:
        blocks_by_cat_bucket[cat] = {new_i: blocks_by_cat_bucket[cat].get(old_i, []) for new_i, old_i in
                                     enumerate(order)}

    # â€”â€” ç»Ÿä¸€å†™é¡µ â€”â€”
    cats_in_use = [c for c in CATEGORY_ORDER if blocks_by_cat_bucket[c]]
    pages_slices_by_cat = {}
    for cat in cats_in_use:
        blocks_dict = {i: blocks_by_cat_bucket[cat].get(i, []) for i in range(len(buckets))}
        pages_slices_by_cat[cat] = ensure_pages_slices_for_cat(wb, cat, blocks_dict)

    target = make_target_order_generic(pages_slices_by_cat, cats_in_use)
    for idx, name in enumerate(target):
        cur = wb.sheetnames.index(name)
        if cur != idx:
            wb.move_sheet(wb[name], idx - cur)

    total_blocks = 0
    for cat in cats_in_use:
        for i in range(len(buckets)):
            total_blocks += len(blocks_by_cat_bucket[cat].get(i, []))
    prog = Prog(total_blocks, "å†™å…¥ Excel")
    for i in range(len(buckets)):
        day_pages = []
        for cat in CATEGORY_ORDER:
            if cat not in cats_in_use:
                continue
            pages = pages_slices_by_cat[cat][i]
            blocks = blocks_by_cat_bucket[cat].get(i, [])
            fill_blocks_to_pages(wb, pages, blocks, prog)
            day_pages += pages
        if write_dates:
            apply_meta_on_pages(
                wb,
                day_pages,
                normalize_date(buckets[i]["date"]),
            )
    prog.finish()

    used_names_total = target
    if unassigned:
        print(f"âš ï¸ æœªæŒ‡æ´¾ï¼š{unassigned} ç»„")
    return used_names_total


def try_handle_mode4(mode, wb, grouped, categories_present) -> list | None:
    """æ¨¡å¼4å…¼å®¹é’©å­ã€‚"""
    if mode != "4":
        return None
    return mode4_run(wb, grouped, categories_present)


# ===== æ—§æ³•å­æ¨¡å¼ =====
def prompt_break_submode(has_gz, has_gl):
    """
    äº¤äº’å¼é€‰æ‹©æ¥¼å±‚æ–­ç‚¹å­æ¨¡å¼ï¼Œé€‚é…ä¸åŒæ•°æ®åœºæ™¯ã€‚

    æ ¹æ®æ˜¯å¦åŒæ—¶å­˜åœ¨é’¢æŸ±å’Œé’¢æ¢æä¾›é€‰é¡¹ï¼š
    - åŒæ—¶å­˜åœ¨ï¼šæ”¯æŒå…±ç”¨æ–­ç‚¹ã€åˆ†åˆ«æ–­ç‚¹æˆ–æ— æ–­ç‚¹
    - ä»…å•ç±»ï¼šæ”¯æŒæ— æ–­ç‚¹æˆ–åˆ†åˆ«æ–­ç‚¹
    ç¡®ä¿å­æ¨¡å¼é€‚é…å®é™…æ•°æ®ç±»å‹ã€‚

    Args:
        has_gz: æ˜¯å¦å­˜åœ¨é’¢æŸ±æ•°æ®ï¼ˆboolï¼‰
        has_gl: æ˜¯å¦å­˜åœ¨é’¢æ¢æ•°æ®ï¼ˆboolï¼‰
    Returns:
        str: å­æ¨¡å¼ç¼–å·ï¼ˆ"1"|"2"|"3"ï¼‰
    """
    if has_gz and has_gl:
        t = ask("æ–­ç‚¹å­æ¨¡å¼ï¼š1) æŸ±æ¢å…±ç”¨æ–­ç‚¹ï¼ˆç®€ä¾¿ï¼‰  2) æŸ±æ¢åˆ†åˆ«æ–­ç‚¹  3) æ— æ–­ç‚¹ï¼ˆæ•´å•åŒä¸€å¤©ï¼‰")
        return t if t in ("1", "2", "3") else "1"
    else:
        t = ask("æ–­ç‚¹å­æ¨¡å¼ï¼šä»…å­˜åœ¨å•ç±»ï¼ˆæˆ–åŠ â€œå…¶ä»–â€ï¼‰ â†’ 3) æ— æ–­ç‚¹  æˆ–  2) åˆ†åˆ«æ–­ç‚¹ï¼ˆæŒ‰å„è‡ªæ–­ç‚¹ï¼‰")
        return t if t in ("2", "3") else "3"


# ===== ä¸»æµç¨‹ =====
def _parse_breaks_text(text: str) -> list[int]:
    tokens = re.split(r"[\s,ï¼Œ,;ï¼›ã€]+", str(text or ""))
    vals: set[int] = set()
    for tok in tokens:
        tok = tok.strip()
        if not tok:
            continue
        norm = re.sub(r"[~ï½ã€œï¼â€”â€“âˆ’è‡³åˆ°]", "-", tok)
        m_range = re.fullmatch(r"(\d+)\s*-\s*(\d+)", norm)
        if m_range:
            start = int(m_range.group(1))
            end = int(m_range.group(2))
            if start <= end:
                vals.update(range(start, end + 1))
            else:
                vals.update(range(end, start + 1))
            continue
        m_single = re.search(r"(\d+)", tok)
        if not m_single:
            continue
        try:
            vals.add(int(m_single.group(1)))
        except ValueError:
            continue
    return sorted(vals)


def _segment_blocks_by_floor(blocks, breaks: list[int]):
    buckets = defaultdict(list)
    for blk in blocks or []:
        seg = segment_index(floor_of(blk.get("name", "")), breaks)
        buckets[seg].append(blk)
    if not buckets:
        buckets[0] = []
    return buckets


def _segment_blocks_by_number(blocks, breaks: list[int], extractor):
    buckets = defaultdict(list)
    for blk in blocks or []:
        raw = extractor(blk.get("name", ""))
        if raw is None:
            seg = len(breaks) if breaks else 0
        else:
            seg = len(breaks)
            for idx, val in enumerate(breaks):
                if raw <= val:
                    seg = idx
                    break
        buckets[seg].append(blk)
    if not buckets:
        buckets[0] = []
    return buckets


def _run_mode2_auto(
    wb,
    grouped,
    categories_present,
    *,
    breaks_gz: str = "",
    breaks_gl: str = "",
    breaks_support: str = "",
    include_support: bool = True,
):
    categories_present = [c for c in categories_present if grouped.get(c)]
    if not include_support and "æ”¯æ’‘" in categories_present:
        categories_present = [c for c in categories_present if c != "æ”¯æ’‘"]

    breaks_gz_list = _parse_breaks_text(breaks_gz)
    breaks_gl_list = _parse_breaks_text(breaks_gl)
    anchor_breaks = sorted(set(breaks_gz_list + breaks_gl_list))

    sup_breaks_raw = (globals().get("NONINTERACTIVE_SUPPORT_BREAKS") or "").strip()
    sup_breaks_list = _parse_breaks_text(sup_breaks_raw) if sup_breaks_raw else anchor_breaks

    support_strategy = (globals().get("NONINTERACTIVE_SUPPORT_STRATEGY") or "number").lower()
    net_strategy = (globals().get("NONINTERACTIVE_NET_STRATEGY") or "number").lower()
    net_breaks_raw = (globals().get("NONINTERACTIVE_NET_BREAKS") or "").strip()
    net_breaks_list = _parse_breaks_text(net_breaks_raw) if net_breaks_raw else anchor_breaks

    blocks_by_cat = {cat: expand_blocks(grouped.get(cat, []), PER_LINE_PER_BLOCK)
                     for cat in categories_present}

    buckets_by_cat = {}
    segment_ids = set()

    for cat in categories_present:
        blocks = blocks_by_cat.get(cat, [])
        if cat in ("é’¢æŸ±", "é’¢æ¢"):
            buckets = _segment_blocks_by_floor(blocks, anchor_breaks)
        elif cat == "æ”¯æ’‘":
            if support_strategy == "floor":
                buckets = _segment_blocks_by_floor(blocks, sup_breaks_list)
            else:
                buckets = _segment_blocks_by_number(blocks, sup_breaks_list, _wz_no)
        elif cat == "ç½‘æ¶":
            if net_strategy == "floor":
                buckets = _segment_blocks_by_floor(blocks, net_breaks_list)
            else:
                buckets = _segment_blocks_by_number(blocks, net_breaks_list, _net_no)
        else:
            buckets = defaultdict(list)
            buckets[0] = list(blocks)
        buckets_by_cat[cat] = buckets
        segment_ids.update(buckets.keys())

    if not segment_ids:
        segment_ids = {0}
    ordered_segments = sorted(segment_ids)

    pages_slices_by_cat = {}
    blocks_slices_by_cat = {}

    for cat in categories_present:
        bucket_map = {seg: list(buckets_by_cat[cat].get(seg, [])) for seg in ordered_segments}
        if cat == "å…¶ä»–":
            pages_list = []
            blocks_list = []
            for seg in ordered_segments:
                seg_blocks = bucket_map.get(seg, [])
                need = pages_needed(seg_blocks)
                pages_batch = [] if not need else ensure_total_pages_from(wb, "é’¢æŸ±", "å…¶ä»–", need)
                pages_list.append(pages_batch)
                blocks_list.append(seg_blocks)
            pages_slices_by_cat[cat] = pages_list
            blocks_slices_by_cat[cat] = blocks_list
        else:
            pages_slices, blocks_slices = ensure_pages_slices_for_cat_muaware(wb, cat, bucket_map)
            pages_slices_by_cat[cat] = [_filter_pages_for_cat(sl, cat) for sl in pages_slices]
            blocks_slices_by_cat[cat] = blocks_slices

    total_blocks = 0
    for cat in categories_present:
        for seg_blocks in blocks_slices_by_cat[cat]:
            total_blocks += len(seg_blocks)

    prog = Prog(total_blocks or 1, "å†™å…¥ Excel")

    used_pages: list[str] = []
    date_first = (globals().get("NONINTERACTIVE_MODE2_DATE_FIRST") or "").strip()
    date_second = (globals().get("NONINTERACTIVE_MODE2_DATE_SECOND") or "").strip()
    norm_first = normalize_date(date_first) if date_first else ""
    norm_second = normalize_date(date_second) if date_second else ""

    for seg_idx, _seg in enumerate(ordered_segments):
        for cat in CATEGORY_ORDER:
            if cat not in categories_present:
                continue
            pages_list = pages_slices_by_cat.get(cat, [])
            blocks_list = blocks_slices_by_cat.get(cat, [])
            if seg_idx >= len(pages_list):
                continue
            pages = pages_list[seg_idx]
            blocks_piece = blocks_list[seg_idx]
            if not pages:
                continue
            fill_blocks_to_pages(wb, pages, blocks_piece, prog)
            used_pages.extend(pages)
            date_to_write = norm_first if seg_idx == 0 else (norm_second or norm_first)
            if date_to_write:
                apply_meta_on_pages(wb, pages, date_to_write)

    prog.finish()

    for idx, name in enumerate(used_pages):
        if name not in wb.sheetnames:
            continue
        cur = wb.sheetnames.index(name)
        if cur != idx:
            wb.move_sheet(wb[name], idx - cur)

    cleanup_unused_mu_templates(wb, used_pages)
    return used_pages


def run_mode(
    mode: str,
    wb,
    grouped=None,
    categories_present=None,
    *,
    src: Union[str, Path] | None = None,
    grouped_preloaded=None,
    breaks_gz: str = "",
    breaks_gl: str = "",
    include_support: bool = True,
):
    """æŒ‰æŒ‡å®šæ¨¡å¼æ‰§è¡Œä¸€æ¬¡å¯¼å‡ºï¼ˆå…¨æ¨¡å¼æ”¯æŒ Î¼ é€»è¾‘ï¼›mode4 æš‚ä¿æŒåŸæ ·æµç¨‹ï¼‰ã€‚"""
    global support_bucket_strategy, net_bucket_strategy
    support_bucket_strategy = None
    net_bucket_strategy = None

    if grouped_preloaded is not None:
        grouped_data = grouped_preloaded
    elif grouped is not None:
        grouped_data = grouped
    elif src is not None:
        grouped_data, categories_from_src = prepare_from_word(Path(src))
        if categories_present is None:
            categories_present = categories_from_src
    elif _PROBE_CACHE.get("src") and Path(str(_PROBE_CACHE.get("src"))).exists() and src is None:
        grouped_data = _PROBE_CACHE.get("grouped") or {}
    else:
        raise ValueError("run_mode éœ€è¦æä¾› grouped/grouped_preloaded/src ä¹‹ä¸€")

    if isinstance(grouped_data, dict) and not isinstance(grouped_data, defaultdict):
        tmp = defaultdict(list)
        for k, v in grouped_data.items():
            tmp[k] = list(v)
        grouped_data = tmp

    if categories_present is None:
        categories_present = [cat for cat in CATEGORY_ORDER if grouped_data.get(cat)]

    # å…ˆäº¤ç»™ mode4 çš„ä¸“ç”¨å¤„ç†ï¼ˆä¸åŠ¨å®ƒå†…éƒ¨é€»è¾‘ï¼‰
    res = try_handle_mode4(mode, wb, grouped_data, categories_present)
    if res is not None:
        return res

    force_same_breaks = bool(globals().get("NONINTERACTIVE_MODE2_FORCE_SAME_BREAKS"))
    if mode == "2" and (grouped_preloaded is not None or force_same_breaks):
        return _run_mode2_auto(
            wb,
            grouped_data,
            categories_present,
            breaks_gz=breaks_gz,
            breaks_gl=breaks_gl,
            include_support=include_support,
        )

    # ============ mode 2ï¼šæŒ‰æ¥¼å±‚æ–­ç‚¹ ============
    if mode == "2":
        has_gz = "é’¢æŸ±" in categories_present
        has_gl = "é’¢æ¢" in categories_present
        sub = prompt_break_submode(has_gz, has_gl)

        blocks_by_cat = {cat: expand_blocks(grouped_data[cat], PER_LINE_PER_BLOCK)
                         for cat in categories_present}

        # â€”â€” å­æ¨¡å¼ 3ï¼šæ— æ–­ç‚¹ï¼Œæ•´ç±»ä¸€æ¬¡æ€§æ’ï¼ˆä¹Ÿç”¨ Î¼-awareï¼‰â€”â€”
        if sub == "3":
            pages_by_cat = {}
            blocks_by_cat_ordered = {}

            for cat in categories_present:
                blocks_all = blocks_by_cat[cat]
                if cat == "å…¶ä»–":
                    need = pages_needed(blocks_all)
                    pages_by_cat[cat] = [] if not need else ensure_total_pages_from(wb, "é’¢æŸ±", "å…¶ä»–", need)
                    blocks_by_cat_ordered[cat] = blocks_all
                else:
                    # å¤ç”¨ Î¼-awareï¼Œè§†ä½œâ€œåªæœ‰ä¸€ä¸ªæ¡¶â€ï¼Œç´¢å¼• 0
                    pages_slices, blocks_slices = ensure_pages_slices_for_cat_muaware(
                        wb, cat, {0: blocks_all}
                    )
                    pages_by_cat[cat] = pages_slices[0]
                    blocks_by_cat_ordered[cat] = blocks_slices[0]

            target = []
            for cat in CATEGORY_ORDER:
                if cat in categories_present:
                    target += pages_by_cat[cat]
            for idx, name in enumerate(target):
                cur = wb.sheetnames.index(name)
                if cur != idx:
                    wb.move_sheet(wb[name], idx - cur)

            total_blocks = sum(len(blocks_by_cat_ordered[cat]) for cat in categories_present)
            prog = Prog(total_blocks, "å†™å…¥ Excel")
            for cat in CATEGORY_ORDER:
                if cat in categories_present:
                    fill_blocks_to_pages(wb, pages_by_cat[cat], blocks_by_cat_ordered[cat], prog)
            prog.finish()

            d = normalize_date(ask("ğŸ“… æ•´å•æ—¥æœŸï¼ˆå›è½¦=ä¸å†™ï¼‰ï¼š") or "")
            apply_meta_on_pages(wb, target, d)
            cleanup_unused_mu_templates(wb, target)
            return target

        # â€”â€” å­æ¨¡å¼ 1/2ï¼šæŒ‰æ–­ç‚¹åˆ†æ®µï¼ˆæ¯æ®µä¹Ÿæ˜¯ Î¼-awareï¼‰â€”â€”
        same_breaks = None
        if has_gz and has_gl and sub == "1":
            same_breaks = prompt_floor_breaks("é’¢æŸ±/é’¢æ¢ï¼ˆå…±ç”¨ï¼‰")

        breaks_by_cat = {}
        for cat in categories_present:
            if cat == "æ”¯æ’‘":
                prompt_support_strategy_for_bucket()
                if support_bucket_strategy == "floor":
                    breaks_by_cat[cat] = prompt_floor_breaks(cat)
                else:
                    breaks_by_cat[cat] = []  # æ”¯æ’‘ä¸åˆ†æ®µ
            elif cat in ("é’¢æŸ±", "é’¢æ¢"):
                if ((cat == "é’¢æŸ±" and "é’¢æ¢" in categories_present) or
                    (cat == "é’¢æ¢" and "é’¢æŸ±" in categories_present)) and same_breaks is not None:
                    breaks_by_cat[cat] = same_breaks
                else:
                    breaks_by_cat[cat] = prompt_floor_breaks(cat)
            else:
                breaks_by_cat[cat] = prompt_floor_breaks(cat)

        # å»ºæ®µï¼šç”¨ floor_of + segment_index
        byseg = {cat: defaultdict(list) for cat in categories_present}
        for cat in categories_present:
            if cat == "æ”¯æ’‘" and support_bucket_strategy != "floor":
                byseg[cat][0] = blocks_by_cat[cat]
            else:
                for b in blocks_by_cat[cat]:
                    seg = segment_index(floor_of(b["name"]), breaks_by_cat[cat])
                    byseg[cat][seg].append(b)

        # å…ˆå¯¹æ¯ä¸ªç±»åˆ«ä¸€æ¬¡æ€§åˆ‡ç‰‡ï¼Œä¿è¯ç¼–å·è¿ç»­
        pages_slices_by_cat = {}
        blocks_slices_by_cat = {}
        for cat in categories_present:
            seg_dict = byseg[cat]
            if cat == "å…¶ä»–":
                pages_slices_by_cat[cat] = []
                blocks_slices_by_cat[cat] = []
                for seg in sorted(seg_dict.keys()):
                    seg_blocks = seg_dict[seg]
                    need = pages_needed(seg_blocks)
                    pages_batch = [] if not need else ensure_total_pages_from(wb, "é’¢æŸ±", "å…¶ä»–", need)
                    pages_slices_by_cat[cat].append(pages_batch)
                    blocks_slices_by_cat[cat].append(seg_blocks)
            else:
                pages_slices_by_cat[cat], blocks_slices_by_cat[cat] = ensure_pages_slices_for_cat_muaware(
                    wb, cat, seg_dict
                )
                pages_slices_by_cat[cat] = [_filter_pages_for_cat(sl, cat) for sl in pages_slices_by_cat[cat]]

        # æ„é€  (pages, blocks) é˜Ÿåˆ—ï¼ŒæŒ‰ ç±»Ã—æ®µ é€å¯¹å†™å…¥
        rounds = max(len(pages_slices_by_cat[c]) for c in categories_present)
        pairs = []
        for i in range(rounds):
            for cat in CATEGORY_ORDER:
                if cat not in categories_present:
                    continue
                p_list = pages_slices_by_cat[cat]
                b_list = blocks_slices_by_cat[cat]
                if i < len(p_list) and p_list[i]:
                    pairs.append((p_list[i], b_list[i]))

        target = []
        prog = Prog(sum(len(b) for _, b in pairs), "å†™å…¥ Excel")
        for pages, blocks_piece in pairs:
            target += pages
            fill_blocks_to_pages(wb, pages, blocks_piece, prog)
        prog.finish()

        # è°ƒæ•´é¡ºåºå¹¶å†™å…¥å…ƒä¿¡æ¯
        for idx, name in enumerate(target):
            cur = wb.sheetnames.index(name)
            if cur != idx:
                wb.move_sheet(wb[name], idx - cur)

        apply_meta_on_pages(wb, target, "")
        cleanup_unused_mu_templates(wb, target)
        return target

    # ============ mode 3ï¼šå•æ—¥æ¨¡å¼ï¼ˆå·²æœ‰ Î¼ é€»è¾‘ï¼Œè¿™é‡Œæ¥åˆ° Î¼-awareï¼‰ ============
    elif mode == "3":
        pages_by_cat = {}
        blocks_by_cat_ordered = {}

        for cat in categories_present:
            blocks_all = expand_blocks(grouped_data[cat], PER_LINE_PER_BLOCK)
            if cat == "å…¶ä»–":
                need = pages_needed(blocks_all)
                pages_by_cat[cat] = [] if not need else ensure_total_pages_from(wb, "é’¢æŸ±", "å…¶ä»–", need)
                blocks_by_cat_ordered[cat] = blocks_all
            else:
                pages_slices, blocks_slices = ensure_pages_slices_for_cat_muaware(
                    wb, cat, {0: blocks_all}
                )
                pages_by_cat[cat] = _filter_pages_for_cat(pages_slices[0], cat)  # ğŸ‘ˆ æ–°å¢è¿‡æ»¤
                blocks_by_cat_ordered[cat] = blocks_slices[0]

        target = []
        for cat in CATEGORY_ORDER:
            if cat in categories_present:
                target += pages_by_cat[cat]
        for idx, name in enumerate(target):
            cur = wb.sheetnames.index(name)
            if cur != idx:
                wb.move_sheet(wb[name], idx - cur)

        prog = Prog(sum(len(blocks_by_cat_ordered[c]) for c in categories_present), "å†™å…¥ Excel")
        for cat in CATEGORY_ORDER:
            if cat in categories_present:
                fill_blocks_to_pages(wb, pages_by_cat[cat], blocks_by_cat_ordered[cat], prog)
        prog.finish()

        # æ–°ç‰ˆï¼šä¼˜å…ˆä½¿ç”¨â€œéäº¤äº’æ³¨å…¥â€çš„æ—¥æœŸï¼Œé¿å… ask å¡ä½
        _injected = globals().pop("NONINTERACTIVE_MODE3_DATE",
                                  None) if "NONINTERACTIVE_MODE3_DATE" in globals() else None

        if _injected is not None:
            # UI/éäº¤äº’è°ƒç”¨ï¼šä¼  None/"" è¡¨ç¤ºè·³è¿‡å†™æ—¥æœŸ
            _date_in = _injected
        else:
            # ä»…åœ¨ CLI äº¤äº’æ—¶æ‰è¯¢é—®
            try:
                _date_in = ask("ğŸ“… è¯·è¾“å…¥æ£€æµ‹æ—¥æœŸï¼ˆå›è½¦è·³è¿‡ï¼›è¾“å…¥ q è¿”å›ä¸Šä¸€æ­¥ï¼‰ï¼š")
            except BackStep:
                raise

        if str(_date_in).strip():
            apply_meta_on_pages(wb, target, normalize_date(str(_date_in)))
        else:
            apply_meta_on_pages(wb, target, "")

        cleanup_unused_mu_templates(wb, target)
        return target


    # ============ mode 1ï¼šæ—¥æœŸåˆ†æ¡¶ï¼ˆæ¯ä¸ªâ€œæ—¥æ¡¶â€ä¹Ÿ Î¼-awareï¼‰ ============
    elif mode == "1":
        buckets = prompt_date_buckets(categories_present, grouped_data)
        if buckets is None:
            return

        later_first = prompt_later_priority()
        cat_byb, remain_by_cat = assign_by_buckets(grouped_data, buckets, later_first)
        ok, auto_last_preview = preview_buckets_generic(cat_byb, remain_by_cat, buckets, categories_present)
        if not ok:
            return

        forced_choice = prompt_auto_merge_remains(
            remain_by_cat=remain_by_cat,
            buckets=buckets,
            categories_present=categories_present,
            preview_choice=auto_last_preview,
        )
        auto_last = bool(auto_last_preview)
        forced_provided = forced_choice is not None
        if forced_provided:
            auto_last = bool(forced_choice)

        unassigned = sum(len(v) for v in remain_by_cat.values())
        if unassigned and not auto_last and not forced_provided:
            print(f"âš ï¸ æœªæŒ‡æ´¾ï¼š{unassigned} ç»„")
            auto = ask("æ˜¯å¦è‡ªåŠ¨æŠŠæœªæŒ‡æ´¾å¹¶å…¥æœ€åä¸€å¤©ï¼Ÿï¼ˆy=æ˜¯ / å…¶å®ƒ=å¦ï¼‰", allow_empty=False, lower=True)
            if auto == "y":
                auto_last = True
            elif auto == "q":
                raise BackStep()

        if auto_last:
            last = len(buckets) - 1
            for cat in categories_present:
                cat_byb[cat][last].extend(remain_by_cat[cat])
                remain_by_cat[cat] = []

        blocks_by_cat_bucket = expand_blocks_by_bucket(cat_byb)

        # â€”â€” å…³é”®ï¼šæŠŠâ€œæ¯å¤©â€çš„å—åšæˆ Î¼-aware çš„åˆ‡ç‰‡ â€”â€”
        pages_slices_by_cat = {}
        blocks_slices_by_cat = {}
        for cat in categories_present:
            # blocks_by_cat_bucket[cat] æ˜¯ dict: day_idx -> blocks(list)
            pages_slices_by_cat[cat], blocks_slices_by_cat[cat] = ensure_pages_slices_for_cat_muaware(
                wb, cat, blocks_by_cat_bucket[cat]
            )
            pages_slices_by_cat[cat] = [_filter_pages_for_cat(sl, cat) for sl in pages_slices_by_cat[cat]]

        # æ‹¼æˆæœ€ç»ˆé¡ºåºï¼ˆæŒ‰å¤©äº¤é”™ï¼šæŸ±â†’æ¢â†’æ”¯æ’‘â†’å…¶ä»–ï¼‰
        target = []
        num_days = len(buckets)
        for i in range(num_days):
            for cat in CATEGORY_ORDER:
                if cat in categories_present:
                    target += pages_slices_by_cat[cat][i]

        for idx, name in enumerate(target):
            cur = wb.sheetnames.index(name)
            if cur != idx:
                wb.move_sheet(wb[name], idx - cur)

        # å†™å…¥ï¼ˆé€å¤©ï¼‰
        total_blocks = 0
        for cat in categories_present:
            total_blocks += sum(len(v) for v in blocks_by_cat_bucket[cat].values())
        prog = Prog(total_blocks, "å†™å…¥ Excel")

        for i in range(num_days):
            day_pages = []
            day_blocks = []
            for cat in CATEGORY_ORDER:
                if cat in categories_present:
                    day_pages += pages_slices_by_cat[cat][i]
                    day_blocks += blocks_slices_by_cat[cat][i]
            fill_blocks_to_pages(wb, day_pages, day_blocks, prog)
            apply_meta_on_pages(wb, day_pages, buckets[i]["date"])

        prog.finish()
        cleanup_unused_mu_templates(wb, target)
        return target

    else:
        raise ValueError(f"æœªçŸ¥çš„æ¨¡å¼ï¼š{mode}")


    # ===== é¢„å¤„ç†ä¸æ¨¡å¼è¿è¡Œå°è£… =====


def prepare_from_word(src: Path):
    groups_all_tables, all_rows = read_groups_from_doc(src)
    grouped = defaultdict(list)
    for g in groups_all_tables:
        grouped[kind_of(g["name"])].append(g)
    categories_present = [cat for cat in CATEGORY_ORDER if grouped.get(cat)]
    print("ğŸ“Š è¯†åˆ«ï¼š " + "ã€".join(f"{cat} {len(grouped.get(cat, []))}" for cat in categories_present))

    doc_out = build_summary_doc_with_progress(all_rows)
    set_doc_font_progress(doc_out, DEFAULT_FONT_PT)
    out_docx = src.with_name("æ±‡æ€»åŸå§‹è®°å½•.docx")
    print("ğŸ’¾ æ­£åœ¨ä¿å­˜æ±‡æ€» Word â€¦")

    save_docx_safe(doc_out, out_docx)
    print(f"âœ… æ±‡æ€» Word å·²ä¿å­˜ï¼š{out_docx}")
    return grouped, categories_present


def run_with_mode(src: Path, grouped, categories_present, meta):
    tpl_path = XLSX_WITH_SUPPORT_DEFAULT  # å§‹ç»ˆä½¿ç”¨æœ‰æ”¯æ’‘æ¨¡æ¿
    if not tpl_path.exists():
        raise FileNotFoundError(f"Excel æ¨¡æ¿ä¸å­˜åœ¨ï¼š{tpl_path}")

    wb = load_workbook_safe(tpl_path)

    try:
        mode = prompt_mode()
        used_names_total = run_mode(mode, wb, grouped, categories_present)
    except BackStep:
        return

    apply_meta_fixed(wb, categories_present, meta)
    enforce_mu_font(wb)
    cleanup_unused_sheets(wb, used_names_total, bases=tuple(CATEGORY_ORDER))

    def unique_out_path(dest_dir: Path, stem: str) -> Path:
        cand = dest_dir / f"{stem}.xlsx"
        if not cand.exists():
            return cand
        i = 1
        while True:
            cand = dest_dir / f"{stem}({i}).xlsx"
            if not cand.exists():
                return cand
            i += 1

    final_path = unique_out_path(src.parent, f"{TITLE}_æŠ¥å‘Šç‰ˆ")
    save_workbook_safe(wb, final_path)
    print(f"âœ… Excel å·²ä¿å­˜ï¼š{final_path}")
    print("âœ” å®Œæˆã€‚æœ¬æ¬¡å¯¼å‡ºç»“æŸã€‚")

# ===== éäº¤äº’å…¥å£ï¼ˆä¾› GUI è°ƒç”¨ / å¯è„šæœ¬åŒ–ï¼‰ =====
# ====== æ—¥æœŸå¡«å……å·¥å…·ï¼ˆæ–°å¢ï¼‰ ======
import re
from datetime import datetime
from pathlib import Path

def _normalize_date(date_str: str) -> str:
    """
    æ¥å— '2025-10-13' / '2025/10/13' / '2025.10.13' / '2025å¹´10æœˆ13æ—¥' / '2025 10 13'
    ç»Ÿä¸€è§„èŒƒä¸º 'YYYY-MM-DD'ï¼›ä¸åˆæ³•åˆ™æŠ›å¼‚å¸¸ã€‚
    """
    s = str(date_str).strip()
    if not s:
        raise ValueError("æ£€æµ‹æ—¥æœŸä¸ºç©º")
    nums = list(map(int, re.findall(r"\d+", s)))
    if len(nums) >= 3:
        y, m, d = nums[:3]
        dt = datetime(year=y, month=m, day=d)
        return dt.strftime("%Y-%m-%d")
    try:
        return datetime.fromisoformat(s).strftime("%Y-%m-%d")
    except Exception:
        raise ValueError(f"æ— æ³•è¯†åˆ«çš„æ—¥æœŸæ ¼å¼ï¼š{s}")

def _fill_date_in_sheet(ws, date_text: str) -> bool:
    """
    åœ¨å•ä¸ªå·¥ä½œè¡¨é‡Œå¯»æ‰¾â€œæ—¥æœŸ/æ£€éªŒæ—¥æœŸ/æ¢ä¼¤æ—¥æœŸâ€å­—æ ·ï¼ˆå‰20è¡ŒÃ—å‰20åˆ—ï¼‰ï¼Œ
    ä¼˜å…ˆå†™åˆ°å³ä¾§å•å…ƒæ ¼ï¼›è‹¥å³ä¾§ä¸å¯å†™ï¼Œåˆ™æŠŠå½“å‰å•å…ƒæ ¼æ–‡æœ¬æ›¿æ¢ä¸ºâ€œâ€¦â€¦ï¼šYYYY-MM-DDâ€ã€‚
    è¿”å›æ˜¯å¦å†™å…¥æˆåŠŸã€‚
    """
    ROW_MAX, COL_MAX = 20, 20
    for r in range(1, min(ws.max_row, ROW_MAX) + 1):
        for c in range(1, min(ws.max_column, COL_MAX) + 1):
            cell = ws.cell(r, c)
            v = cell.value
            if isinstance(v, str) and ("æ—¥æœŸ" in v or "æ£€éªŒæ—¥æœŸ" in v or "æ¢ä¼¤æ—¥æœŸ" in v):
                # 1) å³ä¾§é‚»æ ¼ä¼˜å…ˆ
                try:
                    neighbor = ws.cell(r, c + 1)
                    if neighbor.value in (None, "", "â€”â€”", "-", "â€”"):
                        neighbor.value = date_text
                        return True
                except Exception:
                    pass
                # 2) æ”¹å½“å‰æ ¼æ–‡æœ¬
                txt = v
                txt = re.sub(r"(æ£€éªŒæ—¥æœŸ|æ¢ä¼¤æ—¥æœŸ|æ—¥æœŸ)[:ï¼š]?\s*$", r"\1ï¼š" + date_text, txt)
                cell.value = txt
                return True
    return False

def apply_date_to_workbook(wb, date_text: str) -> int:
    """æŠŠæ—¥æœŸå†™å…¥å·¥ä½œç°¿çš„å¯è§å·¥ä½œè¡¨ï¼›è¿”å›æˆåŠŸå†™å…¥çš„è¡¨æ•°é‡ã€‚"""
    ok = 0
    for ws in wb.worksheets:
        try:
            if _fill_date_in_sheet(ws, date_text):
                ok += 1
        except Exception:
            pass
    return ok


# ====== éäº¤äº’å…¥å£ï¼ˆæ›¿æ¢ä¸ºè¿™ä¸ªå®Œæ•´ä½“ï¼‰ ======
def run_noninteractive(
    src_path,
    mode=3,
    meta=None,
    support_strategy=None,   # "number" | "floor"
    net_strategy=None,       # "number" | "floor"
    dates=None,              # é¢„ç•™ï¼šmode1 ç”¨
    temperature=None,        # é¢„ç•™
    quota_plan=None,         # é¢„ç•™ï¼šmode4 ç”¨
    single_date=None,        # æ–°å¢ï¼šå•æ—¥æ¨¡å¼çš„â€œæ£€æµ‹æ—¥æœŸâ€
):
    """
    ä¸€æ¬¡æ€§æ‰§è¡Œå®Œæ•´æµç¨‹ï¼ˆè¯»å– Word â†’ ç”Ÿæˆ Excel â†’ ä¿å­˜ï¼‰ï¼Œä¸ä¾èµ– input()ã€‚
    ç›®å‰ç¨³å®šæ”¯æŒ mode=3ï¼ˆå•æ—¥æ¨¡å¼ï¼‰ç›´è·‘ï¼›å…¶å®ƒæ¨¡å¼ä¼šè‡ªåŠ¨å›é€€è‡³ 3ï¼Œé¿å…å¡ä½ã€‚
    è¿”å›ï¼š{"excel": Path, "word": Path}
    """
    # 1) æ ¡éªŒæº
    src = Path(str(src_path)).expanduser().resolve()
    if not src.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°æºæ–‡ä»¶ï¼š{src}")
    if src.suffix.lower() != ".docx":
        raise ValueError("æºæ–‡ä»¶å¿…é¡»ä¸º .docx")

    # 2) ä» Word è¯»å–ã€åˆ†ç»„ & æ±‡æ€»
    grouped, categories_present = prepare_from_word(src)

    # 3) è®¾ç½®åˆ†æ¡¶ç­–ç•¥ï¼ˆè‹¥ä¼ å…¥åˆ™è¦†ç›–å…¨å±€ï¼‰
    global support_bucket_strategy, net_bucket_strategy
    if support_strategy in ("number", "floor"):
        support_bucket_strategy = support_strategy
    if net_strategy in ("number", "floor"):
        net_bucket_strategy = net_strategy

    # 4) é€‰æ‹©æ¨¡æ¿å¹¶è½½å…¥
    mode_str = str(mode) if str(mode) in {"1", "2", "3", "4"} else "3"
    if mode_str != "3":
        # å½“å‰ä»…ä¿è¯å•æ—¥æ¨¡å¼æ— äº¤äº’ç›´è·‘ï¼Œå…¶å®ƒæ¨¡å¼å›é€€åˆ° 3
        mode_str = "3"

    tpl_path = XLSX_WITH_SUPPORT_DEFAULT
    if not tpl_path.exists():
        raise FileNotFoundError(f"Excel æ¨¡æ¿ä¸å­˜åœ¨ï¼š{tpl_path}")
    wb = load_workbook_safe(tpl_path)

    # 5) ç”Ÿæˆå¡«è¡¨ï¼ˆæŒ‰ä½ çš„å†…éƒ¨å®ç°ï¼Œè¿™é‡Œæ˜¯ä½ å·²æœ‰çš„â€œå•æ—¥æ¨¡å¼â€å…¥å£ï¼‰
    #    æ³¨æ„ï¼šå¦‚æœä½ é¡¹ç›®é‡Œå¯¹åº”å‡½æ•°åæ˜¯ run_with_mode(...)ï¼Œè¯·æ®å®æ›¿æ¢è¿™ä¸€è¡Œã€‚
    used_names_total = run_mode(mode_str, wb, grouped, categories_present)

    # 6) å†™å…ƒä¿¡æ¯ & ç»Ÿä¸€å­—ä½“ & æ¸…é™¤æ— ç”¨è¡¨
    meta = meta or {}
    apply_meta_fixed(wb, categories_present, meta)
    enforce_mu_font(wb)
    cleanup_unused_sheets(wb, used_names_total, bases=tuple(CATEGORY_ORDER))

    # 7) è‹¥ä¼ å…¥â€œæ£€æµ‹æ—¥æœŸâ€ï¼Œè§„èŒƒåŒ–å¹¶å†™å…¥å·¥ä½œç°¿
    if single_date:
        dt_norm = _normalize_date(single_date)
        _ = apply_date_to_workbook(wb, dt_norm)

    # 8) ç”Ÿæˆä¸è¦†ç›–çš„è¾“å‡ºè·¯å¾„å¹¶ä¿å­˜
    def _unique_out_path(dest_dir: Path, stem: str) -> Path:
        cand = dest_dir / f"{stem}.xlsx"
        if not cand.exists():
            return cand
        i = 1
        while True:
            cand = dest_dir / f"{stem}({i}).xlsx"
            if not cand.exists():
                return cand
            i += 1

    final_xlsx = _unique_out_path(src.parent, f"{TITLE}_æŠ¥å‘Šç‰ˆ")
    save_workbook_safe(wb, final_xlsx)

    # 9) è¿”å›è·¯å¾„
    word_out = src.with_name("æ±‡æ€»åŸå§‹è®°å½•.docx")
    return {"excel": final_xlsx, "word": word_out}


def _norm_entry_list(entries):
    """è§„èŒƒåŒ–è®¡åˆ’æ¡ç›®åˆ—è¡¨ï¼ˆæ—¥æœŸç»Ÿä¸€ä¸º YYYY-MM-DDï¼Œæ•°é‡è½¬ int/Noneï¼‰ã€‚"""
    out = []
    if not entries:
        return out
    for d, lim in entries:
        nd = None
        last_err = None
        for fn in (normalize_date, _normalize_date):
            if not fn:
                continue
            try:
                nd = fn(d)
                break
            except Exception as exc:  # noqa: PERF203 - éœ€è¦é€ä¸ªå°è¯•
                last_err = exc
        if not nd:
            raise ValueError(f"æ— æ³•è¯†åˆ«çš„æ—¥æœŸï¼š{d}") from last_err
        if lim in (None, "", "-", "âˆ"):
            nl = None
        else:
            try:
                nl = int(lim)
            except Exception:
                digits = re.findall(r"\d+", str(lim))
                nl = int(digits[0]) if digits else None
        out.append((nd, nl))
    return out


def _norm_plan(plan: dict | None) -> dict:
    """è§„èŒƒåŒ–æŒ‰ç±»åˆ«/æ¥¼å±‚çš„è®¡åˆ’ç»“æ„ã€‚"""
    if not plan:
        return {}
    result: dict = {}
    for cat, by_floor in plan.items():
        result[cat] = {}
        for floor, entries in (by_floor or {}).items():
            result[cat][floor] = _norm_entry_list(entries)
    return result


def export_mode4_noninteractive(
        src_docx: Union[str, Path],
        meta: dict | None = None,
        wb=None,
        *,
        plan: dict | None = None,
        include_support: bool = True,
        support_strategy: str = "number",
        net_strategy: str = "number",
        fallback: str = "append_last",
        default_entries: list[tuple[str, int | None]] | None = None,
        write_dates_to_header: bool = True,
) -> tuple[Path, Path | None]:
    """æ— äº¤äº’å¯¼å‡º Mode4ã€‚"""

    src = Path(str(src_docx)).resolve()
    if not src.exists():
        raise FileNotFoundError(f"æœªæ‰¾åˆ° Word æºæ–‡ä»¶ï¼š{src}")

    grouped = None
    categories_present = None
    cache_src = _PROBE_CACHE.get("src")
    if cache_src and Path(str(cache_src)).resolve() == src:
        grouped = _PROBE_CACHE.get("grouped") or defaultdict(list)
        categories_present = list(_PROBE_CACHE.get("categories") or [])

    if grouped is None or categories_present is None:
        grouped, categories_present = prepare_from_word(src)
    else:
        if not isinstance(grouped, defaultdict):
            tmp = defaultdict(list)
            for k, v in (grouped or {}).items():
                tmp[k] = list(v)
            grouped = tmp

    categories_present = [cat for cat in CATEGORY_ORDER if grouped.get(cat)]
    if not include_support and "æ”¯æ’‘" in categories_present:
        categories_present = [c for c in categories_present if c != "æ”¯æ’‘"]

    prev_support = support_bucket_strategy
    prev_net = net_bucket_strategy
    sup_val = (support_strategy or "number").lower()
    net_val = (net_strategy or "number").lower()
    set_support_strategy(sup_val)
    set_net_strategy(net_val)

    if wb is None:
        if not XLSX_WITH_SUPPORT_DEFAULT.exists():
            raise FileNotFoundError(f"Excel æ¨¡æ¿ä¸å­˜åœ¨ï¼š{XLSX_WITH_SUPPORT_DEFAULT}")
        wb = load_workbook_safe(XLSX_WITH_SUPPORT_DEFAULT)

    globals()["NONINTERACTIVE_MODE4_PLAN"] = _norm_plan(plan)
    globals()["NONINTERACTIVE_MODE4_FALLBACK"] = (fallback or "").lower()
    globals()["NONINTERACTIVE_MODE4_DEFAULT"] = _norm_entry_list(default_entries or [])
    globals()["NONINTERACTIVE_MODE4_SUPPORT_STRATEGY"] = sup_val
    globals()["NONINTERACTIVE_MODE4_NET_STRATEGY"] = net_val
    globals()["NONINTERACTIVE_MODE4_WRITE_DATES"] = bool(write_dates_to_header)

    try:
        used_pages = run_mode("4", wb, grouped, categories_present)
        apply_meta_fixed(wb, categories_present, meta or {})
        cleanup_unused_mu_templates(wb, used_pages)

        def _unique_out_path(dest_dir: Path, stem: str) -> Path:
            cand = dest_dir / f"{stem}.xlsx"
            if not cand.exists():
                return cand
            i = 1
            while True:
                cand = dest_dir / f"{stem}({i}).xlsx"
                if not cand.exists():
                    return cand
                i += 1

        final_xlsx = _unique_out_path(src.parent, f"{TITLE}_æŠ¥å‘Šç‰ˆ")
        save_workbook_safe(wb, final_xlsx)

        word_out = src.with_name("æ±‡æ€»åŸå§‹è®°å½•.docx")
        if not word_out.exists():
            all_rows = _PROBE_CACHE.get("all_rows")
            if all_rows:
                try:
                    doc_out = build_summary_doc_with_progress(all_rows)
                    set_doc_font_progress(doc_out, DEFAULT_FONT_PT)
                    save_docx_safe(doc_out, word_out)
                except Exception:
                    word_out = None
            else:
                word_out = None

        return final_xlsx, word_out
    finally:
        set_support_strategy(prev_support)
        set_net_strategy(prev_net)
        for key in (
                "NONINTERACTIVE_MODE4_PLAN",
                "NONINTERACTIVE_MODE4_FALLBACK",
                "NONINTERACTIVE_MODE4_DEFAULT",
                "NONINTERACTIVE_MODE4_SUPPORT_STRATEGY",
                "NONINTERACTIVE_MODE4_NET_STRATEGY",
                "NONINTERACTIVE_MODE4_WRITE_DATES",
        ):
            globals().pop(key, None)


def export_mode1_noninteractive(
        src_docx,
        out_dir=None,
        *,
        buckets,
        support_strategy="number",
        net_strategy="number",
        later_priority=True,
        auto_merge_rest=True,
        meta=None,
):
    """çº¯æ— äº¤äº’å¯¼å‡º Mode1ã€‚"""

    provider = Mode1ConfigProvider(
        buckets,
        support_strategy,
        net_strategy,
        later_priority,
        auto_merge_rest,
        meta=meta,
    )

    src = Path(str(src_docx)).resolve()
    if not src.exists():
        raise FileNotFoundError(f"æœªæ‰¾åˆ° Word æºæ–‡ä»¶ï¼š{src}")

    out_dir = Path(out_dir) if out_dir is not None else src.parent
    out_dir = out_dir.resolve()
    out_dir.mkdir(parents=True, exist_ok=True)

    cache_src = _PROBE_CACHE.get("src")
    grouped = None
    categories_present = None
    if cache_src and Path(str(cache_src)).resolve() == src:
        grouped = _PROBE_CACHE.get("grouped") or defaultdict(list)
        categories_present = list(_PROBE_CACHE.get("categories") or [])
    if grouped is None or categories_present is None:
        grouped, categories_present = prepare_from_word(src)
    else:
        if not isinstance(grouped, defaultdict):
            tmp = defaultdict(list)
            for k, v in (grouped or {}).items():
                tmp[k] = list(v)
            grouped = tmp

    categories_present = [cat for cat in CATEGORY_ORDER if grouped.get(cat)]

    tpl_path = XLSX_WITH_SUPPORT_DEFAULT
    if not tpl_path.exists():
        raise FileNotFoundError(f"Excel æ¨¡æ¿ä¸å­˜åœ¨ï¼š{tpl_path}")
    wb = load_workbook_safe(tpl_path)

    prev_support = support_bucket_strategy
    prev_net = net_bucket_strategy
    set_support_strategy(provider.support_strategy)
    set_net_strategy(provider.net_strategy)

    buckets_norm = provider.get_buckets()
    try:
        cat_byb, remain_by_cat = assign_by_buckets(grouped, buckets_norm, provider.later_priority)
    finally:
        set_support_strategy(prev_support)
        set_net_strategy(prev_net)

    if provider.auto_merge_rest:
        merge_remains_into_last_bucket(cat_byb, remain_by_cat)

    blocks_by_cat_bucket = expand_blocks_by_bucket(cat_byb)

    pages_slices_by_cat = {}
    blocks_slices_by_cat = {}
    for cat in categories_present:
        bucket_map = blocks_by_cat_bucket.get(cat, {})
        pages_slices, blocks_slices = ensure_pages_slices_for_cat_muaware(wb, cat, bucket_map)
        pages_slices_by_cat[cat] = [_filter_pages_for_cat(sl, cat) for sl in pages_slices]
        blocks_slices_by_cat[cat] = blocks_slices

    num_days = len(buckets_norm)
    target = []
    for i in range(num_days):
        for cat in CATEGORY_ORDER:
            if cat in categories_present:
                target += pages_slices_by_cat[cat][i]

    for idx, name in enumerate(target):
        cur = wb.sheetnames.index(name)
        if cur != idx:
            wb.move_sheet(wb[name], idx - cur)

    total_blocks = 0
    for cat in categories_present:
        total_blocks += sum(len(v) for v in blocks_by_cat_bucket.get(cat, {}).values())
    prog = Prog(total_blocks, "å†™å…¥ Excel")

    for day_idx in range(num_days):
        day_pages = []
        day_blocks = []
        for cat in CATEGORY_ORDER:
            if cat in categories_present:
                day_pages += pages_slices_by_cat[cat][day_idx]
                day_blocks += blocks_slices_by_cat[cat][day_idx]
        fill_blocks_to_pages(wb, day_pages, day_blocks, prog)
        raw = buckets_norm[day_idx].get("date_raw") or buckets_norm[day_idx].get("date") or ""
        dt = normalize_date(raw) if raw else ""
        apply_meta_on_pages(wb, day_pages, dt)

    prog.finish()

    cleanup_unused_mu_templates(wb, target)
    apply_meta_fixed(wb, categories_present, provider.meta)
    enforce_mu_font(wb)
    cleanup_unused_sheets(wb, target, bases=tuple(CATEGORY_ORDER))

    def _unique_out_path(dest_dir: Path, stem: str) -> Path:
        cand = dest_dir / f"{stem}.xlsx"
        if not cand.exists():
            return cand
        i = 1
        while True:
            cand = dest_dir / f"{stem}({i}).xlsx"
            if not cand.exists():
                return cand
            i += 1

    final_xlsx = _unique_out_path(out_dir, f"{TITLE}_æŠ¥å‘Šç‰ˆ")
    save_workbook_safe(wb, final_xlsx)

    word_out = src.with_name("æ±‡æ€»åŸå§‹è®°å½•.docx")
    if not word_out.exists():
        all_rows = _PROBE_CACHE.get("all_rows")
        if all_rows:
            try:
                doc_out = build_summary_doc_with_progress(all_rows)
                set_doc_font_progress(doc_out, DEFAULT_FONT_PT)
                save_docx_safe(doc_out, word_out)
            except Exception:
                pass

    return final_xlsx, word_out


# ===== éäº¤äº’ï¼šæŒ‰æ¥¼å±‚æ–­ç‚¹ï¼ˆMode 2ï¼‰å¯¼å‡º =====

def export_mode2_noninteractive(
    src_docx: Union[str, Path],
    meta: dict | None = None,
    wb=None,
    *,
    breaks_gz: str = "",
    breaks_gl: str = "",
    breaks_support: str = "",
    breaks_net: str = "",
    date_first: str = "",
    date_second: str = "",
    include_support: bool = True,
    support_strategy: str = "number",
    net_strategy: str = "number",
):
    src = Path(str(src_docx)).resolve()
    if not src.exists():
        raise FileNotFoundError(f"æœªæ‰¾åˆ° Word æºæ–‡ä»¶ï¼š{src}")

    grouped = None
    categories_present = None
    cache_src = _PROBE_CACHE.get("src")
    if cache_src and Path(str(cache_src)).resolve() == src:
        grouped = _PROBE_CACHE.get("grouped")
        categories_present = _PROBE_CACHE.get("categories")

    if not grouped:
        info = probe_categories_from_docx(src)
        grouped = _PROBE_CACHE.get("grouped")
        if isinstance(info, dict):
            categories_present = info.get("categories")

    if not grouped:
        groups_all_tables, all_rows = read_groups_from_doc(src, progress=False)
        grouped = defaultdict(list)
        for g in groups_all_tables:
            grouped[kind_of(g["name"])].append(g)
        categories_present = [cat for cat in CATEGORY_ORDER if grouped.get(cat)]
        _PROBE_CACHE.update({
            "src": str(src),
            "grouped": grouped,
            "all_rows": all_rows,
            "categories": categories_present,
        })

    if not isinstance(grouped, defaultdict):
        tmp = defaultdict(list)
        for k, v in (grouped or {}).items():
            tmp[k] = list(v)
        grouped = tmp

    categories_present = categories_present or [cat for cat in CATEGORY_ORDER if grouped.get(cat)]
    categories_present = list(categories_present)
    if not include_support and "æ”¯æ’‘" in categories_present:
        categories_present.remove("æ”¯æ’‘")

    globals()["NONINTERACTIVE_MODE2_FORCE_SAME_BREAKS"] = True
    globals()["NONINTERACTIVE_MODE2_DATE_FIRST"] = (date_first or "").strip()
    globals()["NONINTERACTIVE_MODE2_DATE_SECOND"] = (date_second or "").strip()
    globals()["NONINTERACTIVE_SUPPORT_BREAKS"] = (breaks_support or "").strip()
    globals()["NONINTERACTIVE_SUPPORT_STRATEGY"] = (support_strategy or "number").lower()
    globals()["NONINTERACTIVE_NET_STRATEGY"] = (net_strategy or "number").lower()
    globals()["NONINTERACTIVE_NET_BREAKS"] = (breaks_net or "").strip()

    created_here = wb is None
    if wb is None:
        template_path = None
        for name in ("XLSX_WITH_SUPPORT_DEFAULT", "XLSX_TEMPLATE_WITH_SUPPORT", "DEFAULT_XLSX_WITH_SUPPORT"):
            if name in globals() and globals()[name]:
                template_path = Path(globals()[name])
                break
        if not template_path or not template_path.exists():
            raise FileNotFoundError("æœªæ‰¾åˆ° Excel æ¨¡æ¿å¸¸é‡ï¼ˆXLSX_WITH_SUPPORT_DEFAULT / XLSX_TEMPLATE_WITH_SUPPORT / DEFAULT_XLSX_WITH_SUPPORTï¼‰ã€‚")
        wb = load_workbook_safe(template_path)

    try:
        used_pages = run_mode(
            "2",
            wb,
            categories_present=categories_present,
            grouped_preloaded=grouped,
            breaks_gz=breaks_gz or "",
            breaks_gl=breaks_gl or "",
            include_support=include_support,
        )
    finally:
        for key in (
            "NONINTERACTIVE_MODE2_FORCE_SAME_BREAKS",
            "NONINTERACTIVE_MODE2_DATE_FIRST",
            "NONINTERACTIVE_MODE2_DATE_SECOND",
            "NONINTERACTIVE_SUPPORT_BREAKS",
            "NONINTERACTIVE_SUPPORT_STRATEGY",
            "NONINTERACTIVE_NET_STRATEGY",
            "NONINTERACTIVE_NET_BREAKS",
        ):
            globals().pop(key, None)

    if created_here:
        all_rows = _PROBE_CACHE.get("all_rows")
        if all_rows:
            doc_out = build_summary_doc_with_progress(all_rows)
            set_doc_font_progress(doc_out, DEFAULT_FONT_PT)
            save_docx_safe(doc_out, src.with_name("æ±‡æ€»åŸå§‹è®°å½•.docx"))
        apply_meta_fixed(wb, categories_present, meta or {})
        enforce_mu_font(wb)
        cleanup_unused_sheets(wb, used_pages, bases=tuple(CATEGORY_ORDER))
        final_path = src.with_name(f"{TITLE}_æŠ¥å‘Šç‰ˆ.xlsx")
        save_workbook_safe(wb, final_path)
        word_out = src.with_name("æ±‡æ€»åŸå§‹è®°å½•.docx")
        return {"excel": final_path, "word": word_out}

    return {"used_pages": used_pages, "workbook": wb}

def read_groups_from_doc(path: Path, *, progress: bool = True):
    """
    ä»Wordæ–‡æ¡£ä¸­è¯»å–å¹¶è§£ææ„ä»¶æ•°æ®ç»„ï¼Œè¿”å›ç»“æ„åŒ–åˆ†ç»„æ•°æ®å’ŒåŸå§‹è¡Œæ•°æ®ã€‚

    æµç¨‹ï¼š
    1. æ‰“å¼€Wordæ–‡æ¡£å¹¶éå†æ‰€æœ‰è¡¨æ ¼ï¼Œç­›é€‰å«â€œæµ‹ç‚¹1â€å’Œâ€œå¹³å‡å€¼â€çš„æœ‰æ•ˆæ•°æ®è¡¨æ ¼
    2. å¯¹æ¯ä¸ªæœ‰æ•ˆè¡¨æ ¼æå–æ•°æ®è¡Œï¼ˆå¸¦è¿›åº¦æç¤ºï¼‰
    3. å°†æå–çš„åŸå§‹è¡Œæ•°æ®è½¬æ¢ä¸ºæŒ‰æ„ä»¶åç§°åˆ†ç»„çš„ç»“æ„åŒ–æ•°æ®

    ç»“æ„åŒ–æ•°æ®ç»„åŒ…å«æ„ä»¶åç§°å’Œå¯¹åº”çš„æµ‹ç‚¹æ•°æ®ï¼ˆ8ä¸ªè¯»æ•°+1ä¸ªå¹³å‡å€¼ï¼‰ï¼Œé€‚é…åç»­Excelå¡«å……éœ€æ±‚ã€‚

    Args:
        path: Wordæ–‡æ¡£è·¯å¾„ï¼ˆPathå¯¹è±¡ï¼‰
    Returns:
        tuple: åŒ…å«ä¸¤ä¸ªå…ƒç´ çš„å…ƒç»„ï¼š
            - æ„ä»¶æ•°æ®ç»„åˆ—è¡¨ï¼ˆlist[dict]ï¼‰ï¼Œæ¯ä¸ªå…ƒç´ å«'name'ï¼ˆæ„ä»¶åï¼‰å’Œ'data'ï¼ˆæ•°æ®è¡Œåˆ—è¡¨ï¼‰
            - æ‰€æœ‰åŸå§‹æ•°æ®è¡Œåˆ—è¡¨ï¼ˆlist[dict]ï¼‰ï¼Œå«æå–çš„æµ‹ç‚¹å€¼ã€å¹³å‡å€¼ç­‰åŸå§‹ä¿¡æ¯
    """
    doc = Document(str(path))
    all_rows = []
    tables = doc.tables
    T = sum(1 for t in tables if is_data_table(t))  # noqa
    used = 0
    for tbl in tables:
        if not is_data_table(tbl):
            continue
        used += 1
        part = extract_rows_with_progress(tbl, used, T, show_progress=progress)
        if part:
            all_rows.extend(part)
    return groups_from_your_rows(all_rows), all_rows


def main():
    """å‘½ä»¤è¡Œäº¤äº’å…¥å£ã€‚"""
    print(f"{TITLE} {VERSION}")
    print("è¾“å…¥ help æŸ¥çœ‹æ¨¡å¼è¯´æ˜ï¼›éšæ—¶è¾“å…¥ q è¿”å›ä¸Šä¸€æ­¥ã€‚")

    while True:
        try:
            src = prompt_path("ğŸ“„ è¯·é€‰æ‹©åŸå§‹è®°å½• Word", WORD_SRC_DEFAULT)
        except BackStep:
            print("â†© å·²è¿”å›ã€‚")
            continue
        except KeyboardInterrupt:
            print("\nå·²å–æ¶ˆã€‚")
            return
        except EOFError:
            print("\nå·²é€€å‡ºã€‚")
            return

        try:
            probe = probe_categories_from_docx(src)
        except Exception as exc:
            print(f"âŒ è¯†åˆ«å¤±è´¥ï¼š{exc}")
            continue

        categories = list((probe or {}).get("categories") or [])
        counts = (probe or {}).get("counts") or {}
        if categories:
            print("ğŸ“Š è¯†åˆ«ï¼š" + "ã€".join(f"{cat} {counts.get(cat, 0)}" for cat in categories))
        else:
            print("âš ï¸ æœªè¯†åˆ«åˆ°å¯ç”¨æ„ä»¶ã€‚")

        all_rows = _PROBE_CACHE.get("all_rows")
        if all_rows:
            try:
                doc_out = build_summary_doc_with_progress(all_rows)
                set_doc_font_progress(doc_out, DEFAULT_FONT_PT)
                out_docx = Path(src).with_name("æ±‡æ€»åŸå§‹è®°å½•.docx")
                print("ğŸ’¾ æ­£åœ¨ä¿å­˜æ±‡æ€» Word â€¦")
                save_docx_safe(doc_out, out_docx)
                print(f"âœ… æ±‡æ€» Word å·²ä¿å­˜ï¼š{out_docx}")
            except Exception as exc:
                print(f"âš ï¸ æ±‡æ€» Word ä¿å­˜å¤±è´¥ï¼š{exc}")

        try:
            proj = ask("å·¥ç¨‹åç§°ï¼ˆå›è½¦è·³è¿‡ï¼Œè¾“å…¥ q è¿”å›ï¼‰ï¼š")
            order = ask("å§”æ‰˜ç¼–å·ï¼ˆå›è½¦è·³è¿‡ï¼Œè¾“å…¥ q è¿”å›ï¼‰ï¼š")
        except BackStep:
            print("â†© è¿”å›æ–‡ä»¶é€‰æ‹©ã€‚")
            continue

        meta = {}
        if proj:
            meta["proj"] = proj
        if order:
            meta["order"] = order

        grouped_cached = _PROBE_CACHE.get("grouped") or defaultdict(list)
        try:
            run_with_mode(Path(src), grouped_cached, categories or None, meta)
        except BackStep:
            print("â†© è¿”å›æ¨¡å¼é€‰æ‹©ã€‚")
            continue
        except Exception as exc:
            print(f"âŒ å‡ºè¡¨å¤±è´¥ï¼š{exc}")
            continue

        try:
            again = ask("æ˜¯å¦ç»§ç»­å¤„ç†å…¶ä»–æ–‡ä»¶ï¼Ÿï¼ˆy=ç»§ç»­ / å…¶å®ƒ=é€€å‡ºï¼‰ï¼š", lower=True)
        except BackStep:
            break
        if again != "y":
            break


if __name__ == "__main__":
    main()

                                                                                                         # v 1.0.1
